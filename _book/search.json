[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thoughts and Advice on Writing your BAP",
    "section": "",
    "text": "Preface\nI have been teaching a Bachelor’s Project (BAP) at Leiden University for several years now. I have seen great theses, poor theses, and everything in between as both a supervisor and as a second reader on theses in other courses. This book is a collection of materials that I have been developing for the past several years aimed at giving you additional guidance for developing a good (perhaps even great) thesis.\nThere are three main sections to this book:\n\nWriting the Thesis: This part slightly puts the cart before the horse by considering the written product of the thesis - what should be in it, how to structure it, and how to communicate effectively. It also provides links to some other resources that I think can help you in this process (see the starting preface/part of each section for additional resources).\nDoing Research: This part focuses on the process of doing research. In particular, it provides some guidance on how to find relevant research to review for your project and how to think about finding relevant data to analyze (presuming you are not collecting your own).\nPractical Issues in Data Analysis: This section focuses on providing some advice on how to analyze and present data in an effective manner. It is not a full primer on statistical analyses (you should consult your Statistics I/II notes and the resources provided there for that), but seeks to bridge the gap between the abstract and the practical.\n\nWhat I write naturally reflects my own opinions and tastes. That does not make them “correct”, of course, although this information may still be of value even if it is purely idiosyncratic to me as a reader given that I’m one of the people grading your thesis! In addition, this document, by its very nature, is focused on general advice; your specific project may require deviations or alterations to what I suggest here.\nI will highlight some other resources for helping you along the way in subsequent sections as I note above. One resource that I want to highlight here is a Substack written by two political scientists–Paul Musgrave and Nicholas Davis–called “Thesis Statement” (weblink). These authors have written a host of short, but very insightful, posts on how to approach nearly every part of writing one’s senior thesis. I would strongly recommend reading through what they have written as well.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "part_writing.html",
    "href": "part_writing.html",
    "title": "Writing the Thesis",
    "section": "",
    "text": "Some Other Guides\nWhat I write in the sections to come is based on my experience supervising BAP thesis projects and assessing thesis projects as a second reader. The points below are thus potentially idiosyncratic to me as a reader. However, I have also drawn on a variety of other resources from political and social scientists in composing this advice. You can find links to some of the resources I draw upon below. I recommend giving them a quick read through as well since they are pretty short, but note that some of these documents are pitched at particular institutions or particular types of assignments, so not every point in them will be relevant.",
    "crumbs": [
      "Writing the Thesis"
    ]
  },
  {
    "objectID": "part_writing.html#some-other-guides",
    "href": "part_writing.html#some-other-guides",
    "title": "Writing the Thesis",
    "section": "",
    "text": "Thesis Statement by Paul Musgrave and Nicholas Davis\nWriting Advice by John Gerring\nAssorted Tips for Students on Writing Research Papers, Dos and Don’ts of Writing for Students, and Research Design Paper Instructions by Steven V. Miller\nThree Templates for Introductions to Political Science Articles\nCh. 12: Communicating Research in “Research Design in Political Science” by Dimiter Toshkov",
    "crumbs": [
      "Writing the Thesis"
    ]
  },
  {
    "objectID": "writing_01_structure.html",
    "href": "writing_01_structure.html",
    "title": "1  Structuring the Thesis",
    "section": "",
    "text": "In general, a BAP thesis can be divided into the following portions:\n\n\nIntroduction\nReview of Literature and Argument\nResearch Design\nAnalyses/Results\nConclusion\nAppendix/Appendices\n\nI discuss each portion of the thesis in the chapters to come. But, first, a few beginning thoughts.\nFirst, do not take the above as meaning that you need to have exactly this number of sections in your thesis. It may, and often does, make sense to divide some of these portions into separate sections or subsections. The Review of Literature and Argument portion of the thesis, for instance, is often best divided into two or more sections to avoid overwhelming the reader with too much information all at once and to signal changes in topic within the thesis. It is not a bad idea, meanwhile, to divide the Research Design section into sub-sections as I’ll discuss later on.\nRegardless of these points, there should be clear section headers to delineate where each portion of the thesis begins and where the next one ends with the exception of the Introduction which does not require a section header. I prefer, and recommend, the APA style for section headers shown here with main sections centered and bolded and then sub-section headers left-aligned and bolded. For instance:\n\nA second consideration concerns length The thesis, as a whole, should be between 7,000-8,000 words (excluding the title page and references). A natural question might then be: how long should each portion be? There is no single correct answer to this question. What follows, then, are highly general recommendations that will need to be revised in relation to your particular research question and project. I would not get too attached to the specific numbers here - they were not created via any type of scientific process beyond me trying to eyeball my own writing, and past theses, and throwing out some plausible sounding ranges. However, the ranges may give some sense of the relative importance of each section.\n\nIntroduction: ~500-1000 words\nLiterature Review and theory: ~1500-2500 words\nResearch Design: ~700-1500 words\nAnalyses/Results: ~700-1500 words\nConclusion: ~600-1500 words\n\nFinally, I have a sixth item above: Appendix or Appendices. There are two types of appendix that it makes sense to include: (1) one that gives some additional details on how control variables are measured and operationalized in your model; (2) one that goes into detail about your assumption checks. I’ll discuss these in sections to come. An important point: appendices do not count toward the word count of the thesis.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Structuring the Thesis</span>"
    ]
  },
  {
    "objectID": "writing_02_introduction.html",
    "href": "writing_02_introduction.html",
    "title": "2  The Introduction",
    "section": "",
    "text": "The introduction is a mini-preview of the paper to come. The reader should come away from this section understanding what the topic of the paper is, what the specific question you are investigating is, why these things are interesting/relevant (the stakes of the research), and have a sense of what your answer to the question is.\nHow should you go about writing this section and accomplishing these goals? Here is an outline of a paper I recently wrote that exemplifies one way of approaching these tasks.1 The bit after the colon (e.g., “Paragraph 1: blah blah blah”) indicates the general goal of the paragraph, while the bit after “Example:” indicates the specific point I was trying to make in that paragraph in this particular paper. This is an example of how to think about using an outline to help you structure your thinking and make it easier to write (see Chapter 7).\n1 Although, given the nature of time, “recently” here is receding further and further into the past.\nParagraph 1: Here is an important subject to consider\n\nExample: People follow party cues with potentially problematic normative implications for democracy.\n\nParagraph 2: Ah, but there is something we don’t know…but should!\n\nExample: Previous work hasn’t examined the effectiveness of party cues when other actors (journalists, rival politicians) allege that the party is motivated by ulterior motives despite this being a common element of political rhetoric\n\nParagraph 3: Preview of your argument\n\nExample: I argue that cues will be less effective in these circumstances because cue taking is built on trustworthiness and these messages undermine trustworthiness.\n\nParagraph 4: Preview of study and findings\n\nExample: I use an experiment where I randomly assign party cues with and without these types of messages; I find that cue taking is indeed undermined.\n\nParagraph 5: Roadmap of paper (potentially)\n\nIn the above, I first try to establish that people should care about party cue taking by highlighting findings that people actually do use them (i.e., they’re a common element of politics) and also that there is something at stake in their use. In this case, the idea is that cue taking may threaten an understanding of democracy wherein elected officials follow the wishes of the public rather than the public simply following its leaders.2 I have thus set up some broader stakes for the paper by tying it to more general normative concerns about democracy. Identifying the stakes of the paper is important; the reader should have some sense of an answer to the ‘so what’ question (i.e., why should we care about this?) before they exit the introduction. In the second paragraph, meanwhile, I tried to highlight a gap in the literature - something we don’t know about. I then gave a preview of my particular argument and what I found. This is something that BAP students sometimes fail to do when writing their thesis to their detriment. As Davis and Musgrave note, the introduction is “not a murder mystery”. You do not want to bury the lede about what you argue and ultimately find! Instead, you should give the reader a roadmap into what is coming further on in the thesis.\n2 Druckman offers a discussion of this tension from the perspective of a public opinion researcher and whether the potential “endogeneity” of public opinion to elite communicates means that the public does not hold “quality opinions”. Disch offers a discussion from the perspective of a normative political theorist and provides an alternative understanding of responsiveness and mass opinion.There are different ways of structuring introductions and, indeed, of motivating the importance of research questions. I strongly recommend reading Three Templates for Introductions to Political Science Articles by Andrew T. Little as this document provides some great ideas of how to effectively structure this section of a paper. In my rendition above, I did this by highlighting something missing from previous research on this topic. In the actual paper, I actually used something like one of the templates identified by Little wherein I suggested that different theoretical traditions in this field predict different outcomes (i.e.: There is something common in politics we don’t know about and, moreover, Theory A predicts X about it while Theory B predicts Y…which one is correct?).\nIn the outline above, I placed a final entry: “Roadmap (potentially)”. This is a reference to ‘roadmap’ paragraphs in which the author outlines the steps they will take in the paper to come (I will first talk about X; then, I will discuss why that is insufficient; then I’ll do this…). I am personally agnostic about the inclusion of such roadmaps in the introduction to a thesis paper. Most of the time the structure is pretty common so its inclusion feels unnecessary. Not everybody feels the same, so it may be sensible to include a short roadmap to cover your bases.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Introduction</span>"
    ]
  },
  {
    "objectID": "writing_03_review.html",
    "href": "writing_03_review.html",
    "title": "3  Review of Literature and Argument",
    "section": "",
    "text": "3.1 Goals\nThe next portion of your paper is the literature review and theory building portion. You should aim to accomplish several things in this section:\nYou should have already previewed these points in the Introduction. However, here is the place to unpack that shortened preview into something with more depth.\nOne issue students sometimes have in writing this portion of their thesis is an over-focus on goal 1 to the detriment of goals 2 and 3. BAP literature reviews can sometimes read as a laundry list of readings (and this person did X and then another person did Y, and then this other person did Z) without it being clear how these different readings relate to one another or what is really the problem that you will be addressing. However, everything you write in the thesis should have a reason or point for being. Every paragraph should build on the last and help you toward realizing the goal of persuading the reader that there is something we should know but which we don’t and that you have a (potential) answer to that question and hence the content of your contribution.\nOne way of thinking about this is via a discussion from an article by Siddaway, Wood, and Hedges. The goal of this manuscript is to discuss best practices when writing what is known as a ‘systematic reviews’ of a research literature. The goal of a ‘systematic review’ is to “bring together, synthesize, and critique one or more literatures to provide an overall impression of the extent, nature, and quality of evidence in relation to a particular research question, highlighting gaps between what we know and what we need to know.”\nSiddaway, et al. discuss two types of systematic review. The first is meta-analysis in which the researcher gathers together as many published and un-published research articles on the subject at hand and then merges them together into a single dataset for subsequent quantitative analysis. The second is a “narrative” literature review in which the author still systematically canvasses a literature but instead provide a more qualitative discussion of major themes in the work under discussion.\nSiddaway et al. draw a distinction between “reviewing literature” and “literature reviews” (by which they mean these systematic reviews):1\nIdeally, you should put on the hat of judge for the assigned literature review in the substantive seminar (albeit on a smaller sampling of research). In your thesis, on the other hand, compose your review of the literature more like a lawyer. You are not just telling us what others have found, but doing so in a way to scaffold your own argument and contribution.\nMusgrave and Davis make a similar point when they tell students not to write “literature reviews” in their thesis:\nThese authors have a nice discussion of some ways of thinking about narrow (but important) ways that a senior thesis can contribute to a research literature in this post.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Review of Literature and Argument</span>"
    ]
  },
  {
    "objectID": "writing_03_review.html#goals",
    "href": "writing_03_review.html#goals",
    "title": "3  Review of Literature and Argument",
    "section": "",
    "text": "Tell the reader what we already know about this subject\nTell the reader what we don’t know (e.g., that something isn’t studied, or that there is a conflict between theories, etc.)\nTell the reader what your answer to the ‘what we don’t know’ issue is and try to persuade them that it is a good answer\n\n\n\n\n\n\n1 They discuss reviewing literature in the introduction in this excerpt. The authors are psychologists and articles in psychology often feature less of a separation between the introduction section as discussed in the preceding chapter and the ‘literature review’ section as discussed here.\nBefore discussing systematic reviews and the different types of literature review, it may be instructive to dispel two common misunderstanding about [systematic] literature reviews. The first is that conducting a literature review is the same as the task of reviewing literature, which occurs when writing the introductory section of all quantitative and qualitative journal articles (including review articles). Reviewing literature involves selectively discussing the literature on a particular topic to make the argument that a new study will make a new and/or important contribution to knowledge. In contrast, literature reviews make up a distinct research design and type of article in their own right. Rather than selectively reviewing relevant literature to make a flowing rationale for a study’s existence, they provide a comprehensive synthesis of the available evidence to allow the researcher to draw broad and robust conclusions….To best achieve the purposes of a systematic review, we like Baumeister’s (2013) advice to adopt the mindset of a judge and jury rather than a lawyer. A judge and jury skeptically evaluate the evidence to render the fairest judgment possible, whereas a lawyer’s approach is to make the best case for one side of the argument. Returning to the differences between a literature review and the task of reviewing literature, the introduction section of a quantitative or qualitative article is usually written using the lawyer’s approach.\n\n\n\n\nThe “literature review” in a thesis, by contrast, has to establish what is relevant in prior arguments to situate your own work. Some of this should reflect your own reading and research, but a good deal of it should come from the suggested readings from your adviser. Your goal with the literature review is sufficiency, not novelty and certainly not mastery. All you have to do is demonstrate that your project is not adrift alone somewhere but, rather, that it is directly linked to earlier research….Your goal is to demonstrate how your contribution fits with that conversation, and to do that you will also have to show shortcomings, conflicts, or unresolved issues in some aspect of that earlier conversation. Moreover, the problems you uncover need to relate to the work you will be doing—there’s no benefit in showing that the theory is flawed if you’re writing a methods paper, and there’s no benefit in showing that methods are flawed if you’re writing a theory paper.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Review of Literature and Argument</span>"
    ]
  },
  {
    "objectID": "writing_03_review.html#specific-tips-points-and-warnings",
    "href": "writing_03_review.html#specific-tips-points-and-warnings",
    "title": "3  Review of Literature and Argument",
    "section": "3.2 Specific Tips, Points, and Warnings",
    "text": "3.2 Specific Tips, Points, and Warnings\nThe following sub-sections provide some additional thoughts on approaching this portion of the paper.\n\n3.2.1 Focus on Theory\nA key goal of your ‘theory’ section is to describe for the reader the major approaches relevant to your question; why they don’t seem to actually answer your question (or do so in some unsatisfactory manner); and, ultimately, what your answer is to the question at hand. And, of course, to do this in as efficient an amount of space as possible.\nMy general advice here is to abstract away from the particular a little bit and focus on the general or abstract. In other words, try to avoid what I’ll call a ‘listing’ approach to the literature and focus more attention on the theories that connect the reviewed research (the general) rather than detailing every detail of the specific articles (the specific).\nAs an example, let’s return to the example of a paper on whether accusations of ulterior motivation undermine the effectiveness of party cues on public opinion (a paper topic introduced in Chapter 2). I might begin by reviewing a variety of articles on party cue taking. I might write:\n\n“Cohen (2003) uses lab experiments to study this topic. He finds that cues matter irrespective of policy specifics, i.e. people choose party over ideology. [New paragraph] Bullock (2011) uses a survey experiment to show that substantive information about a policy undermines cue taking. He does this by doing X and Y. [New paragraph] And then there is Bolsen et al (2014). Bolsen et al. recruit subjects from the Bovitz online panel and randomly assign people to various conditions. They then do all these analyses and find that cues matter a lot. [New paragraph] And then there is….”\n\nThe issue here is that your reader will likely be confused and uncertain as to what the central or most important point is of all this listing. What is the main point above beyond, perhaps, that there is variation in the literature? This might be an important point, but it does not, by itself, shed light on why party cues might matter and why, in turn, accusations of ulterior motives might moderate their influence. And, even if this is the main point, it could surely be stated much more simply!For instance:\n\nResearch is divided on the influence of party cues with some finding that they dominate other considerations (Cohen 2003; Bolsen et al. 2014) but others finding that they have a limited impact in more realistic environments (Bullock 2011; Boudreau and Mackenzie 2014)\n\nInstead, consider organizing your literature review around theories and prominent answers to your question and use specific articles to elaborate on key elements of those theories as needed. In the example above, I could instead write:\n\nMany studies suggest that party cues matter, although they vary in the theoretical basis for their claims. Importantly, these theoretical claims suggest divergent reactions to party cue taking when accusations of ulterior motivations are present. [New Paragraph] On the one hand, some authors root cue taking in social identity processes. Here, partisanship is a social identity formed young in life and then reaffirmed over time due to psychological processes that ‘bias’ the reception of new information (Cohen 2003; Bolsen et al. 2014; other relevant citations). As an example, consider Bolsen et al…. Based on this perspective we might assume that Z will not matter for cue taking because…. [New Paragraph] While the social identity approach is perhaps dominant, other scholars consider partisanship from a different lens. Here, partisanship is a running tally based on performance evaluations (Fiorina 1981; Achen 1992). Party cues are followed only in certain cases, such as X and Y. For instance, Bullock (2011) shows…. Based on this perspective, we would expect reduced cue taking because…\n\nThis latter style focuses attention on what matters: the theory or theories involved in this debate, what they imply for the research question, and the evidence relevant for the paper’s argument. You can still use articles/books/book chapters here, but they are meant to elaborate on or exemplify something more general.\nThere may be exceptions to the above of course. If you theory section requires you to directly contrast two different articles, for instance, then that would be fine. But, in general, I think it wise to root yourself in the theoretical approach(es) that underlie the articles you are talking about - to make sure that these are clearly described and discussed.\nSee this post by Steven V. MIller for some additional thoughts on structuring literature reviews. Read Musgrave and Davis on “how to talk about other people”.\n\n\n3.2.2 Draw it out before you write\nIn class you will be introduced to the practice of drawing a causal diagram to think through the theoretical assumptions underlying a paper’s argument. This is a good thing to do for your thesis as well as you prepare your literature review and argument portion.\nFor instance, suppose that there is one or two key theoretical traditions in the literature that you are reviewing. In my party cues example, these might be instrumental approaches based on rational choice theory (people follow party cues as rational shortcuts based on feelings of trust) and social identity theory (people follow party cues because they identify with the party and are motivated to be good group members). It can make sense to draw out a causal diagram for each theory linking the IV and DV in an attempt to think through what the theory takes as necessary for the IV to affect the DV (i.e. what are the mechanisms) as well as whether there are potential moderators that the literature has not examined or alternatively potential confounds. In so doing, you may be able to better identify gaps in the literature and hence see the path forward to writing your literature review.\nThis can also be useful for you in thinking about the analyses that you want to perform. Suppose that you are writing a paper on the subject of political ideology and trust in the legal system. Perhaps you hypothesize that trust will be higher among those on the right-side of the ideological spectrum than the left and find data relevant to examining hypothesis. Okay…what should you control for in your model? Going through the steps of drawing out a causal diagram for yourself might help you further refine your argument about why this relationship should emerge (e.g., through the process of identifying potential mediators of the relationship) as well as helping you think about what the most important confounds of the relationship happen to be (age? education? openness to experience?). You can then discuss why you include particular variables as controls in your thesis. In addition, if there are important confounds that you cannot control for, well, you have now identified some of them and have something to discuss in your conclusion.\nNote: while I think causal diagrams like the ones we’ll use in class are good tools to use, you do not need to then include them in the final thesis.\n\n\n3.2.3 Stating your hypothesis/hypotheses\nExplicitly state clear and testable hypotheses. What you expect may be implicit, or even clear, based on what you have written. But, that isn’t always the case from experience; what is clear in your mind may not be clear to the reader. So, explicitly state your hypotheses. This is preferably done outside the confines of a paragraph, e.g. below it (see the image in Chapter 1 for a better example).\n\nHypothesis 1: Blah blah blah.\n\nYou should not include the rationale for the hypothesis in the statement of the hypothesis. For instance, you might be tempted to write: “Hypothesis: Y will increase alongside X because of these following reasons”. But, you should have already discussed those reasons in the foregoing paragraph(s) so there is no need to reiterate them in the hypothesis. Focus the hypothesis on the specific thing being tested.\nMake sure that your hypothesis/hypotheses is clear and falsifiable. Instead of “Hypothesis 1: There will be a relationship between X and Y”, one could state “Hypothesis 1: As X increases, so will Y” or something like that. This is better because it gives the specified direction of the relationship. It is clear what would falsify the expectation (i.e. a null relationship or even a negative one).\nYou do not need to state the null hypothesis. The only exception here is if the null is something other than “zero effect of the IV”.\nIf you write a hypothesis that implies multiple tests, then I think you should probably break it up. So, for instance, you might write: “Hypothesis 1: Voter turnout will increase alongside education, age, and social network size”. Now, if your intent is to consider the joint influence of these three things (i.e. you are comparing an old person with lots of education and a big network to young people with little education and a small network) then that may be fine. But, probably you are instead intending to independently test these relationships (i.e. is turnout higher among older people?; is turnout higher among the college educated?; is turnout higher among those with bigger social networks?). In that case, then separate them into distinct hypotheses (H1, H2, and H3) to be more clearly communicate your expectations.\nAs a final note, it is probably not a good idea to provide the hypotheses in a separate “Hypotheses” section (and definitely not in the Research Design section). Doing so can break the connection between argument and hypothesis and make it more difficult to understand why you’re claiming what you’re claiming.\n\n\n3.2.4 Focus on what is important\nYour analyses will involve running a regression of some type (linear, logit, etc.) with the goal of attempting to uncover the relationship between X and Y while avoiding, as far as possible, worries of omitted variable bias unless you are analyzing experimental data. In so doing, you will want to include various control variables. You will need to tell us about those variables. The trick here is giving the reader just enough detail about what is being included and why without giving so much that the reader’s attention wanders.\nYou do not need to discuss what control variables you intend to use in this portion of the thesis. Instead, focus your attention on advancing a clear and persuasive argument for why X \\(\\rightarrow\\) Y. The influence of other variables can and should be discussed here to the extent that they shed light on that relationship, e.g., moderating variables (variables that augment the relationship between X and Y).",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Review of Literature and Argument</span>"
    ]
  },
  {
    "objectID": "writing_04_design.html",
    "href": "writing_04_design.html",
    "title": "4  Research Design/Data Section",
    "section": "",
    "text": "4.1 Goals\nThis section of your thesis has a fairly straightforward goal: telling your reader why your analyses take the form that they do (e.g., why this data, why these measurements, why this model, etc.). I have noticed that students often structure this section in very…interesting ways. So, I will discuss how best to structure this area of your thesis and some other supplemental points below.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Research Design/Data Section</span>"
    ]
  },
  {
    "objectID": "writing_04_design.html#basic-structure",
    "href": "writing_04_design.html#basic-structure",
    "title": "4  Research Design/Data Section",
    "section": "4.2 Basic Structure",
    "text": "4.2 Basic Structure\nHere is a general guideline on how to structure this section:\n\nData: Where does the data come from/how was it created?\nDependent Variable: How are you operationalizing your dependent variable(s) using this data?\nMain Independent Variable(s): How are you operationalizing your main independent variable(s), i.e., the predictor variable(s) specified in your hypothesis/hypotheses? This also covers a discussion of any moderators although you should discus the main IV first and then the moderator.\nModel and Controls: How are you actually going to test your hypothesis/es? What variables are you including as controls while doing so (and why them)?\n\nThis structure is my general recommendation and it should apply to nearly all theses in my BAP. However, I could see some room for deviations in situations where multiple data sources are being used in the analyses. The above, for instance, works great if you’re using data from something like the World Values Survey or European Social Survey by itself. But, your particular project may require a combination of datasets. If this involves simply adding some data to an existing survey dataset (e.g., merging data on economic growth from the World Bank into your European Social Survey dataset), then I would keep the general structure the same as above and talk about the source of the added data when you introduce the variable in question (e.g., in the Main Independent Variables or Control variable sections).1 On the other hand, your project may require you to combine multiple databases together into a wholly new data source.2 If that were the case, then I would skip the first bit on Data and discuss the source of each variable as you introduce it.\n1 See the Statistics I R Book for how to “join” datasets (weblink).2 As an example, a student in a prior year wrote a BAP predicting hate crime prevalence in the United States based on economic conditions and various other predictors. The student combined data on hate crimes from the US Department of Justice, data on economic conditions from another source, and so on, into a new dataset.Note: It generally makes sense to have these discussions as separate subsections with bolded section headers. See Chapter 1 for an example.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Research Design/Data Section</span>"
    ]
  },
  {
    "objectID": "writing_04_design.html#some-more-specific-points",
    "href": "writing_04_design.html#some-more-specific-points",
    "title": "4  Research Design/Data Section",
    "section": "4.3 Some More Specific Points",
    "text": "4.3 Some More Specific Points\n\n4.3.1 Discuss(ing) how the data was generated\nYou are probably using a secondary data source: the American National Election Studies, the World Values Survey, and so on. Per the structure above, you should probably begin this section of the thesis by telling your reader(s) about this data source. A stronger discussion here goes beyond simply saying “I’m using ANES data” to tell the reader a little bit more about how the data was actually generated by the people running these surveys. How as the sample generated? (Probability-based sample? Convenience?) How were people interviewed? (In-person? By phone? Online?). When was the data collected specifically?\nThe answers to some of these questions may be important for understanding your results and potentially their limitations when it comes to generalization. For instance, perhaps the survey is done using a face to face interview. If so, are any of your core variables “sensitive items” that may invite (some) people to hide their true preferences due to social desirability pressures? If so, then then what does that mean for your results? Does it make it easier or harder to find evidence consistent with your hypothesis(es)?\nBuilding on this last point, you will want to discuss potential “limitations” with your data and how they may affect your results (see also, Chapter 6). Where should those discussions occur? I would generally say that it is okay to preview this discussion in this section of the paper with a fuller one in the Conclusion since you will want to incorporate into this discussion some thoughts on how the “limitation” in question might be addressed by other researchers (and what might happen if they did so).\n\n\n4.3.2 Assumption checks\nYou should check the assumptions underlying the particular model that you are using. The results of these assumption checks can be presented in an Appendix (which does not count against your overall word count) and referenced in the Model and Controls sub-section. I would recommend not bogging this discussion down with too much detail about the procedures you took and their results as you can leave that for the Appendix. Instead, you would want to clearly and efficiently communicate that you checked the assumptions and (1) they all more/less checked out so you did not need to take any additional steps or (2) you identified some issue, in which case you’d note how you updated or altered your statistical model to address it (e.g., the inclusion of a squared term, you used some other type of standard error, etc.).\nAn additional point about this Appendix: you should include an actual discussion of the results of your assumption checks. In other words, don’t just plop down some graphs or tables and leave them as is, but instead also include a short interpretation of those results. Treat the assumptions appendix much you would the analyses in text - with polished tables/graphs and clear discussions.\n\n\n4.3.3 Talking about models and control variables\nYou should tell your reader how you are analyzing your DV.\nThe first component of this discussion is a note regarding the specific type of statistical model that you are using and why you have chosen this one. This generally does not require very much elaboration as the type of analysis is going to be determined the nature of your DV (i.e., continuous DV = OLS, binary = logistic). So, a simple proviso is all most of you will need: “I use an [model] because my dependent variable is [scale]”. Of course, your specific project may use other types of data that may entail some other modeling strategy. Maybe your DV is categorical in which case you need something like a multinomial logistic model (see SECTION for more on this). Perhaps you have a “clustered” dataset, in which case you would need to do something to deal with that and make a note of what that “something” happens to be (see SECTION for more on this topic).\nYou also need to tell the reader what else (besides your main IVs of interest) is going into this model to predict the DV. In particular, you need to tell the reader what you are controlling for in your model(s) and sufficient information about how they are coded/scaled so that your reader can understand your regression output. In general, I would recommend leaving the in-depth details about the measurement (e.g. question wording) and coding of control variables to an Appendix with a reference to this appendix in the main-text. It’s still a good idea to have some mention of the direction of the variable’s coding in the main text (e.g., higher values on the variable = ?) though. This information could be a simple bullet point list like so:\n\nIdeology\n\nRespondents were asked the following question: “In general, how do you place yourself on this scale from 0-10 where 0 = left, 10 = right” (mean = X; SD = X).\n\nEducation\n\nRespondents were asked to indicate the highest level of education that they had finished with the following options: (1) less than secondary education; (2) secondary; (3) tertiary; (4) etc. I created a binary version of this variable where 0 = less than secondary (X%) and secondary and 1 = tertiary and post-collegiate degrees (X%).\n\nVariable 3\n\nDetails\n\n\nThe idea here is to give the reader sufficient information so that they can understand your model without bogging down the discussion with potentially superfluous details. The appendix is one way to thread that needle.\nAs an additional point here, it makes sense to include a short point about why you are including these particular variables as controls. That is, justify their inclusion in some way. Typically, we control for variables to reduce bias in our models - we are worried that the relationship between our main variable of interest and the DV is actual spurious in nature because this relationship occurs due the correlation between them and confounding variables. A justification on this front may highlight a reason (such as previous studies) suggesting that the control might cause both the DV and the main IV of interest in the model with a reference to relevant work to give warrant to the claim.\nI discuss additional questions and issues in relation to statistical modeling that you may encounter in the final section of this book.\n\n\n4.3.4 Don’t use SPSS/R variable names in-text\nYou may be tempted to refer in text to one of your variables by the name you gave it in R (e.g. something like “lrscale” to capture ideology). Do not do this. You are only forcing your reader to remember more things and do more work, which in the process may confuse matters for them. More work for a reader means a more disagreeable reader. Instead, refer to the concepts that these variables are trying to measure or operationalize.\nAnother version of this is the tendency to talk about the minute procedures you undertook to analyze the data, e.g. importing the data into R from an .csv file or something like that. Unless these procedures required some out of the ordinary effort this is almost certainly unnecessary as it is unlikely to convey any useful information. Don’t waste words!\n\n\n4.3.5 Provide descriptive data!\nWhen you have your data ready to analyze you will probably want to just jump straight into multivariate models. Resist this temptation. Instead, get to know your data. How much variation is there in your key variables? Are they skewed? Are there potential outliers? What are the bivariate relationships between your core variables (and particularly your main IV and the DV)? Doing so may help you identify potential issues that you will need to address as well as giving you a deeper understanding of your data that can help you better discuss you results.\nWhen you turn to writing your research design and results sections, it is a good idea to present/discuss some of these descriptive results to give the reader a sense of the underlying data, although avoid bogging the discussion down with too many minute details. At the very least, you should provide some descriptive data about your main IV(s), any moderator, and your DV. This would typically focus on their mean and measure of dispersion (e.g., standard deviation or confidence interval for continuous variables) or frequencies (for binary or categorical variables). This could also include some bivariate analyses (e.g., contingency tables, correlations), although bivariate analyses between your main variables may be more useful as the start of the analyses section.\nHow should you present this data? Simple summary statistics about a variable can often be communicated in-text. For instance, you might write something like:\n\nWe use two questions as dependent variables. First, respondents were asked about their level of democratic satisfaction (“how satisfied are you with the way democracy is working in [country name]”) with four response options (very satisfied, somewhat satisfied, not too satisfied, and not at all satisfied). We rescaled this variable to range from 0-1 with higher values indicating more democratic satisfaction (mean = 0.518 [95% CI: 0.513, 0.522]). Second, respondents were asked whether their country’s political system “needs to be completely reformed, needs major changes, needs minor changes, or doesn’t need to be changed”. We use this question as an indicator for system legitimacy. We rescaled this measure to range from 0-1 but here higher values indicate support for changing the system and hence lower legitimacy (mean = 0.599 [0.594, 0.603]).\n\nYou could alternatively provide this information in a table or graph (boxplot, histogram, etc.) as well.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Research Design/Data Section</span>"
    ]
  },
  {
    "objectID": "writing_05_analyses.html",
    "href": "writing_05_analyses.html",
    "title": "5  Results/Analyses Section",
    "section": "",
    "text": "5.1 Goals\nYou will naturally discuss the results of your analyses after discussing your data. This section is fairly straightforward in that its central goal is to tell the reader what you have found and how this relates to your hypothesis/hypotheses. What follows are some general points on the content of this section.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Results/Analyses Section</span>"
    ]
  },
  {
    "objectID": "writing_05_analyses.html#some-more-specific-points",
    "href": "writing_05_analyses.html#some-more-specific-points",
    "title": "5  Results/Analyses Section",
    "section": "5.2 Some More Specific Points",
    "text": "5.2 Some More Specific Points\n\n5.2.1 Structure in the same order as your hypotheses\nIf you have more than one hypothesis, then you should discuss them in order: analyses relevant to H1, then analyses relevant to H2, etc. It may make sense in this case to create subsections for each test, but that is not always necessary.\n\n\n5.2.2 Restate your hypotheses when you start discussing them\nRemind your reader about your expectations when you start discussing your analyses and be clear about what would fit with those expectations in the analysis/es you are showing.\nSo, to return to the running example of whether accusations of ulterior motives undermine the influence of party cues, I might begin this section by writing something like:\n\nI argued in Hypothesis 1 that receiving an in-party cue should result in greater policy support among partisans. Table 1 provides the results of a regression model where policy support is regressed receiving a in-party cue or whether they did not (baseline condition). A positive coefficient would thus be consistent with Hypothesis 1. And, indeed…\n\nThen, I might write something like this when discussing a secondary hypothesis:\n\nTable 1 shows that party cues mattered. However, in Hypothesis 2 I argued that this effect would be conditional, i.e. it would be smaller when the party was described as motivated by ulterior goals. I test this claim by including an interaction term between Variable1 and Variable2; see Model 2 in Table 1. A negative coefficient on the interaction term would be consistent with my hypothesis. And, indeed, Table 1 shows…\n\nDoing this makes sure you and your reader are on the same page.\n\n\n5.2.3 Go beyond “statistical significance”\nIn interpreting your results, you might be tempted to just say “well, there is a statistically significant (or insignificant) effect of the independent variable” and leave it at that. But this does not tell the reader very much. And, indeed, it elides the more important question of substantive significance as one could obtain an estimate that one is confident is more than statistical noise but which is unimportant.\nWhen writing up your results, discuss the direction and magnitude of results with some level of specificity. One way to do this is to interpret the direction and magnitude of the effect implied by the coefficient in your model. So, for instance, if the coefficient is 0.8, you might say: “for every one unit increase in my variable, the DV is expected to change by 0.8 [DV units] on average”.1 It if often a good idea to then supplement such a statement with predictions from the model: “Based on this model and its results, we would expect that someone at the minimum of X to score a 2 on the scale, while someone at the maximum of X would score a 6” (or something like that). One can also do this with logistic models using average marginal effect estimates and/or predicted probability estimates. Incorporating a figure to show these changes is also a wonderful idea although it is not always needed.\n1 Well, if the IV is continuous. If it is a binary or categorical variable, then you would discuss this as a difference in means test: is the mean in category A different from the mean in category B? And, of course, we would not use quite the same language when describing a logistic regression model since the coefficients of that model are on the hard to communicate log of the odds (logit) scale. A focus on average marginal effects and predicted probabilities would be more useful there.2 Moreover, a statistically significant relationship does not necessarily mean that your hypothesis is true. Maybe your DV is actually causing your IV (e.g., “reverse causality”). Or perhaps you have an omitted variable that confounds the relationship. Meanwhile, an “insignificant” result could result from measurement error or a small sample.One thorny subject here is substantive significance. The p-value associated with a regression coefficient might be small leading you to “reject” the null hypothesis. But, that doesn’t mean the effect in question is particularly important! We can precisely measure small effects with enough data after all.2 It is thus a good idea to give some consideration to whether the effect that you are observing is “meaningful” or “substantively” important. What characterizes a “substantively” important finding, however, is not always clear. This can depend on the topic in question and our prior knowledge about it. Consider a study investigating variation in the percentage of votes cast for an incumbent party that finds that a one unit change in X leads to an expected gain of 1% more votes. If the incumbent party in the context of this study typically wins elections and does so by 45-50% on average, then a 1% change due to X is probably irrelevant in the “real world”. However, if real elections are typically within 1-3% on average, then a 1% change due to X could mean a different winner and hence a represents a “substantive” or “important” effect! So, there is no straightforward answer here. This is one reason why I suggest incorporating predictions from your model into the discussion of your results as these may aid in interpreting substantiveness in light of your knowledge of this case and the literature regarding it.\nDiscussing the substantive importance of coefficient can be tricky. I discuss this point in more detail in a chapter in the final section of this book.\n\n\n5.2.4 Discussing Control Variables\nYou’ll likely have various control variables in your model (unless you are analyzing an experiment). How much attention should you give them in your discussion of results? The answer is “probably not much if any”. Recall that the standard justification for why we “control” for other variables is to avoid statistical bias when estimating the relationship between the predictor variable we care about and the DV. In other words, we have some reason to believe that the “control” variable causes both our main IV of interest and the DV such that failing to include it in the model would yield faulty conclusions about the relationship between that main IV and the DV. We are not including the control variable in the model, in other words, because we actually care about its direct relationship with Y…so why should we then spend a lot of time talking about it? Indeed, every word you spend on talking about the “controls” is a word you could be using to better set up your research question, or justify your theoretical argument, or elaborate on the implications of your results, i.e., the stuff your readers will most care about.\nThere is an additional reason why you might want to limit your attention to the control variables: they don’t necessarily tell you what you think they’re telling you (which can lead your discussions into error-prone directions) and what they are telling you may not be very interesting on its own.\nConsider the following simplified causal diagram:\n\n\n\n\n\n\n\n\n\nThe diagram above shows a set of theorized causal relationships wherein a person’s immigration attitudes (DV) are influenced by their own educational attainment (main IV) and a potential confounder (Parents Education). Parents Education is thus theorized to influence Immigration Attitudes and to do so, at least partially, because of its influence on a person’s Own Education. If our interest is on obtaining an unbiased estimate of the relationship between Own Education and Immigration Attitude, then we should include Own Education and Parents Education in our resulting statistical model.3 The coefficient for Own Education would tell us about the relationship between Own Education and Immigration Attitudes “controlling for” or “after adjusting for” Parents Education. In essence: if we compared people with different educational backgrounds but whose parents had the same educational attainment, then what is the difference in immigration attitudes that we’d expect to see?\n3 There are plausibly other confounds out there of the education/immigration relationship, but I am keeping the figure simple. Hainmueller and Hopkins provide a relatively recent review of the literature on immigration attitudes.What would the coefficient for “Parent Education” represent in such a model? Well, it would provide us with an estimate of the relationship between this variable and the DV that is unrelated to “Own Education” (e.g., the influence of this variable after adjusting for differences in Own Education). Stated differently: it is an estimate of the relationship between “Parent Education” and “Immigration Attitudes” that emerges due to other mechanisms/mediators besides “Own Education”. For instance, it might represent the influence of Parent Education on Immigration Attitudes that works through a person’s ideology, or their self-interest, etc. (unless those things are also controlled in the model!). It is thus not an estimate of the total relationship between Parents Education and Immigration Attitudes but the relationship that is left over after addressing one of the mediator’s of this variable’s relationship with the DV. This coefficient is thus a “biased” estimate of the total relationship between this variable and the DV. Specifically, it suffers from what is known as post-treatment bias. Any resulting discussion of this coefficient that does not take this fact into account would thus fall into error…but why risk error if this topic is kind of besides the point to begin with?\nSo, in general, I would tell you to focus on just the variable(s) relevant to your hypothesis/es and interpret them. There might be some reason to deviate from this rule if something really surprising or odd turned up, but that is something we would need to discuss with each other.\n\n\n5.2.5 Talking about R2\nStudents writing the BAP can sometimes seen quite preoccupied with the R2 of their model and whether it is “too low” or not. You probably don’t need to worry much about that and, even, have to talk about the R2 at all. Let me explain why.\nThe first reason why talking about R2 is not usually necessary concerns the purpose of what you’re doing. Regressions can be used to try and build strong predictive models, e.g., to build a model to try and make accurate electoral forecasts. In that context,R2 may be quite relevant as a model that doesn’t explain much variation in existing data may generate inaccurate predictions for new data.4 However, your goal is likely more focused on using a regression model to say something about a particular variable and its relationship with Y: to describe that relationship and, contingent on research design considerations and assumptions about omitted variables, to say something about whether X causes Y. R2 is not particularly relevant for that latter purpose. The more important things to focus on are the coefficients for the variables core to your hypotheses, the predictions they generate, and the (un)certainty surrounding them (e.g., confidence intervals and standard errors).\n4 Although, the reverse is also true. A highly predictive model with one set of data might not generate accurate predictions on a different data set!Second, R2 has some potential issues that have led some to question its validity as a measure of model fit to begin with. See here for a deeper dive.\n\nIt assumes a linear model, which means that you can get a very low R2 even with a very strong relationship between X and Y if the relationship is non-linear and you have failed to account for that aspect of the data.\nYou can theoretically get a “good” or “high” R2 with a silly model. For instance, if I were to regress a person’s height on the height of their parents (as measured before the parents had the child), I’d probably get a decent R2 value. Height, after all, is one of the most heritable traits we have. If I were to reverse that regression, I’d still get a good R2, even though a child’s height cannot go back in time and cause their parents’ height! Or, as an alternative example, if I were trying to explain a person’s vote choice, I could survey them five minutes beforehand and ask them whom they intended to vote for. Including that variable in a resulting model would certainly lead to a very high R2, but it wouldn’t tell me anything about why they voted for the party/candidate they voted for because the high R2 is emerging because you’ve included an IV in the model that is essentially the same as the DV!\nR2 values are inherently sample dependent. If I field a survey and estimate a model and then you field a separate survey with the same exact measures and run the same exact model we’d expect the R2 to be non-identical (although, hopefully, quite similar).\nFinally, what counts as a “high” R2 can be field specific. R2, in essence, tells you how much systematic variation in Y you are capturing with your models versus residual noise…but some Y variables are simply noisier than others. Individual human behavior is much more noisy than, say, the behavior of molecules under controlled conditions. A discussion of whether R2 is high or low must then proceed from knowledge of how much variation researchers can typically explain with their models in the domain of study rather than as a blanket statement.\n\nR2 statistics can have some value. In particular, they are helpful in comparing models using the same data. If you run one model and then another which includes an additional variable, then it can make sense to talk about how this did or did not increase the variance explained. However, to go back to point one, even that is not necessarily the relevant consideration.\nThe point is not to never talk about this statistic. Rather, you shouldn’t use it as a heuristic for judging the quality of your, or other’s research or, at least, use it by itself to make those judgments. Research quality is dependent on the clarity of theory and its relationship to existing knowledge, the nature of the sample, how variables are measured, and what goes into the model. R2, as well as other model fit statistics, can be useful in this endeavor, but only as one piece of evidence.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Results/Analyses Section</span>"
    ]
  },
  {
    "objectID": "writing_06_conclusion.html",
    "href": "writing_06_conclusion.html",
    "title": "6  Conclusion",
    "section": "",
    "text": "6.1 Structure\nIn the Introduction, your goal is to motivate a research question and then preview your answer to that question, how you tested that argument, and what you found. Something like this (although, obviously, the format here will depend on your question, argument, data source, etc.)\nYour conclusion should concisely do the above but as a jumping off point for a discussion of the limitations and broader implications of your results. A potential structure could be:",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "writing_06_conclusion.html#structure",
    "href": "writing_06_conclusion.html#structure",
    "title": "6  Conclusion",
    "section": "",
    "text": "Many people say X. But, we actually see Y. Why is that? Well, I argue that X occurs when Z is high but does not when Z is low. I tested this argument using evidence from Data Source. I first regressed Y on X and found that that Y happens, much as earlier work shows. But, when I include Z as a moderator, I found support for my argument. This has important implications for something which I discuss in the conclusion.\n\n\n\nReview of what you’ve done\n\nIt is important to understand whether parties can dominate public opinion. I argued that party influence is conditioned by X. I showed via some method that this indeed appear to be the case. While cue taking occurred when X was absent, their power diminished when X was present. In the remainder of this conclusion, I discuss limitations of this study and potential avenues for future work.\n\nLimitations and future work\n\nI used a one time experiment, but party messages are experienced over time. We could study this in this way. We might find this which would have this implication.\n\nBroader implications\n\nSome think party cue taking threatens norms of democratic representation, but that’s only a problem if people follow them unthinkingly and I’ve shown that’s not an issue. This means X for democracy.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "writing_06_conclusion.html#be-specific",
    "href": "writing_06_conclusion.html#be-specific",
    "title": "6  Conclusion",
    "section": "6.2 Be Specific",
    "text": "6.2 Be Specific\nOne thing you are trying to communicate here is that you have reflected on your project; that you have critically thought through your methodological decisions and their implications for your results and how others should relate to them. Another is to expand on why your study has some further interest for other people.\nOne important point I’d like to communicate here: be as specific as possible when you discuss the limitations of your study. Many times students say “well, I focused on X cases which might be problematic” and leave it at that. However, a comment like that is really just the start of the conversation. Indeed, it could very well be the case that the relationship between X and Y is basically the same across contexts! For instance, consider this paper which examines the generalizability of experiments commonly done in the US in a multiple of countries and basically finds that treatment effects are consistent across context (at least, for the experiments and contexts they are looking at). You may even think this is the case in your own example - in which, you should make that argument! “One potential limitation is that I only looked at Brazil. But that isn’t as concerning as we might think!”\nGo one step further and consider the specific problems that might arise (and why!) based on the theory (or theories) you have been working with and how they could be addressed in future work. Doing so shows a higher level of analytical thinking whereas simply plunking down a “eh, it’s a single case” type comment just reads as if you are ticking off a box and will almost certainly receive a “the conclusion discusses limitations but should go further” comment from me on the evaluation box.\nTo return to the example about party cue taking that I have used throughout this book, let’s say that this study used very old data for some reason. I might then write:\n\nA recurring debate in the literature on party cues is why people follow the party line. I argued that they do because of X. I then used the best available evidence to test this argument, where I found Y. In the remainder of this conclusion, I’ll talk about some potential limitations in these analyses and questions this all raises. [New Paragraph:] One potential limitation is the time frame of my study, which focuses on party cue taking in the year 1981. That is potentially problematic because political parties in the US have polarized since then and partisans have better sorted themselves into party camps. This may matter if this means that partisans instead have stronger group motivations to follow the party, which would mean that my results understate the influence of cue taking. Future studies could address this by doing X. … [New Paragraph] This study has important implications for party competition. To the extent that cue taking is reduced when Z occurs, then this means that elite partisans have little ability to influence the public given that Z is quite prevalent. As a result, worries over elite manipulation should be reduced. This is good for democracy (we think).\n\nOne reason why students sometimes do not elaborate here is because they are machine-gunning out potential issues (“and this could be an issue and this and this and this”). Two well fleshed ideas are much better than seven superficial ideas. Focus and elaborate.\nConclusions can be weirdly hard to write in no small part because you have to take a step back, critically reflect on what you have done, but then project forward to other time periods, or contexts, or experimental instruments, or whatever. So, do not rush this section! Give it the time it deserves.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "writing_06_conclusion.html#dont-only-be-negative",
    "href": "writing_06_conclusion.html#dont-only-be-negative",
    "title": "6  Conclusion",
    "section": "6.3 Don’t Only Be Negative",
    "text": "6.3 Don’t Only Be Negative\nYou should highlight the potential limitations of your study. However, remember to also remind us why this paper is interesting in the first place! What’s good or interesting about this project? Why should we care? This is something you can communicate in the first paragraph of the conclusion per the outline above (‘this study addresses an important question not fully examined by others’, etc.).",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "writing_07_general.html",
    "href": "writing_07_general.html",
    "title": "7  General Writing Advice",
    "section": "",
    "text": "7.1 Outline first\nIn general, it is a good idea to begin with an outline of what you want to write. This will let you start to feel out your argument, and flexibly revise it, without the pressure of having to compose perfect sentences.\nA potential outline might look like this (although, the contents will obviously vary based on what you are writing):\nIn filling in the outline for each paragraph, be sure to keep in mind that each paragraph should be organized around a clear, and singular, point or idea. A goal of the thesis, and indeed all writing, is to clearly communicate ideas. One writing tic that may undermine this goal is over-stuffing paragraphs with too many ideas or points. Instead, focus each paragraph on a main idea with successive paragraphs building on each other to form an overarching argument or point. Steven V. Miller’s tip’s for students page has an example of what I am talking about here that is quite useful.\nThe following is an example of what I mean. It stems from my efforts to puzzle through an idea I had that I was going to write up as a paper for a research workshop. As you can see, such an outline is a start to writing - there are places where I have placeholders (“Final Sentence”), for instance - and its contents are rather barebones. But, it provided me with a start!",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>General Writing Advice</span>"
    ]
  },
  {
    "objectID": "writing_07_general.html#outline-first",
    "href": "writing_07_general.html#outline-first",
    "title": "7  General Writing Advice",
    "section": "",
    "text": "Introduction\n\nFirst Paragraph: Main point\nSecond Paragraph: Main point\nThird Paragraph…\n\nLiterature Review & Theory Section(s)\n\nPotential Sub-section 1\n\nParagraph 1…\n\nPotential Sub-section 2\n\nMethods\n\nData Source: Where does the data come from and why this data source?\nDependent Variable: How do you measure your dependent variable?\nIndependent Variable(s): How do you measure your dependent variable?\nModel: Any relevant details on how you will analyze the data.\n\nResults\nConclusion\n\nParagraph 1…\n\n\n\n\n\nIntroduction\n\nParagraph 1: Promises aren’t believed\n\nPoliticians make promises on the campaign trail.\nThey seem to mostly keep them\nHowever, the mass public doesn’t believe this\nThe public largely distrusts politicians and believe them to be liars and not promise keepers\nCampaign statements are often non-credible\nThis poses some challenges to broader accounts of democratic representation and operation that place promissory representation at their core - how can the public hold elites accountable if they don’t believe them in the first place?\n\nParagraph 2: Moral language might increase credibility\n\nWhat would make a politician’s statements more credible?\nOne possibility highlighted in recent work is the use of moral language.\nWhat is moral language\nPoliticians and parties do sometimes use this language\nSimas and Clifford show that general moral language (of this sort) does lead to more perceived sincerity\n\nParagraph 3/4: This paper’s contribution(s)\n\nWe contribute to this work in two ways.\nFirst, we consider targeting. (maybe switch to language of matched morals rather than targeted? easier to talk about mis-matched?).\nImportant for this reason.\nSecond, we focus on competence. speaker credibility is based on two components: beliefs about the intentions/motives of the speaker (sincerity, trustworthiness) and their competence or ability to perform the action in question.\nCompetence is important for this reason.\n\nParagraph 4 or 5: This paper’s argument\n\nWe argue…\nTargeted &gt; non-targeted when it comes to sincerity & matched; targeted = more perceived similarity, so there is a halo effect (?); moral character stuff\nTargeted &lt; non-targeted when it comes to sincerity & mismatched; backlash effect (but is that what the feinberg stuff shows?)\nWhat about competence? More of an open question.\nOn the one hand, if more committed then more likely to work on it.\nOn the other hand, if more commiteed then less likely to compromise and perhaps less likely to have influence!\n\nParagraph 5 or 6: roadmap?\n\nWhat comes next.\n\n\nReview/Theory\n\nParagraph 1: Position Taking is central …\n\nCommon models of democratic politics grounds electoral competition in the programmatic offerings of parties and candidates\nFor instance, promissory theories of representation are based on the idea that parties and candidates make policy promises during election season; voters choose based on those promises; and then parties/candidates are held accountable at the next election based on whether they upheld those promises.\nThis process is iterative however with partisan elites potentially changing their positions over time in response to prior electoral performance and in an effort to anticipate the behavior of future voters (Mansbridge, Erikson, Adams etc.)..\nRegardless of whether this is a backwards or forwards process, policy is central to elections and representation.\n\nParagraph 2: But contested?\n\nParties and candidates spend a good deal of effort cultivating policy positions and publishing party manifestos documenting how they will act in office.\nAnd, yet, the actual role of policy positions on voter choices is contested.\nStudies of proximity voting suggest a role for voter positions, but one that is perhaps limited relative to more long standing concerns such as partisan predispositions or group loyalties. (Warshaw, etc.; maybe footnote Lenz – Of course, this assumes that voters have exogenous preferences to begin with, which is a quite problematic assumption - Kuklinski, Druckman, Lenz, Disch)\nChanges in party positions, on the other hand, do seem to matter but perhaps mostly over time and only to a small extent.\nPolicy is central to how we talk about elections and democratic politics, but it is not always clear whether it should be or not. (too strong?)\n\nParagraph 3: What explains this disconnect?\n\nWhat might explain the lack of more robust evidence about party positioning and voter policy positions?\nThere are a variety of potential answers to this including the (contested) possibility that only some types of issues matter (e.g., strong attitudes, issue importance, moralization; something on moralization) or that party strategies to remain ambiguous may muddy the waters (Somer-Topcu; etc.).\nAnother reason might be that the policy statements of parties and candidates may lack credibility in the eyes of audience members.\nParty position taken occurs in a context wherein many people distrust parties and politicians, believing them to only care about electoral success and willing to pander.\nSuspicion about the motives of partisan elites, perhaps stoked by other actors, may thus undermine the influence of party position taking (Adams, pandering minds, my papers).\nOn the other hand, party positions may be more influential in contexts wherein audience members believe the position to be more credible as when parties take unpopular positions as this is read as going against the party’s interests and hence serves as a costly signal about the party’s true intentions (Vazquez; see also Berinsky; Lupia?).\nFinal sentence\n\n…",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>General Writing Advice</span>"
    ]
  },
  {
    "objectID": "writing_07_general.html#write-and-edit-edit-edit",
    "href": "writing_07_general.html#write-and-edit-edit-edit",
    "title": "7  General Writing Advice",
    "section": "7.2 Write and Edit, Edit, Edit…",
    "text": "7.2 Write and Edit, Edit, Edit…\nHaving an outline in hand first helps clarify what you actually need to write…which is a handy thing to know before you start writing! However, it is, of course, just a first step. You then need to start writing and, important, editing and rewriting as well. Your first draft is almost certainly your worst draft in terms of clarity for the reader. Revisit your writing as much as possible. If possible, have someone read a draft of your thesis and ask them for feedback regarding anything that is unclear or doesn’t make sense.\nHere is some advice for the editing process. Musgrave and Davis provide additional points of value in the following blog posts: Simple Tricks for Better Prose; Show and Tell\n\nProofreading for Clarity\n\nRead the entire paper, checking for any awkward or unclear passages. For any such passage that you find, ask yourself what point you want it to convey. Write the point as if you were explaining it to a friend. Then revise to make it formal.\nBe wary of starting sentences with demonstratives that are not connected to a noun (e.g. “This results in”, “These can affect,” etc.). Sentences such as these invite the question: what is “this” again?\nAvoid using the passive voice if you can help it.\nAvoid beginning paragraphs with “However” or similar clauses. “However” implies a contrast (This thing, however this other thing)…but that would suggest combining the two points into a single paragraph. Either do that or rewrite so that you quickly reintroduce the idea you’re setting yourself up against.\nFinally, make your sentences as short as possible. Doing so will force you to write exactly what you mean. It will also prevent complicated sentence structures from obscuring the meaning of your words. One way to accomplish this task: rewrite sentences with multiple clauses separated by commas and try to eliminate as many commas as possible by rearranging the clauses.\n\nProofreading for Content\n\nPut yourself in the shoes of an intelligent reader who is unfamiliar with the topic. Make sure your argument will be clear to such a person and that you have not left any parts of the argument or evidence unstated or assumed them to be common knowledge.\nOne way to make sure you have done this is to write or revise as if you have a curious (and unusually literate) five-year-old child peering over your shoulder. Your evidence should speak to the series of “but why?” questions that this child will inevitably ask you. The logic and facts you present, what you cite from the sources you use, are the best tools you have to convince that child of your argument.\nDo not assume that you reader(s) know everything about the topic you’re writing about.\n\nProofread for Grammar and Miscellaneous Errors\n\nPolish your writing. Make sure there are no grammatical mistakes, no typos (remember to look for missing words, too), no repetitive words/wordings, and no missing or incorrect citations.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>General Writing Advice</span>"
    ]
  },
  {
    "objectID": "writing_07_general.html#some-other-questions-and-topics",
    "href": "writing_07_general.html#some-other-questions-and-topics",
    "title": "7  General Writing Advice",
    "section": "7.3 Some Other Questions and Topics",
    "text": "7.3 Some Other Questions and Topics\ncan you use first person! Yes! “This paper argues” is nonsense. You argue! Own the argument.\navoid tables that overlap pages",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>General Writing Advice</span>"
    ]
  },
  {
    "objectID": "writing_07_general.html#use-a-reference-manager-to-ease-in-text-citing-and-reference-list-building",
    "href": "writing_07_general.html#use-a-reference-manager-to-ease-in-text-citing-and-reference-list-building",
    "title": "7  General Writing Advice",
    "section": "7.4 Use a reference manager to ease in-text citing and reference list building",
    "text": "7.4 Use a reference manager to ease in-text citing and reference list building\nI recommend using a citation manager to help simplify the task of referencing as much as possible. I personally use Zotero but have experienced with Mendeley which also works fine. Either tool can help you insert citations in-text and automatically populate bibliographies for your essays.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>General Writing Advice</span>"
    ]
  },
  {
    "objectID": "part_research.html#footnotes",
    "href": "part_research.html#footnotes",
    "title": "Doing Research",
    "section": "",
    "text": "Well, four is probably more accurate since you also have to write and edit the thing. See the first section of this collection: “Writing the Thesis”.↩︎",
    "crumbs": [
      "Doing Research"
    ]
  },
  {
    "objectID": "doing_01_research.html",
    "href": "doing_01_research.html",
    "title": "8  Finding Literature",
    "section": "",
    "text": "8.1 Strategies for Finding New Sources\nIf the substantive seminar is a tree trunk of knowledge on which you need to build, then that just raises the question of how you should go about doing that. Here, I’ll discuss three strategies for gathering new sources of research for potential inclusion in your thesis. These strategies should not be thought of as exclusive to one another. Rather, each might be useful at different points in the research process and can be used profitably together.",
    "crumbs": [
      "Doing Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Finding Literature</span>"
    ]
  },
  {
    "objectID": "doing_01_research.html#strategies-for-finding-new-sources",
    "href": "doing_01_research.html#strategies-for-finding-new-sources",
    "title": "8  Finding Literature",
    "section": "",
    "text": "8.1.1 Open Search\nFirst, one could begin with a somewhat open-ended search. This is what Miller talks about as “to [just] ‘Google it’”. (I discuss Google Scholar as a research tool below.)\nSuppose that you are interested in understanding public opinion about reactions to terrorist attacks, perhaps guided by a nascent question such as “do terrorist attacks lead people to support their national government more”. One could go to the university library’s website (or, per below, Google Scholar) and enter in a keyword related to that topic (“terrorism”, “terrorist attacks”, etc.) and see what comes up.\nThis can be especially useful at the very beginning of your research endeavors when you’re just trying to get a sense of what might be out there. However, it may pose a problem of surplus - that is, you are very likely to find LOTS AND LOTS of things come up that may not be especially well tailored to your particular interests. That surplus can, paradoxically, be a negative for you at the beginning of the search process since you may feel like you’ve been thrown into the middle of an ocean without knowing which way to swim.\nFor instance, here are the first several entries I found on Google Scholar when using the phrase “terrorist attacks” as my search term:\n\nThe second article by Leonie Huddy seems a relevant, albeit now somewhat old, resource. Huddy is a prominent public opinion researcher and the article clearly concerns public opinion on the subject.1 However, the other articles (save maybe the first one) look less relevant to my original question.\n1 This article is actually a specific type of research article that is published by the journal Public Opinion Quarterly, in which the authors canvass existing survey data to give a descriptive view of public polling concerning a topic.One thing I could do to better my odds of finding relevant work is to refine my search in various ways (choosing a different term, adding more keywords, asking for results from particular journals, etc.). Indeed, I would recommend doing just that if and when you engage in this type of research strategy: start with a keyword but limit your search to selected journals relevant to our class (see Section 8.3 below). For instance, here I add “public opinion” to my query to try and weed out less relevant work and restrict my attention to the American Political Science Review:\n\nThese results are much more helpful. I get a couple of articles not on the subject of terrorism (Hopkins, Chong and Druckman), but which may still be relevant for thinking about its effects in some way as they focus on how messages in the media influence public opinion. The other articles in this snapshot are much more directly focused on research relevant to my starting question.\nAn open search of this type can turn up relevant work and, indeed, quite a lot of it if you’re focused on a ‘hot’ topic. Of course, this strategy has its limitations. An open-ended search is a quite iterative process that may take some work to get you where you want to go (e.g., finding the right keywords, or searching through multiple relevant journals). It may thus be better at the very beginning (to get started) and towards the end (to see if you’re missing anything) strategy.\n\n\n8.1.2 Working Backwards, Working Forwards\nThere are two other, more directed, strategies that one might choose here.2 In both cases one would start with an existing piece of research (a journal article, a book, a book chapter) that one finds interesting and on whose topic one might wish to work for the thesis. From this article one could work backwards: start with the reading, look at what it cites (i.e., its references), and then start reading what looks interesting/relevant. An alternative strategy is to work forwards: start with the reading, look at what cites it, and then start reading what looks interesting/relevant.\n2 Or in combination with an open search strategy. Perhaps you begin with an open search, find an article or two that seem interesting/relevant and then branch out from there.Both strategies speak to the potential problem of relevance noted above by using the judgment of established researchers (those that published the original article in the first case, those publishing subsequent research in the latter) to narrow down the teeming mass of published research into a list of studies that are likely to be relevant to the topic you are interested in. As we’ll discuss in the substantive seminar, relying on the advice or actions of those we trust can sometimes be a sound route to effective decision making.\nBoth of these strategies are sound methods for getting into the research literature on a particular topic. However, there are some issues to think about.\nFirst, the backwards strategy could bias your literature review toward old (and potentially less relevant) research if your starting point is itself somewhat older. For instance, if you pick an article on reactions to terrorist events that was published in, say, 1985 then the research it is citing will likely involve cases that may be less relevant for understanding public reactions to more recent events. And, of course, it may lead you to ignore more recent theoretical and empirical advances on the topic. This need always be the case, but it is a potential risk; see Miller’s’s discussion on focusing research on relatively newer research.\nSecond, it says nothing about either the mechanics of actually going out and finding this literature or about how to sift through the likely large array of resources either strategy will produce. The next two sections consider these topics.",
    "crumbs": [
      "Doing Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Finding Literature</span>"
    ]
  },
  {
    "objectID": "doing_01_research.html#two-useful-tools",
    "href": "doing_01_research.html#two-useful-tools",
    "title": "8  Finding Literature",
    "section": "8.2 Two Useful Tools",
    "text": "8.2 Two Useful Tools\n\n8.2.1 Google Scholar\nGoogle Scholar is a search engine for academic research (you may already have some familiarity with it from your other classes). You can search for a keyword (“immigration”, “populism”, etc.) as well as for particular articles and books. It is thus a powerful tool for all three strategies above. You can then access these materials either directly (in some cases researchers make a copy available) or via the Leiden University library. See below for information on how to set up Google Scholar with access to the library.\nWhen you look up an article you will see something like the image below (staying with the terrorism example, but dropping the specific focus on the American Political Science Review):\n\nThe first entry that comes up is an article by Davis and Silver that focuses on public opinion concerning the trade off between civil liberties and security introduced by new public policy passed into law after the September 11 terrorist attacks in the United States. There are two particularly relevant links here that I want to draw your attention to: “Related Articles” and “Cited by”.\nRelated articles is a list of articles Google has decided are related to this one. This may be a first way to delve into a topic in greater detail. In practice, you are relying on Google’s algorithm to sort articles into more/less relevant…which may work fine in most cases. Indeed, in this case, Google lists a variety of other articles on this particular topic that all strike me as highly relevant for someone doing research on public opinion concerning terrorist attacks (albeit work with a decidedly US skew), as this snippet shows:\n\nAlternatively, one can click on “Cited by”, which will naturally take you to a list of books and articles that cite the original research. Here, is a snippet of that screen for this article:\n\n“Cited by” is a powerful method for researching “forward”. However, it may also pose some issues when starting with highly cited articles such as this one: lots of people cite it even when not working on terrorism! One can, of course, simply start skimming titles to find the relevant stuff, but that can be a bit time consuming if the originating article in question has a lot of citations.\nGoogle Scholar has an additional trick up its sleeve that may help here owing to the fact that this is Google we’re talking about: search. As the left hand column in the image shows, one can restrict this list of citations to a particular date range to help manage a potential excess of resources. In addition, if you click on the “Search within citing articles” box, then you can perform a second search on this list of articles using keywords that may be more relevant for your interests, e.g., by searching for particular countries, topics, etc. Perhaps I am particularly interested in the case of Germany; I might then click that box, add Germany to the search bar, and see what comes up:3\n3 I also added “public opinion” to further refine the search. Of course, I could have added something even more specific here depending on my interests, e.g., “tolerance”, or “prejudice”, or “voter turnout” or whatever.\nNow I have a little bit more to work with. I have the original article and some leads on research that may be more relevant for my own research (assuming, here, that I may want to understand something about German public opinion in particular).\n\n8.2.1.1 Connecting Google Schlar to Leiden University’s Library Resources\nOne can use Google Scholar to not just find research materials but also to access them. There are two ways to do this. First, journals and researchers may provide links to the materials themselves. For instance, in the most recent image, the link “[PDF] sagepub.com” would bring me to the pdf of the article provided by the journal that published the research in question. Researchers themselves may also provide links to articles hosted on their own websites, although note that sometimes these are ‘pre-print’ editions (i.e., versions that were submitted for publication and which might not exactly match the final published version). It should go without saying, though, that you should be careful when clicking on random-looking links on the web!\nThe second route is via Leiden University itself. You may have noticed in the images above the “GetIt@Leiden” option. Clicking on that link will take you to Leiden University’s library page for the article/resource in question and, from there you can access the material.\nTo take this second route you must first add the university as a resource in the settings in Google Scholar. First, access the website’s settings page via three horizontal lines in the upper top-left corner of the website. Then go to “Library Links”, search for “Leiden” in the search box, click on Leiden and select the box for “Universiteit Leiden”. Hit save and then everything should be good to go.\n\n\n\n\n8.2.2 Connected Papers\n[Note: I originally prepared this information prior to the Fall 2021 version of the BAP. Since then Connected Papers has added an account system. One can create two of the graphs discussed below without an account per month and five per month with a free account. It appears that one can largely bypass these limitations via the use of alternative web browsers and/or private/incognito modes, although I’m not sure if all functionalities will be offered without an account. This may limit the tool’s usefulness for you somewhat, but I still think it’s a cool resource that could be used sparingly to flesh out your research efforts.]\nGoogle Scholar, much like Google itself, is an ocean of information. It may be somewhat difficult to find what you want as a result. One new resource that may help with this is Connected Papers.\nMuch like Google Scholar, you can search for a given article (using its title or its digital object identifier (DOI)). What Connected Papers then does is search through articles to try and identify those that are on the same topic based on similarity in each article’s references. The logic is that two articles that cite the same things will likely be on the same topic.\nLet’s check out the Davis and Silver paper that we turned up in the section above. This is as easy as typing in the name and then double clicking on the article.\n\n\n\n\n\nHere is what the output page looks like. You can double click on this image to zoom in on it. The graph can be directly accessed via this link.\n\nThe middle part provides a graphical display of research in the form of a network. Here is how to read the graph:\n\nEach circle represents a different piece of research with our starting article seen in the middle or so of the graph. The size of the circle is related to how many times the piece of research has been cited: larger circle = more highly cited.\nThe shading of the graphs indicates when the research was published - darker shaded articles were published more recently.\nThe circles are connected by lines. The proximity of the circles to one another indicates the similarity in the paper’s citations and, presumably, topical focus.\n\nThere look to be three or so clusters of studies centered around Davis and Silver 2004 (our starting paper). The papers to the north of it focus on attitudes regarding political tolerance and civil liberties. The papers very very close to it (Sullivan 2009, Hutchison 2007) are focused specifically on the relationship between civil conflict/terrorism and tolerance, while those further north are about tolerance more generally. The other two clusters focus on the concepts of authoritarianism (the cluster to the right with Lubbers 2008 in it) while the cluster to the bottom focuses on terrorism and threat; these are both concepts that come up in Davis and Silver. Not only has Connected Papers given me some potential sources to look into on the very specific topic I began with (terrorism and civil liberties) but it has also given me a path toward exploring related concepts and ideas that I might need to make reference to in building up my ideas about this topic. It is also pretty easy to investigate those papers; as the right-hand part of the figure shows, for instance, I can obtain the abstract for the paper, and Google Scholar links, by clicking on the circle for the paper in question.\nConnected Papers also provides two additional areas: “Prior Works” and “Derivative Works”. The former tab provides an overview of the works that are highly cited by works displayed in the graph and which might thus be counted as “seminal works” within this field.\n\nWe also have Derivative Works: papers not in the graph but which cite many of those that are.\n\nThe nice thing about Connected Papers, in my opinion, is that it can reveal to you in one go a big (but manageable) chunk of a broader research literature. If I were writing a paper on terrorism and civil liberties (or, perhaps, one of those component concepts), for instance, most of the articles represented in the image with the graph above would indeed be the core of my literature review as they represent some of the most important work on this topic over the past two decades. I would not want to stop with them, but they would nevertheless provide me with a solid core with which to work. That might not be the case in other examples, of course. In this example I chose a highly influential article, for instance. Nevertheless, Connected Papers can help one get into a research literature on a topic quite quickly.",
    "crumbs": [
      "Doing Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Finding Literature</span>"
    ]
  },
  {
    "objectID": "doing_01_research.html#sec-research-focus",
    "href": "doing_01_research.html#sec-research-focus",
    "title": "8  Finding Literature",
    "section": "8.3 Okay…but what should I focus on?",
    "text": "8.3 Okay…but what should I focus on?\nOkay, so you’ve begun researching via one of the methods above. In doing so, you have likely turned up a large or large-ish number of things to read. How do you pick what to read and focus on? There is likely no “right” answer here, but let’s talk about some strategies.\nFirst, one could try and read everything you have found. This would certainly give you a deep knowledge of the literature. However, nobody can do that and certainly not someone working on your timeline. Thus, out of necessity, you will have to use some heuristics (a concept we’ll cover in the course) to try and cut through the noise, albeit with the risk of creating bias in your search.\nOne natural shortcut is algorithmic and has been the topic of the sections above: use Google or Connected Paper’s algorithm to identify a smaller subset of articles and then read through them. This isn’t a bad idea. However, we don’t necessarily know what goes into Google’s decision making in deciding what is relevant. In addition, one might still need to use judgement to parse out relevant and irrelevant resources as I did in the example above regarding Connected Papers.\nA second cue you can use is prestige. This is a natural cue as humans use prestige all the time to decide whom to pay attention to and learn from. There are two relevant signals of prestige that one can look to here. First, one can look at citation counts. The Davis and Silver article from above, for instance, has been cited nearly 900 times, which is quite a bit for a single paper. That signals that many other researchers think this is an important article. This may create some potential biases though. The natural bias is temporal; recent articles will be cited less simply because they are more recent, so a citation based judgement rule may lead you away from important recent work. That bias, at least, can be rectified by going back to Google Scholar and looking for more recent work on the topic.\nA second signal of prestige is the journal in which the article is published. The blog post by Miller linked to at the beginning of this document discusses this strategy in some more detail. Here are the journals I’d think to be quite relevant for our class (with acronyms in parentheses):\n\nGeneral Interest Journals\n\nGeneral interest journals are those that publish research from across the various sub-fields of political science. These are typically the most prestigious journals in the field.\nThe Big Three: American Political Science Review (APSR), American Journal of Political Science (AJPS), and the Journal of Politics (JOP)\nOthers of particular relevance: British Journal of Political Science (BJPS), Political Research Quarterly (PRQ), Perspectives on Politics\n\nJournals that focus more explicitly on public opinion, communication, and behavior\n\nPublic Opinion Quarterly (POQ), Political Behavior, Political Psychology, Political Communication, Comparative Political Studies, Electoral Studies, Journal of Experimental Political Science, International Journal of Public Opinion Research\n\nNon-political science journals that may be relevant\n\nIn Psychology: Journal of Personality and Social Psychology, Psychological Science\nIn Communication Studies: Journal of Communication, Communication Studies\nIn Sociology: American Journal of Sociology, American Sociological Review, Social Forces\nIn Economics: American Economic Review\n\n“Super” general interest journals\n\nThere has been some very interesting and important work published by political scientists in journals that cover “science” more generally (i.e., journals that also publish research in biology, physics, etc.). Hence, my calling them “super” general interest journals.\nExamples: Science, Proceedings of the National Academy of Science of the United States (PNAS), Nature: Human Behavior\n\n\nThe above should not be taken as meaning that journals not on this list are irrelevant or bad. Indeed, there are likely some not listed above that may be especially relevant for your research question. Nor should it be taken to mean that the research featured in the above journals is necessarily better or even good; that is a judgment that you have to make. Nevertheless, it isn’t a bad idea to focus your attention on work published in these journals first.",
    "crumbs": [
      "Doing Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Finding Literature</span>"
    ]
  },
  {
    "objectID": "doing_02_data.html",
    "href": "doing_02_data.html",
    "title": "9  Finding and Evaluating Data",
    "section": "",
    "text": "9.1 Study Design and Interpretation\nThe books, book chapters, and articles that you read during the seminar and in preparation of your thesis project will often feature both descriptive claims (e.g., claims about the extent of political knowledge in a particular country) and causal or explanatory claims as well (e.g. that political knowledge causes political tolerance). One important task is to consider the validity of these claims; you should not presume that a reading on a course syllabus, or one published by an academic journal or publishing house, is the final word on the subject as science is cumulative, research is fallible, and new conditions or contexts may require revision of what existing research leads us to expect.\nIn thinking about the claims made by a piece of empirical research you should think about the following questions:\nHere are some resources to aid you in thinking about these questions. Of course, you can also ask me for my thoughts as well!\nThe foregoing focuses on the logic of research design - on why researchers might collect data in the way they do and the potential consequences of research design on the findings that result. Understanding a study’s findings also depends on reading the regression tables and figures present in the article as well, something which may feel overwhelming at times. Here are some useful resources for helping you interpret research:",
    "crumbs": [
      "Doing Research",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Finding and Evaluating Data</span>"
    ]
  },
  {
    "objectID": "doing_02_data.html#study-design-and-interpretation",
    "href": "doing_02_data.html#study-design-and-interpretation",
    "title": "9  Finding and Evaluating Data",
    "section": "",
    "text": "Is the underlying theory logically coherent?\nAre the specific empirical claims made by the resource plausible given this theory and other things we now about the world?\nHow was the data generated? (who was surveyed? what experimental conditions are involved? how are key constructs measured?)\nWhy did the researchers opt for these methods (and, crucially, what choices did they make that could have been made otherwise?)\nHow confident should we be about an author’s findings given these design choices?\n\n\n\nEvidence in Governance and Politics (EGAP)\n\nThis website features several short and accessible guides for thinking about research design and causal inference. Their particular focus is on experimental work, but much of what they discuss is relevant in non-experimental settings. See their Methods Guides section.\nSome pages of particular relevance:\n\n10 Things to Know About Causal Inference\n10 Strategies for Figuring Out if X Caused Y\n10 Things to Know About External Validity\n\n\nResearch Design in Political Science by Dimiter Toshkov\n\nThis is a very nice book about the nitty gritty of doing research - on how to think about concepts, the nature of causal inference, and how to do empirical research (and what problems might arise when doing so). It thus goes a bit deeper than the shorter EGAP guides and covers all steps in the research process as well.\nThe following chapters are particularly relevant: “Theory in the Research Process”, “Concepts and Operationalizations”, “Measurement and Description”, “Experimental Designs”, and “Large-N Designs”\n\nSurvey Research\n\nMuch of what we know about how ordinary people think about politics comes from survey evidence (sometimes paired with an experiment, oftentimes not).\nThe political scientist Adam Berinsky provides a nice discussion of some of the important choices that survey researchers must make when designing and implementing surveys and how these choices may influence what can learn about politics in this article.\nPew Research is a major polling firm that performs surveys around the world (although more so in the United States). Not only might they be a valid source of data for your project, they also provide some resources for learning more about survey methods. Their Methodological Research talks about specific issues in survey research (e.g., debates about how best to measure gender or the difficulty of asking questions about religion in China) that may be interesting although not necessarily directly relevant for you. On the other hand, their YouTube page provides a nice series of short videos on survey methods that may be helpful in understanding survey research more generally.\n\nExperiments\n\nYou may be interested in experiments as a method for understanding the world and perhaps even as a method to be employed in your own project.1\nIf you want to learn more about experiments, then I heavily recommend reading through the Toshkov chapter on the subject and skimming through the book Experimental Thinking: A Primer on Social Science Experiments by James Druckman to better understand the possibilities on offer.\n\n\n1 Students in my BAP have, in the past, fielded survey experiments for their projects. I am certainly open to this as a method. If you are interested in this, then I’d ask you to keep in mind the issue of time and feasibility. Collecting your own data does add some steps to the thesis writing process - you will need to come up with the experimental protocols, create the survey (using, for instance, Qualtrics), field the survey, and then analyze it. This can be doable in the short time span available to you (especially if you are replicating/extending an existing experimental design), but does require you to be a bit more ‘on the ball’ in terms of time management. If you’re interested in doing this, then you should talk with me as soon as possible to discuss your ideas.\n\nReading a Regression Table: A Guide for Students by Steven V. Miller\n10 Things to Know about Reading a Regression Table by EGAP",
    "crumbs": [
      "Doing Research",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Finding and Evaluating Data</span>"
    ]
  },
  {
    "objectID": "doing_02_data.html#data-resources",
    "href": "doing_02_data.html#data-resources",
    "title": "9  Finding and Evaluating Data",
    "section": "9.2 Data Resources",
    "text": "9.2 Data Resources\nYou will need data for your thesis projects. The type of data required is determined by your research question and theory. However, I suspect that many of you will make use of existing data sources such as existing surveys and administrative data. How can you identify (and then access) useful data for your projects?\nThere are three resources you can use for figuring out what data to use for your thesis project.\nThe first are the various readings you have done in the course of your review of the literature related to your topic. They are a natural first place to begin as they are, of course, on the same topic as your own thesis. As you read and research, it may make sense to create a spreadsheet to keep track of the various data sources used in the myriad articles you read and what appears to be in them. This may have the benefit of also revealing limitations in existing work (or, at least, the work you have reviewed). For instance, if you notice in your spreadsheet that most of the data comes from European data sources, then this could help you motivate the use of non-European data as part of your answer to the ‘so what’ question for your thesis.\nA second resource are the various websites linked below in the “Data Repositories” sub-section. ICPSR and GESIS are both data repositories wherein researchers have deposited their data. Each enables you to perform a keyword search that may enable you to find datasets connected with your thesis topic. The “dataset with political datasets” webpage, meanwhile, is a very helpful one-stop shop for finding links to a wide array of relevant data sources in political science.\nFinally…you have me! If you’re having trouble finding something, ask me for advice. I may be able to guide you to something that is of relevance to your project, although no guarantees are provided here!\n\n9.2.1 Data Repositories\n\nA Dataset with Political Datasets: This webpage provides links to a huge array of datasets, including both mass level public opinion surveys as well as data on elites and institutions. You can even download the list as a spreadsheet that includes additional data about the data (meta!), such as what regions the resource covers, what time frame, and so on. This is a great place to begin. Scroll down to “Citizens” and take a look.\nGesis : A data repository run by the Leibniz Institute for the Social Sciences. Particularly relevant if you are using Eurobarometer data (see Section 9.2.3 section below) or European Election Studies data.\nICPSR : Another data repository, this one run by the University of Michigan\nPew Research: A leading public opinion firm with polling data available worldwide. See here on how to access their data.\n\n\n\n9.2.2 Some acronyms to know…\nSome surveys tend to come up quite a lot in our discussions. I may refer to them by their acronym due to their seeming ubiquity. Here are some examples:\n\nANES: American National Election Studies.\nWVS: World Values Survey\nEVS: European Values Survey\nESS: European Social Survey\nCSES: Comparative study of Electoral Systems\nEES: European Election Studies\n\n\n\n9.2.3 A Note on the Eurobarometer\nYou may be interested in public opinion within Europe. One useful resource might then be the Eurobarometer, which provides access to a series of surveys in EU member states going back quite a long time. For instance, you might be interested in media use and EU attitudes and head to their website to search for relevant surveys. Let’s say you turn up their Media & News Survey (2023) and think it would be a perfect fit for your project. Scrolling down that page, you might find this link:\n\nFantastic - you’ve found a survey and its data! Surely, all you have to do is follow that link and you can download the relevant data. … Well, not quite. Following that link will give you access to a variety of excel spreadsheets with data in them:\n\nHowever, those data files are not the ones that you will want. These data files are Excel spreadsheets that provide summary statistics for the survey, e.g., the % of people within a particular country and age range that give a particular answer:\n\nThis is almost certainly not what you want as it’s not the underlying raw individual-level data. Even if you are interested in explaining variation at the country-level (e.g., why might the percentage saying yes to a question vary between countries rather than between individuals), the files provided via the link above would be a pain in the ass for doing that because data is organized into separate files for each country and the files themselves are badly organized for use in R (or some other statistical program).\nIf you want to use Eurobarometer data, then you instead need to go to the GESIS data repository and search there for the specific survey that you’re interested in (or, perhaps, simply for “eurobarometer” - although from there you’ll need to find the one you want from the multitude of Eurobarometer surveys). Here is the page for the survey used in this example. It provides access to the raw data, questionnaires, and other documents (under Downloads). Note that you need to create a free account to access these files.\n\nSo, you Eurobarometer interested folk have been warned…\n\n\n9.2.4 R Packages for Accessing Data\nOne way to obtain data is to go to the website for a survey (or, perhaps, a repository that contains its data), download the relevant data file(s) to you computer, load them in to R, and begin working. However, some data sources can be directly accessed via R packages. Here are some relevant examples:\n\nvdemdata\n\nInstalling this package will download the most recent Varieties of Democracy (V-Dem) data files.\nYou can find a vignette on how to use this package here\n\nWDI\n\nYou might need/want to access data from the World Bank (e.g., data on a country’s unemployment rate). This package enables you to query the World Bank’s databases from R and then download the data directly without needed to visit their webpage. (In practice, I’d recommend using the World Bank’s website to help you search for the relevant indicator and then using the package to download it.)\nSteven V. Miller provides a helpful vignette on how to use this package.\n\nOur World in Data\n\nThis package enables access to the data provided by Our World in Data resource.\n\ngesisdata\n\nA resource for accessing data stored on the Gesis repository.\nFrederick Sold provides a vignette discussing how to set up and use the package.",
    "crumbs": [
      "Doing Research",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Finding and Evaluating Data</span>"
    ]
  },
  {
    "objectID": "part_analysis.html",
    "href": "part_analysis.html",
    "title": "Practical Issues in Data Analysis",
    "section": "",
    "text": "Other Resouces\nThe ensuing set of chapters is not a guide to statistics and does not cover all elements of data analysis (e.g., assumptions testing). If you need a refresher on the statistics element, then you should consult Andy Field’s “Discovering Statistics Using IBM SPSS Statistics” and/or OpenIntro Statistics as well as your class notes for Statistics I and II. SPSS users can find videos on how to use SPSS for data analysis at this Kaltura page (and additional written instruction via the .pdf file linked to above). R users can find the videos and overviews from Statistics I and II on this Brightspace page. The Statistics I R overviews have now been collected into a single book here. The Statistics I R overviews are also being collected into a single online resource and I will update this document with a link to it when it is ready. You can, of course, also consult me to supplement these materials.\nThe resources above do not exhaust the potential guides and pedagogical resources available for working with R/R-Studio. Here are some additional resources that you may find useful.",
    "crumbs": [
      "Practical Issues in Data Analysis"
    ]
  },
  {
    "objectID": "part_analysis.html#other-resouces",
    "href": "part_analysis.html#other-resouces",
    "title": "Practical Issues in Data Analysis",
    "section": "",
    "text": "Data Visualization\nYou were introduced to the ggplot package for data visualization in Statistics I/II. This was very much an introduction as the helper materials did not go into everything this series of co[^1]mmands can do. The R Graphics Cookbook provides a much more thorough walkthrough of how to create, and augment, nearly any other type of plot possible with ggplot . Cédric Scherer’s A ggplot2 Tutorial for Beautiful Plotting in R, meanwhile, provides an overview of many of the aesthetic options available in ggplot for creating eye-catching graphics. Finally, Professor Kieran Healy’s book Data Visualization is a great resource for learning about what makes a plot an effective tool for communication in the first place.1\n1 His Bluesky account is a fun read as well.\n\nR Programming\nYou were introduced to the tidyverse in Statistics I/II. One of the most influential resources for working with these tools, and with R in general, can be found in the book R for Data Science by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund. Wickham is one of the progenitors of this library of R packages and on the shortlist for most influential people in the broader R universe. This book is for those who want to push their knowledge of R further and perhaps particularly for those that might be thinking of going into data science or something similar in their post-graduate life.\n\n\nR Markdown\nYou may very well write your thesis using two programs: a word processor (e.g., Microsoft Word or [shudder] Google Docs) for the actual writing and R-Studio for the analyses. However, you could very well write the whole thing in R-Studio as an R Markdown document. The book R Markdown Cookbook by Yihui Zie, Christophe Dervieux, and Emily Riederer is resource is for those bold few who are thinking of doing that as it delves into the wide array of options available to you in doing so.\nOne note here: if you’re writing your thesis in R Markdown, then I recommend that you also install and use Zotero as a citation manager. I recommend using a citation manager in general, but Zotero can be used with R-Markdown documents, making the inclusion of in-text citations and the (automatic) generation of a bibliography section much easier. See this blog entry on how to add citations in R Markdown using Zotero.",
    "crumbs": [
      "Practical Issues in Data Analysis"
    ]
  },
  {
    "objectID": "analysis_01.html",
    "href": "analysis_01.html",
    "title": "10  Analysis Steps",
    "section": "",
    "text": "10.1 What Do You Want to Know?\nThe first step above is not “perform your analyses” but instead “What Do You Want to Know?”. This is because everything depends on this step - what data you should look for and try to use, what types of analyses you perform, and so on.\nWe can think of this step as involving finding an answer to these two questions:\nAnswering these questions is what will primarily occupy your attention in the first 8-11 weeks of the thesis project (i.e., the seminar, proposal writing stage, and post-proposal feedback stage).",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analysis Steps</span>"
    ]
  },
  {
    "objectID": "analysis_01.html#what-do-you-want-to-know",
    "href": "analysis_01.html#what-do-you-want-to-know",
    "title": "10  Analysis Steps",
    "section": "",
    "text": "What is the question?\n\nWhat is your research question? What is it that you want to know about the world?\nFor instance: do young people punish politicians for democratic violations as much as old people?; or: does partisanship promote voter turnout?; or: does social contact reduce prejudice?\nSee Musgrave and Davis on how to ask the “right questions”.\n\nWhat is the claim you want to test?\n\nYou will advance a claim about the world in your thesis that you then seek to test. This claim is your hypothesis (or hypotheses).\nFor instance: young people are less likely to punish non-democratic behavior by politicians; or: partisans are more likely to vote than non-partisans; or: social contact with out-group members reduces prejudice.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analysis Steps</span>"
    ]
  },
  {
    "objectID": "analysis_01.html#pre-analysis-planning",
    "href": "analysis_01.html#pre-analysis-planning",
    "title": "10  Analysis Steps",
    "section": "10.2 Pre-Analysis Planning",
    "text": "10.2 Pre-Analysis Planning\nThe second step is “Pre-Analysis Plan”. This step involves figuring out how to translate your answer to “What Do You Want to Know” into practice. We can also break this down a bit further:\n\nFind a Reasonable Data Source\n\nWhat data or type of data do you need to test your empirical claims and hence answer your question? Can you actually access this data? If you cannot obtain the ‘ideal’ data to test your hypothesis(es), then can you obtain a reasonable alternative?\nThis is commonly a social survey for students in my BAP, but could also be (or laso include) administrative data or an experiment.\n\nIdentity Relevant Variables and Decide How You’ll Treat Them\n\nYou will formulate some hypothesis or hypotheses that you will then test. This test will likely involve a multiple regression of some type in which you predict a dependent variable with an independent variable and some “control” variables. You both need to find a data source with reasonable measurements for the concepts relevant to this statistical model and also figure out how they should be handled in the analyses (e.g., do you need to convert anything into a factor variable, do you need to rescale a variable, etc.).\n\nIdentify Relevant Methods of Analysis\n\nWhat type of statistical model is most appropriate for analyzing your dependent variable? The answer here generally follows from the nature of the dependent variable (e.g., continuous \\(\\rightarrow\\) OLS model, binary \\(\\rightarrow\\) logistic model), but there may be complications along the way (e.g., your DV may be categorical, or you may have clustered/nested data, etc.).\n\nThink about limitations\n\nAll data sources are “limited” in some fashion, e.g., they only focus on some observations rather than all possible observations, the measurement of some key construct may not exactly match the underlying concept in some way, etc. You should give some thought as you go to the potential issues in your data and how they might specifically affect your results. Does the use of this particular data source (etc.) make it easier or harder to find evidence consistent with your expectations? If so, why and how could we avoid this issue in future work? If not, why not? This is perhaps most relevant for the writing stage of the thesis and particularly discussions in the conclusion.\n\n\nWork on this step should begin during the “What Do You Want to Know” portion but may extend further (i.e., you might not really start focusing on this until Weeks 6-7, say, and not have a full answer until a little bit after this). However, you should have a good idea of the data source that you’ll use by mid-November or so and a sense of what you’ll need to do to translate this raw data into your analyses shortly thereafter so that you can, well, begin the analysis portion. The first ‘feedback’ deadline in my BAP is timed for around this point and I recommend/expect a reasonably clear research design section that communicates these points (and especially the data source and nature of the main variables) by this time frame.\nI have an arrow pointing from this box back to “What Do you Want to Know”. You should not decide your research question or hypotheses based on the data available to you. At the same time, feasibility is an important element of a good thesis. A common saying in graduate school is “the only good dissertation is a finished dissertation”. You can identify an interesting and novel question, and advance a plausible argument/hypothesis regarding it, but if you cannot test it all then the thesis process will grind to a halt. It thus could be a case that you’ll need to revisit some of your answers in the “What Do You Want to Know” step (e.g., reframe the question, change the IV, etc.) as you begin hammering out the details here. Ideally this would be done in consultation with me so that I can provide you guidance in this process.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analysis Steps</span>"
    ]
  },
  {
    "objectID": "analysis_01.html#data-cleaning-and-creation",
    "href": "analysis_01.html#data-cleaning-and-creation",
    "title": "10  Analysis Steps",
    "section": "10.3 Data Cleaning and Creation",
    "text": "10.3 Data Cleaning and Creation\nAfter you have decided on what data you need and what variables from this dataset you’ll use (and how), then you can start cleaning the data in preparation for subsequent analyses. By “data cleaning”, I mean removing missing value codes (if necessary), recoding variables, creating new ones, etc. One thing to keep in mind: this will probably be the most time consuming element of the actual analyses. A regression model can be run in seconds on a modern computer…but cleaning the data could take much longer.\nIf you need to create your own dataset in some form (e.g., merging multiple databases together, collecting data for an experiment), then this would also happen at this point in time followed by data cleaning.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analysis Steps</span>"
    ]
  },
  {
    "objectID": "analysis_01.html#descriptive-analyses",
    "href": "analysis_01.html#descriptive-analyses",
    "title": "10  Analysis Steps",
    "section": "10.4 Descriptive Analyses",
    "text": "10.4 Descriptive Analyses\nYou might then be tempted to jump into a statistical model…stop! You should first get to know these variables and especially the ones core to your hypothesis(es).1 In other words, begin with some univariate analyses (mean + measure of dispersion [SD, confidence interval] for continuous variables, tabulations/frequency table for binary/category variables, etc.) to better understand your data. This might suggest to you the need to make some type of transformation or recoding of the variable. Beyond that, it may help you turn up errors in the data cleaning process before you progress to a more fully realized model and commit yourself to an interpretation of its output. And, finally, this step is important because you should provide this information somewhere in your thesis (see Chapter 4).\n1 Some of this may come about in the Data-Cleaning and Creation stage. For instance, you may need to factorize a categorical variable and, in so doing, decide which category will be the reference group for your analyses. If the variable is related to a clear hypothesis, then the reference category will likely depend on what comparison(s) makes most sense for testing the hypothesis. For instance, if you have a four-category region variable (North, West, East, South) and are arguing that your DV will be lower in the South than everywhere else, then the South would be the most relevant reference group. (In this situation it might even make sense to make this into a single variable [South vs. Non-South], but hopefully the point is made). Otherwise, you might want to avoid using a category with very few observation in it, but to do that you’d need to do a quick tabulation to check.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analysis Steps</span>"
    ]
  },
  {
    "objectID": "analysis_01.html#statistical-modelinginterpretation-writing",
    "href": "analysis_01.html#statistical-modelinginterpretation-writing",
    "title": "10  Analysis Steps",
    "section": "10.5 Statistical Modeling…Interpretation & Writing",
    "text": "10.5 Statistical Modeling…Interpretation & Writing\nAfter getting to know your data, proceed to bivariate and multi-variate analyses. If your main variables are continuous, perform a correlation and then your fuller statistical model (i.e., a linear regression) (etc.). After you perform your main statistical model, check its assumptions and, if necessary, make changes to the statistical model (e.g., use a different standard error, transform a variable, etc.). Finally, interpret your model and write up the results.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analysis Steps</span>"
    ]
  },
  {
    "objectID": "analysis_02.html",
    "href": "analysis_02.html",
    "title": "11  Some Practical Questions",
    "section": "",
    "text": "11.1 What should I control for?\nYour goal in the thesis is probably going to be to test some hypothesis. For instance, you might hypothesize that partisan animosity (“affective polarization”) will be greater among people with extreme ideologies than centrist ideologies. In other words, people with extreme ideologies might dislike people who identify with a different political party as themselves more than people with less extreme ideologies dislike people who identify with a different political party as themselves. You will naturally want to find data on your DV and IV and fit some type of regression model. A natural question then arises: what else should go into my model? Or: what should I “control” for?\nWe can answer this question by taking a step back and thinking about what we’re trying to do when we include a “control” variable in our model.1 Suppose we perform some bivariate test predicting partisan animosity and affective polarization and find the relationship we expect to find (i.e., more extremity = more animosity). The challenge before us is that ideological extremity is not randomly distributed in society. Extremists probably differ from non-extremists in all sorts of ways - they might have different education levels, social networks, personality characteristics, etc. This consideration raises the possibility that the relationship we observe (more dogmatism = more animosity) is better explained by one or more of these other characteristics that differentiates extremists and non-extremists (e.g., more dogmatism = more animosity & more extremity?). We include “control” variables in an attempt to statistically account for alternative explanations for the relationship we care about. We should thus “control” for any and all potential confounder variables that we can, i.e., variables that we have good reason to think cause both our main IV of interest and the DV.2\nFigure 11.1: A Relationship with Two Confounders\nPer above, you should control for things that you believe cause both your main IV of interest and your DV. You can justify these beliefs in relation to prior evidence and theory. Per Chapter 10, one important part of the thesis writing process (and of doing your analysis) is first doing the hard work of figuring out how you think the world works in relation to your phenomenon of interest as this will help you identify relevant control variables.\nOne thing you should avoid doing is simply dumping variables into your model. This is because you can get in trouble by leaving out relevant variables (as above) but also by including unnecessary variables in your model as well! In particular, you should try to avoid including what are alternatively known as “post-treatment” or “mediating” variables in your model.3 A post-treatment variable is a variable that is caused by your main IV and in turn causes your DV. For instance:\nFigure 11.2: A Relationship with a Confounder and a Mediator\nWhat is the problem here, exactly? Let’s say that our main IV is ideological extremity and our DV is partisan animosity. We might think that some of this relationship is explained by a common source of both variables (dogmatism or cognitive rigidity) and include that in our model. However, we think extremity should still predict animosity (the relationship is not only due to dogmatic thinking). In particular, we theorize that this relationship should emerge because holding an extreme ideology facilitates a variety of reasoning processes that will lead extremists to hold inaccurate beliefs about the other side’s political views.4 Extremists, for instance, might come to see the other side as holding more extreme beliefs than it actually does, feel threatened by this (somewhat imagined) out-group, and hence express more animosity toward the out-group.\nFigure 11.3: Probably Not a Good Idea to Control for Beliefs about Out-Group Beliefs!\nIf we include variables pertaining to a person’s extremity and their beliefs about the out-group in a model predicting their level of animosity toward the out-group, then we will be estimating the “effect” of extremity “holding constant” beliefs about the out-group, i.e., after adjusting for differences in beliefs about the out-group. However, we will be producing a biased estimate of the total relationship between extremity and animosity because we are essentially “controlling away” (part of) the effect we care about in the first place! Including the mediator in our model accounts for part of (perhaps even the entire) reason why extremity matters for understanding animosity thereby leading to a smaller effect estimate than actually exists - our model can only tell us whether there is a relationship between extremity and animosity after adjusting for differences in the mediator. Indeed, if the only reason why extremists differ from non-extremists is because of the effect of extremity on beliefs about the out-group, then our estimate for the effect of extremity in this model might even be 0 (statistically insignificant)!5 You can see this point in practice via Case 4 in this explainer. The upshot is that you should try and avoid including post-treatment variables in your model…if you can. The difficulty is that what counted as a confounder (causes X and Y) versus a post-treatment variable (caused by X, causes Y) may be ambiguous or contested. At a certain level, we may be stuck with assumptions about how the world works to justify our decisions on this front.\nThere can be contexts in which we would want to include the post-treatment variable in the model although they may not apply to your thesis. First, we might actually be interested in whether there is any left-over relationship between our main IV and the DV after accounting for potential mediators. For instance, we might be investigating wealth biases in college admissions.6 We might examine this question by comparing admission rates for students with rich parents vs. those with non-rich parents and find higher rates among the former group than the latter. A skeptic may say that this is not evidence that colleges are biased toward the rich - perhaps family wealth leads to better academic performance (higher grades, higher test scores) and that is what colleges are paying attention to. In this instance, it might make sense to also include measures of performance to see if there is still a relationship between family background and admissions after adjusting for differences in performance (what we might call a “direct” effect of family background since it is not being mediated by performance). Second, we might be interested in testing for mediation: how much of the relationship between our main IV and the DV is accounted for by a mediator? This is difficult to do in practice, and especially so with observational data, but typically would involve including the post-treatment/mediating variable in the model at some point. However, in the context of your study this type of variable is almost certainly a nuisance variable that you do not want to include in the model because doing so will essential ‘control away’ some of the influence of your main IV by controlling for one of its effects, which is probably not what you want to do!\nTwo final thoughts here. First, the foregoing applies when your data is non-experimental. However, if you are working with an experiment then you don’t generally need any control variables in your model. You can simply predict your DV with an indicator for which treatment group the observation has been randomly assigned to. This is the case because random assignment is handling the task of ruling out alternative explanations. Second, the foregoing focuses on situations where the two predictor variables are causing one another (in someway) and also causing Y. What about if the two IVs are unrelated to one another while both predicting the DV? Including both predictors in the model would be a good idea in this scenario but for other reasons than discussed above. Instead of reducing bias in our estimates, doing so would reduce variance in our estimaets (i.e., it would lead to more certainty); see Case 5 in this explainer. However, this is mainly of interest in experimental studies wherein we know for sure that our main IV (assignment to different experimental groups) is unrelated to other variables (due to random assignment and measuring the other variables prior to assignment).",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Some Practical Questions</span>"
    ]
  },
  {
    "objectID": "analysis_02.html#what-should-i-control-for",
    "href": "analysis_02.html#what-should-i-control-for",
    "title": "11  Some Practical Questions",
    "section": "",
    "text": "1 The focus here is on the inclusion of additional predictors in a context where we are trying to generate an unbiased estimate of the relationship between a particular IV and a DV. We could also be building a statistical model for the sake of prediction, e.g., to construct an accurate forecasting model. The considerations about what to include in such a model would be different as the focus would be on what best reduces predictions errors from the model as a whole rather than what reduces bias in one particular estimate.2 Of course, there may be (likely are) important confounders out there that we cannot measure or perhaps simply do not have a measure of to include in our model. We should thus be careful not to over-interpret statistical models of observational data as definitively telling us that our IV of interest causes the DV - it is better to talk about these relationships as associations between the variables.\n\n\n3 There is yet another potential source of bias from improper variable inclusion: collider bias. Let’s say we have this model: Y = X1 + X2. Collider bias emerges when one of the variables on the right hand side of the equation (X2, for instance) is caused by the other two variables (i.e., X2 is caused by Y and X1). Collider bias can lead to a spurious association much like omitted variable bias when a relevant confound is excluded.\n\n4 Inaccurate “meta-perceptions” (or, beliefs about others beliefs) are indeed an important predictor of group animosity (see, for instance, Moore-Berg et al.) and perhaps also support for political violence (see, for instance, Braley et al.)\nper Moore-Berg a\n\n5 See this article for additional discussion of this type of situation.6 This is a real-life example based on the following news article.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Some Practical Questions</span>"
    ]
  },
  {
    "objectID": "analysis_02.html#if-a-control-variable-is-not-statistically-significant-should-i-remove-it-from-my-model",
    "href": "analysis_02.html#if-a-control-variable-is-not-statistically-significant-should-i-remove-it-from-my-model",
    "title": "11  Some Practical Questions",
    "section": "11.2 If a control variable is not statistically significant, should I remove it from my model?",
    "text": "11.2 If a control variable is not statistically significant, should I remove it from my model?\nNo.\nYou should use theory and evidence to justify what variables go into the model. A “statistically insignificant” variable does not mean the variable does not ‘cause’ the DV or even that it doesn’t matter for you. Insignificant predictors could emerge, for instance, due to poor survey wording (measurement error) or low sample size even in a case where there is a ‘real’ relationship between the two variables. And, per above, the rationale for including the variable in your model is that it is a plausible confounder of a/the variable you actually care about. If those assumptions are correct then we may very well expect to see a null relationship between (some of) the controls and the DV since you are controlling for its potential mechanisms (i.e., your main IV)!",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Some Practical Questions</span>"
    ]
  },
  {
    "objectID": "analysis_02.html#how-many-variables-should-i-can-i-include-in-my-model",
    "href": "analysis_02.html#how-many-variables-should-i-can-i-include-in-my-model",
    "title": "11  Some Practical Questions",
    "section": "11.3 How many variables should I, can I, include in my model?",
    "text": "11.3 How many variables should I, can I, include in my model?\nThere is a basic mathematical ‘basement’ here for OLS models: n &gt; k. n is the number of observations you have while k is the number of coefficients. A bivariate model has two coefficients (intercept and slope) while multiple regression models would have as many added slope terms as there are additional predictors. The number of observations you have must be greater than the number of variables you’re including in the model.\nOkay, but probably you’ll have anywhere from 25 to 10,000 observations in your dataset. So, is it okay to include anywhere from 20 to 9500 variables? Not necessarily. Too many variables for too few observations can lead to an issue known as overfitting. There are some general rules of thumb here though that you may see suggesting that you should have approximately 10-15 observations per model term. A constant + 2 IVs? You’d want 30-45 observations at least. Of course, rules of thumb are not always iron clad.\nThe general aim should probably be for parsimony: “Everything should be as simple as it can be, but not simpler”.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Some Practical Questions</span>"
    ]
  },
  {
    "objectID": "analysis_02.html#what-should-i-do-with-ordinal-data",
    "href": "analysis_02.html#what-should-i-do-with-ordinal-data",
    "title": "11  Some Practical Questions",
    "section": "11.4 What should I do with ordinal data?",
    "text": "11.4 What should I do with ordinal data?\nStatistics II focuses on OLS regression and logistic regression. The former is meant for the analysis of continuous (interval/ratio) DVs, while the latter is meant for binary DVs. What about ordinal data though? These impose some complications.\nConsider age in years as a variable. The difference between 18 years old and 19 years old is the same as the difference between 80 years old and 81 years old: 1 year. Each unit increment in age in years cover the same amount of change. Now, consider a standard left-right ideology measure ranging from 0 (“left”) to 10 (“right”). A person who rates themselves a 1 on the scale is less left-leaning than someone who rates themselves a 0. A person who rates themselves a 10 on the scale is likewise less left-leaning (more right-leaning) than someone who gives themselves a 9. The difference in each case is again a single unit on our scale. But do these 1 unit differences really capture the same amount of ideological difference between the groups? This is not quite as clear cut as with the age variable given the abstractness of the scale.\nThe foregoing ambiguity can create a problem for ordinary least squares regression. OLS models assume that the relationship between the IV and the DV is linear: that moving from 18 years to 19 years will bring with it the same degree of change in Y as moving from 80 to 81 years. Or, that moving from 0 to 1 on the left-right measure brings with it the same degree of change in Y as moving from 9 to 10. However, ordinal variables can violate, or at least potentially violate, that assumption insofar as the difference between levels of the variable doesn’t capture the same degree of change across the range of the IV. So, what should you do?\nIf your DV is ordinal, then my recommendation is to simply run an OLS model. The alternative to an OLS model here is an ordered/ordinal logistic model. I’ll discuss fitting such a model in CHAPTER. However, while this is indeed the technically more appropriate model: you were not trained on how to run it, it is more difficult to interpret, there are debates about statisticians about whether it is really a better model, and it usually leads to the same basic conclusions. In running the OLS model, you will be implicitly assuming that the spacing between categories on the DV is indeed equivalent/identical. If you are concerned about that assumption, then I’d recommend that you present the results of both an OLS model and an ordered logistic model; discuss the results of the OLS model; and then note the similarity with (and/or differences from) the ordered logit model. This is a common tactic. I’ll discuss this more in CHAPTER.\nA perhaps trickier business is what to do with ordinal independent variables. One can do two things here. First, one could treat the variable as a categorical variable: create a dummy variable for each level of the variable and then include all but one in the resulting model. This is the safest tactic as it does not involve making any further assumptions about the data (i.e., that the change between categories is equivalent). However, it throws away information about the variable (the fact that it is ordered in nature) and only enables you to formally test the statistical significance of the difference between the included categories and the common baseline category. Second, one could simply assume that the categories are equivalently spaced and treat the variable as ‘continuous’ in nature. This perhaps enables easier interpretations, but does involve that additional assumption and you know what they say about assumptions.\nIn practice you will see both things done and sometimes even in the same model! Here, I would recommend paying some attention to how other researchers use the variable in question. The 7-pt party identity measure in the US is almost always treated as an interval/continuous scale, for instance.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Some Practical Questions</span>"
    ]
  },
  {
    "objectID": "analysis_02.html#what-should-i-do-if-my-dv-is-categorical",
    "href": "analysis_02.html#what-should-i-do-if-my-dv-is-categorical",
    "title": "11  Some Practical Questions",
    "section": "11.5 What should I do if my DV is categorical?",
    "text": "11.5 What should I do if my DV is categorical?\nIf my DV had multiple categories that do not admit of an obvious ordering (e.g.: do you support the Labour Party, Conservative Party, or the Liberal Democrat Party), then a multinomial logit model would be most appropriate. A multinomial logit model is basically just a logit model but one designed for categorical variables with more than two categories. I’ll give an example in CHAPTER.\n\n\n\nFigure 11.1: A Relationship with Two Confounders\nFigure 11.2: A Relationship with a Confounder and a Mediator\nFigure 11.3: Probably Not a Good Idea to Control for Beliefs about Out-Group Beliefs!",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Some Practical Questions</span>"
    ]
  },
  {
    "objectID": "analysis_rscript.html",
    "href": "analysis_rscript.html",
    "title": "12  R Script Files",
    "section": "",
    "text": "12.1 Suggested Structure\nI would recommend structuring the R Script file with different sections that build on one another. The logic behind this ordering is based on the discussion in Chapter 10. For instance:",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>R Script Files</span>"
    ]
  },
  {
    "objectID": "analysis_rscript.html#suggested-structure",
    "href": "analysis_rscript.html#suggested-structure",
    "title": "12  R Script Files",
    "section": "",
    "text": "Header Information\n\nIt is nice to start with some header information indicating what the contents of the file are for. You are probably going to have a single script file for your project so this can be as simple as providing a short description such as “BAP - Data Analysis Script by NAME”. If you had multiple files (e.g., one for data cleaning, one for one type of analysis, one for another) then you could help keep things straight for yourself and others by having that information in the header (e.g., “Data Cleaning Script” or “Regression Model Analyses”).\nSee below for an example.\n\nPackages\n\nThe various packages you’ll need to complete your analysis.\nThe main thing to watch out for are package conflicts. The car package, for instance, has a conflict with the dplyr package within tidyverse - both car and dplyr have a command called recode(). If you load car before tidyverse, then R will use the recode command from the latter package, but if you load tidyverse before car then R will use the recode command from car instead.\n\nData\n\nLoad your data or datasets.\nIf you are loading multiple different datasets, then it might make sense to have a big section that loads the first dataset and cleans it as needed, then another one that loads the second one and cleans it, and then one for joining them, before progressing onto the analyses.\n\nData Cleaning\n\nThen a section where you clean your data as needed (e.g., recoding variables, creating factor variables, etc.).\nThis section might also include syntax for summary statistics and looking at the attributes of the variables as this may be relevant for deciding how to clean the data.\n\nData Analysis with subsections\n\nThen a section for the various analyses you’ll end up doing with sub-sections for different steps in that process (e.g., running the model, checking assumptions, obtaining/plotting predicted values, creating the regression table, etc.).\nThe sample script file that I provide has this set up slightly differently with one section for descriptive and/or bivariate analyses and then one for the regression model and subsequent steps. The specifics here are not super mission critical, you just want to set things up in a clear and logical manner so that you know what is going on and to help others decipher what you’re doing (e.g., if you ask me for help then I’ll want clarity!).",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>R Script Files</span>"
    ]
  },
  {
    "objectID": "analysis_rscript.html#separating-sections",
    "href": "analysis_rscript.html#separating-sections",
    "title": "12  R Script Files",
    "section": "12.2 Separating Sections",
    "text": "12.2 Separating Sections\nHow can you keep things neat and divided? The # symbol is your friend here. The # symbol defines a comment area which R will not try and evaluate as code. For instance:\n\n# Here is a comment - R will ignore the stuff in this greyed out line\n\n## Here is another comment\nsummary(mtcars$mpg)  #and another\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.40   15.43   19.20   20.09   22.80   33.90 \n\n\nHere is a snapshot of the beginning of the example script file:\n\nI have tried to delineate different sections by using the # commenting tool. The very first part, for instance, is the Header (in the terminology from above) - I would replace “Project Information” and “Author” with relevant information for my project/file. I then have a section for Packages and another for Data and so on. I have used multiple hashtags to try and make different sections stand out from one another a bit more.\nA handy tool here is the Outline section. You can see an Outline of the contexts of the script file on the right side of the image; this can be toggled by clicking on the button next to “Source”. The Outline is nice because clicking on an entry within it will automatically take you to that place in the script file thereby saving you from having to scroll around looking for things. You can add something to the Outline by including four dashes (----) at the end of the comment text. You can remove something from the Outline by including a single dash there instead; I do this for the long row of hashtags used in separating sections because otherwise they’d be added to the Outline but listed as “Untitled” thereby cluttering everything up. You can nest sections within one another by using one, two, or three hashtags (see here as well):\n\n# : Section\n##: Sub-Section\n### Sub-subsection",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>R Script Files</span>"
    ]
  },
  {
    "objectID": "analysis_03.html",
    "href": "analysis_03.html",
    "title": "13  Regression Table Formatting Suggestion",
    "section": "",
    "text": "13.1 Something slightly ugly looking\nThis example will focus on producing a regression table showing the results from three regression models. The IVs are the same in each model, but each one focuses on a different dependent variable. Here is the syntax needed to perform the regressions and create a regression table.\n## Run the models\n#DV = electoral democracy index\nmodel1 &lt;- lm(v2x_polyarchy ~ HDI2005 + Women2003 + Facebook, data = demdata)\n#DV = liberal democracy index\nmodel2 &lt;- lm(v2x_libdem ~ HDI2005 + Women2003 + Facebook, data = demdata)\n#DV = egalitarian democracy index\nmodel3 &lt;- lm(v2x_egaldem ~ HDI2005 + Women2003 + Facebook, data = demdata)\n\n## Create a list\nmodel_list &lt;- list(\n1  \"Electoral Democracy\" = model1,\n  \"Liberal Democracy\" = model2, \n  \"Egalitarian Democracy\" = model3\n)\n\n##Create a table\nmodelsummary(model_list, \n2             stars = T,\n3             gof_map = c(\"nobs\", \"r.squared\", \"adj.r.squared\"),\n4             coef_rename = c(\n               \"(Intercept)\" = \"Intercept\", \n               \"HDI2005\" = \"Human Development Index (2005)\", \n               \"Women2003\" = \"Percentage Female Legislators (2003)\", \n               \"Facebook\" = \"Facebook Use (proportion of population)\"), \n             notes = \"Notes: OLS Coefficients with SEs in Parentheses\")\n\n\n1\n\nProviding a name for the model is a good idea when we have different dependent variables.\n\n2\n\nIncludes asterisks for statistical significance\n\n3\n\nOnly include N, R2 , and Adjusted R2 in the table as goodness of fit statistics\n\n4\n\nGive informative labels for your variables\n\n\n\n\n \n\n  \n    \n    \n    tinytable_yaiw9qzgcq7owiwa92q6\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                Electoral Democracy\n                Liberal Democracy\n                Egalitarian Democracy\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\nNotes: OLS Coefficients with SEs in Parentheses\n        \n                \n                  Intercept                              \n                  0.116  \n                  -0.057  \n                  -0.115  \n                \n                \n                                                         \n                  (0.099)\n                  (0.097) \n                  (0.083) \n                \n                \n                  Human Development Index (2005)         \n                  0.371* \n                  0.415*  \n                  0.505***\n                \n                \n                                                         \n                  (0.166)\n                  (0.162) \n                  (0.139) \n                \n                \n                  Percentage Female Legislators (2003)   \n                  0.006**\n                  0.007***\n                  0.007***\n                \n                \n                                                         \n                  (0.002)\n                  (0.002) \n                  (0.002) \n                \n                \n                  Facebook Use (proportion of population)\n                  0.004* \n                  0.004** \n                  0.003*  \n                \n                \n                                                         \n                  (0.002)\n                  (0.002) \n                  (0.001) \n                \n                \n                  Num.Obs.                               \n                  142    \n                  142     \n                  142     \n                \n                \n                  R2                                     \n                  0.384  \n                  0.463   \n                  0.522   \n                \n                \n                  R2 Adj.                                \n                  0.371  \n                  0.451   \n                  0.511\nThe table above provides the coefficients from each model and then, underneath them, the coefficient’s standard error. Here is my pet peeve: modelsummary() does not include the coefficient and SE in the same cell of the table, which can lead to situations where you get extra spacing between the coefficient and standard error. We can see this above with the “Percentage Female Legislators (2003)” and “Facebook Use (proportion of population)” variables.\nThere is nothing wrong about the output above. I just think it looks kind of ugly. We could perhaps avoid it by specifying a shorter label for the two variables…but that would involve some trial and error to get things right and shortening the label might make it harder to understand the table. But, thankfully, there is another way!",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regression Table Formatting Suggestion</span>"
    ]
  },
  {
    "objectID": "analysis_03.html#solution",
    "href": "analysis_03.html#solution",
    "title": "13  Regression Table Formatting Suggestion",
    "section": "13.2 Solution",
    "text": "13.2 Solution\nWe can avoid the scenario above by changing what modelsummary() puts in the estimate cell. In addition, we’ll have modelsummary() use a different backend for outputting the table to make sure this looks good when we export to Word. Here is some updated syntax:\n\n# The alternative table-formatting package we'll use\nlibrary(flextable)\n\n#Revised table syntax\nmodelsummary(model_list, \n             estimate = \"{estimate}{stars}\\n({std.error})\", \n             statistic = NULL, \n             gof_map = c(\"nobs\", \"r.squared\", \"adj.r.squared\"), \n             coef_rename = c( \n               \"(Intercept)\" = \"Intercept\", \n               \"HDI2005\" = \"Human Development Index (2005)\", \n               \"Women2003\" = \"Percentage Female Legislators (2003)\", \n               \"Facebook\" = \"Facebook Use (proportion of population)\"), \n             notes = list(\"Notes: OLS Coefficients with SEs in Parentheses\",\n                          \"* p &lt; 0.05; ** p &lt; 0.01; *** p &lt; 0.001\"), \n             output = 'flextable')\n\n Electoral DemocracyLiberal DemocracyEgalitarian DemocracyIntercept0.116(0.099)-0.057(0.097)-0.115(0.083)Human Development Index (2005)0.371*(0.166)0.415*(0.162)0.505***(0.139)Percentage Female Legislators (2003)0.006**(0.002)0.007***(0.002)0.007***(0.002)Facebook Use (proportion of population)0.004*(0.002)0.004**(0.002)0.003*(0.001)Num.Obs.142142142R20.3840.4630.522R2 Adj.0.3710.4510.511Notes: OLS Coefficients with SEs in Parentheses* p &lt; 0.05; ** p &lt; 0.01; *** p &lt; 0.001\n\n\nHere is what changed:\n\nlibrary(flextable)\n\nI first load the flextable package.1 I do this for two reasons: (1) I like the formatting of the table a bit better than the default output from modelsummary and (2) I ran into some issues when exporting the table to Word with the other modifications that using this command avoids.\n1 Normally this would be done at the start of the R script with the other packages.\nestimate = \"{estimate}{stars}\\n({std.error})\"\n\nThis is the big change. The default behavior of modelsummary is to present the coefficients from the regression model, which are stored in a column named estimate. Here, we are overriding that default behavior by explicitly specifying what should be in the cell - this is done via the information in the curly brackets. We are telling the command to first plot the coefficient estimate ({estimate}), and then symbols for statistical significance ({stars}). We then tell the command to create a new line in the cell (\\n) and to provide the standard errors surrounded by parentheses (({std.error})). This makes sure that everything is presented in the same cell, which will make sure that we do not get any spacing between the coefficient and standard error, regardless of how long the variable labels happen to be.\n\nstatistic = NULL\n\nThe statistic = option handles whether to plot the standard error or confidence interval for the coefficient (default option = standard error) in a row below the coefficient. We have already printed that statistic via {std.error}, so we tell modelsummary not to print anything here.\n\nnotes = list(….)\n\nThere is a change here: an added line that indicates what the asterisks refer to. We omitted the stars = TRUE option from this syntax because it would end up printing the asterisks next to the standard errors rather than next to the coefficient. However, manually asking for the stars to be printed as above (e.g., {estimate}{stars}) means that modelsummary() won’t automatically print information about what the different asterisks refer to. Hence, this bit of syntax to get around this issue.\n\noutput = 'flextable'\n\nmodelsummary() can use a variety of different table making “back-ends” to create the table; see this page. This tells the command to use the flextable package. This influences the general look of the table as you can probably notice. It also makes sure that the linebreak between the coefficient and standard error is used when we export the table to a Microsoft Word document - other table packages simply printed “\\n” rather than creating a new line. The use of this package also has implications for how we export this table to a .docx file, as discussed below.\n\n\nThere is one other step I would take here. It is probably not noticeable above, but the columns listing the coefficients are not centered. We can, of course, export the table to Word and then center it there, but I’m going to handle this all in one go via syntax. In addition, the horizontal line separating “Num.Obs” and the coefficients above it does not always get printed out when exporting to Word, so I’ll add some syntax to make sure that happens. These steps are optional since you could do them manually later.\n\n# Save the modelsummary results to an object\n\nreg_table &lt;- modelsummary(model_list, \n             estimate = \"{estimate}{stars}\\n{std.error}\", \n             statistic = NULL, \n             gof_map = c(\"nobs\", \"r.squared\", \"adj.r.squared\"), \n             coef_rename = c( \n               \"(Intercept)\" = \"Intercept\", \n               \"HDI2005\" = \"Human Development Index (2005)\", \n               \"Women2003\" = \"Percentage Female Legislators (2003)\", \n               \"Facebook\" = \"Facebook Use (proportion of population)\"), \n             notes = list(\"Notes: OLS Coefficients with SEs in Parentheses\",\n                          \"* p &lt; 0.05; ** p &lt; 0.01; *** p &lt; 0.001\"),\n             output = 'flextable')\n\n# Add some formatting - all of these commands\n# come from the flextable package - it needs to be loaded \n# before they are used\n\nreg_table &lt;- reg_table |&gt; \n1  hline(i = nrow_part(reg_table) - 3) |&gt;\n  align(i = 1:nrow_part(reg_table), j = 2:ncol_keys(reg_table), \n2        align = 'center') |&gt;\n3  align(align = 'center', part = 'header')\n\n\n1\n\nMakes sure a line is printed separating the coefficients from the goodness of fit statistics.\n\n2\n\nMakes sure the columns with coefficients are centered\n\n3\n\nMakes sure the column titles/labels are also centered\n\n\n\n\nI first create the table and store it as an object (named here as reg_table). Then I use some syntax to format the table. This syntax requires the flextable library to be loaded first:\n\nhline(i = nrow_part(reg_table) - 3) |&gt;\n\nThis line makes sure there is a horizontal line (hline) between the last standard error and the part of the table with the number of observations and R2 statistics. The nrow_part… stuff handles where the line is drawn. One can specify a specific number here; for instance, i = 5 would tell the command to draw a line after the fifth row. However, that means you have to create the table and count, which is annoying. Instead, this asks the command to calculate the total number of rows in the table (nrow_part(reg_table)) and then draw the line after whatever row = total number - 3. I use 3 here because I have three rows in the goodness of fit area (Num.Obs, R2, and Adj. R2). If I had 4 things in that area, then I’d use 4 instead of 3. This line of syntax can be used in your own examples - you’d just need to update the regression table name (e.g., nrow_part(reg_table) to nrow_part(name of your table).\n\nalign(i = 1:nrow_part(reg_table), j = 2:ncol_keys(reg_table),  align = 'center')\n\nThis handles column alignment. The logic here is similar to above: instead of creating the table, counting the number of columns that need centering, and using those specific numbers, I am using some commands in flextable to handle these steps for me. i = 1:nrow_part(): I want each row from row 1 to the final row to be centered. j = 2:ncol_keys(): I want columns 2 through however many columns there are in the table to be centered. This makes sure the column with the variable labels is not centered - just those with the coefficients/standard errors. Again, this syntax can be used freely.\n\nalign(align = 'center', part = 'header')\n\nThis makes sure the column header information (e.g., the model names) is also centered. This could stay the same.\n\n\nLet’s take a look:\n\n# Let's take a look\nreg_table\n\n Electoral DemocracyLiberal DemocracyEgalitarian DemocracyIntercept0.1160.099-0.0570.097-0.1150.083Human Development Index (2005)0.371*0.1660.415*0.1620.505***0.139Percentage Female Legislators (2003)0.006**0.0020.007***0.0020.007***0.002Facebook Use (proportion of population)0.004*0.0020.004**0.0020.003*0.001Num.Obs.142142142R20.3840.4630.522R2 Adj.0.3710.4510.511Notes: OLS Coefficients with SEs in Parentheses* p &lt; 0.05; ** p &lt; 0.01; *** p &lt; 0.001\n\n\nYou’re likely writing in Word or Google Docs, so you need to export the table to a .docx file so that you can copy and paste the table into your report. We need to use a command from the flextable library to do this since we changed the table-making back-end above.\n\n# How to export to word\n\nsave_as_docx(reg_table, \n             path = \"regression_table.docx\")\n\nAll you have to do here is update the name of the table in the command (e.g., from “reg_table” to whatever you have named your table object).\nHere is what this looks like in Word:\n\nThe one remaining negative: things look a bit “scrunched up”, i.e., the table is not using all available horizontal space in Word. However, this can be fixed in Word by clicking using the “Autofit” option in Word.2\n2 I believe this would be handled by the Format \\(\\rightarrow\\) Table \\(\\rightarrow\\) Distribute Columns option in Google Docs. There is an autofit() option in the flextable package that will try and do this for us. We could have added this line |&gt; autofit() as the final line when making the alignment changes above to use it. However, the command is perhaps a bit too aggressive when exporting to Word - instead of taking too little space, it tends to take a little bit too much. We’d then have to manually fix things anyways.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regression Table Formatting Suggestion</span>"
    ]
  },
  {
    "objectID": "analysis_04.html",
    "href": "analysis_04.html",
    "title": "14  Combining Plots",
    "section": "",
    "text": "14.1 Example Using patchwork\nLet’s say that our paper was focused on investigating the relationship between inequality and democracy scores. Democracy, of course, is a complex concept. We can see this from the name of one of the most important sources of data on regime type available to researchers (emphasis mine): the Varieties of Democracy (V-Dem) project. The V-Dem dataset, for instance, includes v2x_polyarchy which refers to the extent of electoral democracy, v2x_libdem which focuses on the extent of liberal democracy, and v2x_egaldem which focuses on the extent of egalitarian democracy.1 Perhaps we plan on analyzing all three indicators as DVs in our paper. We would naturally want to describe the nature of these variables at some point in our paper, i.e., provide some indication about their central tendency and how much they vary. We could do this rather simply in-text by telling the reader about the mean and standard deviation of each variable. (“My first dependent variable is a measure of electoral democracy from V-Dem. This variable measures blah blah blah (mean = X, SD = X). My second….”) We could also provide a nice figure showing the degree of variation in these variables as well to supplement these discussions with a histogram making a lot of sense given the nature of these variables. For example:\n# Plot 1\npoly_plot &lt;- ggplot(demdata, aes(x = v2x_polyarchy)) + \n1  geom_histogram(color = \"black\", fill = \"white\") +\n  labs(title = \"Electoral Democracy\", \n2       x = NULL, y = \"Count\") +\n3  scale_x_continuous(limits = c(0,1))\n\n# Plot 2\nlib_plot &lt;- ggplot(demdata, aes(x = v2x_libdem)) + \n  geom_histogram(color = \"black\", fill = \"white\") +  \n  labs(title = \"Liberal Democracy\", \n       x = NULL, y = \"Count\") + \n  scale_x_continuous(limits = c(0,1)) \n\n\n# Plot 3\negal_plot &lt;- ggplot(demdata, aes(x = v2x_egaldem)) + \n  geom_histogram(color = \"black\", fill = \"white\") +  \n  labs(title = \"Egalitarian Democracy\", \n       x = NULL, y = \"Count\") + \n  scale_x_continuous(limits = c(0,1)) \n\n#The output\npoly_plot\n\n\n1\n\nThis alters how the histogram bars are presented. fill= covers the interior color, while color concerns the color of the borders.\n\n2\n\nThe title provides information of what the x-axis is showing, so I removed the x-axis label (x = NULL) to avoid redundancy.\n\n3\n\nSets the “limits” or range of the x-axis. Not strictly needed, but made sense here since I’m plotting different variables with the same theoretical range but potentially different observed minimums and maximums.\n\n\n\n\n\n\n\n\n\n\nlib_plot\n\n\n\n\n\n\n\negal_plot\nThis is nice - we can see how much variation there is on the variables and also some differences between them (e.g., more countries at the higher levels of electoral democracy than liberal or egalitarian democracy). In terms of the paper, however, it might be a bit awkward to provide each histogram as its own figure - it’s not wrong per se, but it would take up a lot of space in the paper.\nInstead of providing each plot separately, we can combine these different histograms into a single figure and include that in our thesis. We can then refer to that figure as we go. The patchwork package is one very useful package for doing this (see their webpage for all the different things you can do with this package).2\nHere is an example:\n#Load the package\n1library(patchwork)\n\n#Combine the plots\npoly_plot + lib_plot + egal_plot\n\n\n1\n\nNormally we would load all of our packages at the very beginning of our script file.\nThe simplest way to combine plots with patchwork is by using a + sign as here. This may not be the best looking version of our figure though given the number of plots we’re combining. They are currently a bit scrunched up since there is only so much horizontal space in the figure. We can use patchwork to combine the different figures a bit differently to avoid this issue (if we think it is a problem). For instance, we could stack each plot on top of each other (use more vertical space); this is generally done by using the / divider rather than a + sign. We could mix and match the + and \\ signs as in the second plot below. Or, we could explicitly specify how many rows there should be as in the third option.3\n#One stacked on top of the other\npoly_plot / lib_plot / egal_plot\n\n\n\n\n\n\n\n#One plot on top, two side by side\npoly_plot / (lib_plot + egal_plot)\n\n\n\n\n\n\n\n#Setting the number of rows or columns\npoly_plot + lib_plot + egal_plot + plot_layout(nrow = 2)\nIn this instance, I think either the first or third options works the best. I kind of like the first one because it best facilitates a comparison across the different variables. If that were not of interest (e.g., if one plot was a histogram of a continuous variable and another was a bar-plot of a categorical variable), then specifying the number of columns/rows might be better.\nWe can export this figure by storing the output as an object and then using ggsave() to save the image as a .png file. I will take one final step here when storing the final plot: I’ll add a ‘tag’ (“A”, “B”, “C”) to each plot. This makes it easier to reference the different components of the plot when writing about them (e.g,. “Plot A in Figure 1 shows….Meanwhile, Plot B…). 4\n# Store\nfinal_plot &lt;- poly_plot / lib_plot / egal_plot + \n  plot_annotation(tag_levels = 'A')\n\n#Save\nggsave(\"final_plot.png\", \n       plot = final_plot, \n       height = 8, width = 12)\nThe ggsave() command works as follows:\nThe example above focused on combining histograms. This can of course be used with other types of plots, with combinations of types, and with coefficient plots. It is not always needed, but can make sense in some instances.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Combining Plots</span>"
    ]
  },
  {
    "objectID": "analysis_04.html#example-using-patchwork",
    "href": "analysis_04.html#example-using-patchwork",
    "title": "14  Combining Plots",
    "section": "",
    "text": "1 To say nothing of v2x_partipdem (participatory democracy) and v2x_delibdem (deliberative democracy)!\n\n\n2 We could also “reshape” our data to accomplish these ends as well; see Chapter 15 .\n\n\n3 We could also specify the number of columns (ncol = 2) .\n\n\n4 Tagging, and the various ways one could modify these tags, is discussed on this page of the patchwork website.\n\n\n\"final_plot.png\"\n\nYou begin by specifying the name of the output file. You can control the format here as well. I save the file as a .png file, which is a common figure output file type. There are other options, but this one should be sufficient for the thesis.\n\nplot = …\n\nI then tell the command what object should be saved in the figure. If this line is missing then ggsave will save the most recently produced plot…which may not be the one you want to save!\n\nheight = ….\n\nI then specify the dimensions of the figure. These are in inches by default, but can be changed to other units as well (e.g., including units = 'cm' would allow you to control the size in centimeters). This is usually a trial and error approach - save the image, open it up and see if it is distorted in some way, and then update as needed (make wider, make shorter, etc.).",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Combining Plots</span>"
    ]
  },
  {
    "objectID": "analysis_pivot.html",
    "href": "analysis_pivot.html",
    "title": "15  Reshaping Datasets",
    "section": "",
    "text": "15.1 Wide vs. Long Datasets\nLet’s first get a handle on what is meant by “wide” and “long” datasets.\nThe following dataset is a simplified version of the European Social Survey:\n#Import our data\ness &lt;- import(\"./data/ess_small.rda\")\n\n#A snapshot\nhead(ess)\n\n    idno contplt wrkprty wrkorg badge sgnptit pbldmn bctprd eisced agea trstprl\n1 600002       2       2      2     2       2      2      2      4   53       7\n2 600003       1       2      1     2       1      2      1      5   41       4\n3 600005       2       2      1     2       2      2      2      7   42       7\n4 600011       2       2      2     2       2      2      2      1   87       3\n5 600013       2       2      2     2       2      2      2      1   78       3\n6 600014       2       2      2     2       1      2      2      2   48       5\n  trstlgl trstplc trstplt\n1       8       6       6\n2       5       5       2\n3       8       8       7\n4       4       8       3\n5       9       8       6\n6       3       0       3\nThe columns are different variables. idno, for instance, is a unique and anonymized ID number for each respondent in the survey. The variable between contplt and bctprd provide data on whether the respondent indicated that they had completed some type of political behavior (e.g, wrkprt = 1 if the respondent reported working for a political party and 2 if not). The remaining columns provide data on education (eisced), age (agea), and trust in various institutions (trstprl through trstplt).\nThe foregoing is what we would call a “wide” dataset. The first column of a dataset is usually an identifier.1 A wide-formatted dataset is one wherein the values in this identifier column do not repeat. Each row in the dataset above focuses on a different survey respondent (as seen from the unique values in idno).\nCompare what is shown above to the following dataset, which focuses on data from the Varieties of Democracy (V-Dem) project:\n#Import our data\nvdem &lt;- import(\"./data/vdem_small.rda\")\n\n#A snapshot\nhead(vdem, n = 10L)\n\n   country_name year v2x_polyarchy v2x_egaldem v2x_corr\n1        Mexico 1789         0.028          NA     0.68\n2        Mexico 1790         0.028          NA     0.68\n3        Mexico 1791         0.028          NA     0.68\n4        Mexico 1792         0.028          NA     0.68\n5        Mexico 1793         0.028          NA     0.68\n6        Mexico 1794         0.028          NA     0.68\n7        Mexico 1795         0.028          NA     0.68\n8        Mexico 1796         0.028          NA     0.68\n9        Mexico 1797         0.028          NA     0.68\n10       Mexico 1798         0.028          NA     0.68\nThis is a “long” formatted dataset. The first column is again an identifier. In this case, the variable is named country_name and provides the name of each country in the dataset. The values here repeat. We can think of the observations in a “long” formatted dataset as a “dyad”.2 In this example, the dyad would be a “country-year” - our data includes observations on countries by year. A dyad could refer to something else though. For instance, suppose we ask respondents to evaluate multiple political parties; we could organize our dataset in a “long” format wherein each row provides a respondents evaluation of a distinct party (e.g., a respondent-party dyad).\nMost students in my BAP will use survey data. This type of data will generally be in a “wide” format unless, perhaps, the survey is a panel survey (e.g., the respondent is surveyed at multiple time points). In general, it will make the most sense to keep the data in that format. However, there are some situations wherein it may be useful to “reshape” the data (e.g., convert a wide dataset to a long dataset). This can facilitate some types of descriptive analyses and may be required for others. Likewise, you may have need to reshape a long dataset into a wider format. The remainder of this document will show you how to do this using two commands from the tidyverse: pivot_longer() and pivot_wider().",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Reshaping Datasets</span>"
    ]
  },
  {
    "objectID": "analysis_pivot.html#wide-vs.-long-datasets",
    "href": "analysis_pivot.html#wide-vs.-long-datasets",
    "title": "15  Reshaping Datasets",
    "section": "",
    "text": "1 This is not always true. Big social surveys like the ESS or ANES will often starts with lots of columns focused on survey-related variables (e.g., the country of the respondent, when the respondent answered the survey, what mode was used, weights, etc.) before getting to the actual survey responses.\n\n\n2 We could perhaps have data with an even more complicated unit of analysis. Instead of countries, or country-years, perhaps we have party-country-years or something.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Reshaping Datasets</span>"
    ]
  },
  {
    "objectID": "analysis_pivot.html#from-wide-to-long-pivot_longer",
    "href": "analysis_pivot.html#from-wide-to-long-pivot_longer",
    "title": "15  Reshaping Datasets",
    "section": "15.2 From Wide to Long: pivot_longer()",
    "text": "15.2 From Wide to Long: pivot_longer()\nLet’s consider a first use case where it may make sense to reshape our data as an example of how to do so. The ESS dataset above includes columns (variables) pertaining to whether the respondent reported performing various political actions. Here is an overview:\n\n\nShow the code\ness |&gt; \n  select(contplt:bctprd) |&gt; \n1  sjPlot::view_df()\n\n\n\n1\n\nI am using the view_df() command from the sjPlot package to take a look at the variable’s attributes. This set up allows me to use the command without loading the package first, which sometimes can be handy.\n\n\n\n\n\n\nData frame: select(ess, contplt:bctprd)\n\n\n\n\n\n\n\n\n\nID\nName\nLabel\nValues\nValue Labels\n\n\n1\ncontplt\nContacted politician or government official last\n12 months\n1\n2\nNA(b)\nNA(c)\nNA(d)\nYes\nNo\n\n\n2\nwrkprty\nWorked in political party or action group last 12\nmonths\n1\n2\nNA(b)\nNA(c)\nNA(d)\nYes\nNo\n\n\n3\nwrkorg\nWorked in another organisation or association last\n12 months\n1\n2\nNA(b)\nNA(c)\nNA(d)\nYes\nNo\n\n\n4\nbadge\nWorn or displayed campaign badge/sticker last 12\nmonths\n1\n2\nNA(b)\nNA(c)\nNA(d)\nYes\nNo\n\n\n5\nsgnptit\nSigned petition last 12 months\n1\n2\nNA(b)\nNA(c)\nNA(d)\nYes\nNo\n\n\n6\npbldmn\nTaken part in lawful public demonstration last 12\nmonths\n1\n2\nNA(b)\nNA(c)\nNA(d)\nYes\nNo\n\n\n7\nbctprd\nBoycotted certain products last 12 months\n1\n2\nNA(b)\nNA(c)\nNA(d)\nYes\nNo\n\n\n\n\n\n\nLet’s say we were writing a paper where we are investigating the relationship between education and political participation. We might hypothesize that those with a university degree will be more likely to take action than those without one.\nWe should first begin by getting to know our variables. For instance, we might first ask whether rates of participation vary between these different behaviors. This may help us provide context for the results of the statistical model that we report later on in our paper.\nOne way we can do this is by creating a figure that plots the proportion of respondents who reported taking each action. However, our dataset is not very well organized for this goal given that each participation measure is a separate column in the dataset. Reshaping the data so that it is in a longer format can get around this problem. Specifically, we’ll do the following:\n\nReshape the data into a respondent-behavior dyad\nUse group_by() and summarize() to create a data object with the proportions (see Section 3.2 in the Statistic I R book for a refresher on these tools)\nPlot our data\n\nLet’s begin by reshaping the data. Here is the syntax:\n\ness_long &lt;- ess |&gt; \n  pivot_longer(\n    cols = c(\"contplt\", \"wrkprty\", \"wrkorg\", \"badge\", \n             \"sgnptit\", \"pbldmn\", \"bctprd\"), \n    names_to = \"measure\", \n    values_to = \"outcome\" )\n\nHere is the data produced by this command. Seeing it will help illuminate what I did in the syntax above:\n\nhead(ess_long, n = 10L)\n\n# A tibble: 10 × 9\n     idno eisced  agea trstprl trstlgl trstplc trstplt measure outcome\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1 600002      4    53       7       8       6       6 contplt       2\n 2 600002      4    53       7       8       6       6 wrkprty       2\n 3 600002      4    53       7       8       6       6 wrkorg        2\n 4 600002      4    53       7       8       6       6 badge         2\n 5 600002      4    53       7       8       6       6 sgnptit       2\n 6 600002      4    53       7       8       6       6 pbldmn        2\n 7 600002      4    53       7       8       6       6 bctprd        2\n 8 600003      5    41       4       5       5       2 contplt       1\n 9 600003      5    41       4       5       5       2 wrkprty       2\n10 600003      5    41       4       5       5       2 wrkorg        1\n\n\nThe first column is idno, our respondent identifier. We can see now that the values here now repeat - each respondent is now represented multiple times in the data. We then get columns for education, age, and trust. These still provide the values for these variables for each respondent. We can see that they do not vary within respondents (i.e., the same values are provided for each row associated with respondent 600002).3 We then see a column named measure - the contents of this column indicate which participation measure the particular row tells us about. Row 1, for instance, tells you how respondent 600002 answered the “contplt” question, while row 2 tells you how respondent 600002 answered the “wrkprty” question, and so on. Finally, outcome gives the specific value provided on the participation measure associated with the respondent. A 2 on these measures indicate that the respondent did not take the action in question, while a 1 indicates that they did. Respondent 600002 did not report taking any of the actions in question.\n3 In practice, I would probably have a precursor line in my syntax that selects only the variables I want in the reshaped dataset to avoid including unnecessary elements. For instance: ess |&gt; select(contplt, wrkprty, wrkorg, badge, sgnptit, pbldmn, bctprd) |&gt; pivot_longer(…)Let’s unpack the syntax now:\n\ncols = c(\"contplt\", …)\n\nThis tells the command which columns should be part of the reshaping process. I included the variable names (in parentheses) within c(). I could have made this much simpler in the present instance by instead using: cols = contplt:bctprd. (Quotation marks not needed here.) This would have told R to include all of the columns between contplt and bctprd when reshaping to the longer data object. This is a generally easier thing to type out, but would have been problematic had there been a variable in that sequence that I did not want to reshape by.\n\nnames_to = \"measure\"\n\nThis is how we indicate the name for the new column that provides information on the columns we’ve made the dataset longer by. If this omitted, then pivot_longer() would use “name” as the, uh, name of this column.\n\nvalues_to = \"outcome\"\n\nThis is how we indicate the name for the new column that provides information on the responses/observed values for each variable we’ve included in the reshaping syntax. If this is omitted, then pivot_longer() would use “value” as the name of the column.\n\n\nWe now have our data in a longer format. We can now use group_by() and summarize() to get the proportions we want to plot (or other types of statistics, such as the mean or median, if that was more relevant). However, we need to first recode our outcome variable into a 0/1 variable (0 = did not take the action, 1 = did take the action). We could have done this before reshaping the data, but that would have involved seven different commands so this is comparatively simpler.\n\ndescriptives &lt;- ess_long |&gt; \n  mutate(\n1    outcome_recode = case_when(\n      outcome == 1 ~ 1, \n      outcome == 2 ~ 0)) |&gt; \n2  group_by(measure) |&gt;\n  summarize(beh_prop = mean(outcome_recode, na.rm = T)) |&gt; \n3  ungroup()\n\n\n1\n\nI use case_when() here to make it a bit easier to handle observations with missing values on this variable. I create a new variable here because if I overwrite the original variable but make a mistake while doing so, then I need to re-run the pivoting command.\n\n2\n\nBasically tells R to separate (or group) the data by each unique value in the measure column and then do the next step to each grouping\n\n3\n\nThis tells R to undue the grouping. Not taking this step can lead to issues when we follow group_by() with mutate(). It isn’t a problem to not include this syntax after summarize() (see the discussion here), but I include it to be consistent.\n\n\n\n\nLet’s take a look:\n\ndescriptives\n\n# A tibble: 7 × 2\n  measure beh_prop\n  &lt;chr&gt;      &lt;dbl&gt;\n1 badge     0.0407\n2 bctprd    0.137 \n3 contplt   0.135 \n4 pbldmn    0.0298\n5 sgnptit   0.221 \n6 wrkorg    0.252 \n7 wrkprty   0.0347\n\n\nWe now have a data object with two variables: one indicating the behavior measure (measure) and one indicating the proportion of respondents with a value of 1 (i.e., performed this action) on the measure. We can next make a plot either for our own edification or for inclusion in our paper:\n\nggplot(descriptives, aes(x = measure, y = beh_prop)) + \n  geom_col() + \n  geom_text(aes( label = round(beh_prop, 2)), vjust = -0.4)\n\n\n\n\n\n\n\n\nThis is not too bad. However, if we were going to include this in our paper, then we should rename “badge”, etc., into something the consumers of our plot will understand. We might also want to reorder the data such that the bars progress largest to smallest or vice versa. For instance:\n\n\nShow the code\ndescriptives |&gt; \n1  mutate(measure = recode(measure,\n                          'badge' = 'Worn Badge', \n                          'bctprd' = 'Boycott Products', \n                          'contplt' = 'Contact Politician', \n                          'pbldmn' = 'Public Demonstration', \n                          'sgnptit' = 'Signed Petition', \n                          'wrkorg' = 'Worked in Organization', \n                          'wrkprty' = 'Worked in Party')) |&gt; \n2  ggplot(aes(x = reorder(measure, -beh_prop), y = beh_prop)) +\n  geom_col() + \n  geom_text(aes(label = round(beh_prop, 2)), vjust = -0.4) + \n3  scale_x_discrete(labels = scales::label_wrap(10)) +\n  labs(x = \"Participation Measure\", \n       y = \"Proportion\")\n\n\n\n1\n\nWe can give the different behaviors more informative names by recoding the contents of measure\n\n2\n\nreorder(measure, -beh_prop) handles the ordering. If we dropped the - sign, then the bars would ascend rather than descend.\n\n3\n\nThis makes sure the labels warp around so that they do not overlap. The number can be changed to give more/less space. Note: this requires that the scales package is installed.\n\n\n\n\n\n\n\n\n\n\n\nYou can see this discussion for a longer discussion of the different ways of reordering axes in a ggplot. You can learn more about how to add the automatic line break to value labels here.\nThe set up for this example suggested a hypothesis: people with a university degree participate more than those without a degree. We could also break down participation by education via this same basic process. We just need to add our education variable to the group_by() command.4 In the present case we will first want to create a binary variable for education (no degree vs. university degree) although one could imagine doing this for more than two categories as well.\n4 In practice, we might want to combine these different participation measures into a single scale. For instance, we could recode each measure into a 0/1 binary and then sum them up into a scale ranging from 0 (no actions taken) to 7 (all actions taken) and then use that as our DV.\ndescriptives_univ &lt;- ess_long |&gt; \n  mutate(\n    outcome_recode = case_when(\n      outcome == 1 ~ 1, \n      outcome == 2 ~ 0), \n1    univ = case_when(\n      eisced %in% c(1:5, 55) ~ \"No Degree\", \n      eisced %in% c(6:7) ~ \"Degree\")) |&gt; \n2  group_by(measure, univ) |&gt;\n  summarize(beh_prop = mean(outcome_recode, na.rm = T)) |&gt; \n  ungroup()\n\n\n1\n\neisced is my education variable. It takes on values between 1 (less than lower secondary education) to 7 (higher tertiary) with another category numbered 55 for “Other”. The c(6:7) type notation works here because all categories are whole numbers; if eisced could take on a value of 6.7 (for instance), then observations with this value would not get recoded when using c(6:7).\n\n2\n\nI am now telling R to separate (group) data by each combined value of measure and univ (e.g., measure = “badge” & univ = 0, measure = “badge” & univ = 1, …) and then do the next stuff.\n\n\n\n\n`summarise()` has grouped output by 'measure'. You can override using the\n`.groups` argument.\n\nhead(descriptives_univ, n = 10L)\n\n# A tibble: 10 × 3\n   measure univ      beh_prop\n   &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt;\n 1 badge   Degree      0.0502\n 2 badge   No Degree   0.0376\n 3 badge   &lt;NA&gt;        0     \n 4 bctprd  Degree      0.260 \n 5 bctprd  No Degree   0.0960\n 6 bctprd  &lt;NA&gt;        0     \n 7 contplt Degree      0.232 \n 8 contplt No Degree   0.103 \n 9 contplt &lt;NA&gt;        0     \n10 pbldmn  Degree      0.0591\n\n\nWe can now turn this into a plot although we should first filter our “NA” values.\n\n\nShow the code\ndescriptives_univ |&gt; \n1  filter(!is.na(univ)) |&gt;\n  mutate(measure = recode(measure,\n                          'badge' = 'Worn Badge', \n                          'bctprd' = 'Boycott Products', \n                          'contplt' = 'Contact Politician', \n                          'pbldmn' = 'Public Demonstration', \n                          'sgnptit' = 'Signed Petition', \n                          'wrkorg' = 'Worked in Organization', \n                          'wrkprty' = 'Worked in Party'), \n2         univ = factor(univ, levels = c(\"No Degree\", \"Degree\"))) |&gt;\n  ggplot(aes(x = univ, y = beh_prop)) + \n  geom_col() + \n  geom_text(aes(label = round(beh_prop, 2)), vjust = -0.4) + \n3  facet_wrap(~ measure) +\n  scale_y_continuous(limits = c(0, 0.5)) + \n  labs(x = \"University Degree?\", \n       y = \"Proportion\")\n\n\n\n1\n\nMakes sure we keep all observations where univ is not missing (! = not, is.na = is NA).\n\n2\n\nI convert univ to a factor and specify the levels with “No Degree” being the first one. This makes sure that “No Degree” (the “lower” value on education) is plotted before “Degree”.\n\n3\n\nCreates separate facets/graphs for each value of our measure variable.\n\n\n\n\n\n\n\n\n\n\n\nIt does look like those with higher levels of education are more like to take these particular political acts. The difference is fairly small when we look at behaviors that are uncommon, but much larger with behaviors that are a bit more likely to be observed in general (e.g., boycotting). Of course, this doesn’t mean that education is causing these differences. Indeed, whether education causes political participation is something of a recent debate. We would thus want to move to a more fully specified model to more fully justify our claims.\nThe foregoing can also be useful for making the same type of plot but for different variables. For instance, our ess dataset contains several measures of trust: trust in the country’s parliament (trstprl), trust in the legal system (trstlgl), trust in the police (trstplc), and trust in politicians (trstplt) on a scale ranging from 0 (no trust at all) to 10 (complete trust). . We could, for instance, provide a bar-graph showing the number (or even proportion) of respondents giving each response from 0-10. Instead of creating a plot for each variable separately and then using patchwork to combine them (as shown in Chapter 14), we could do this all at once by using tidy_longer(). For instance:\n\n\nShow the code\ness |&gt; \n  pivot_longer(\n1    cols = trstprl:trstplt,\n    names_to = \"target\", \n    values_to = \"trust\") |&gt; \n2  group_by(target, trust) |&gt;\n3  summarize(n = n()) |&gt;\n4  mutate(target = recode(target,\n                         'trstlgl' = 'Legal System', \n                         'trstplc' = 'Police', \n                         'trstplt' = 'Politicians', \n                         'trstprl' = 'Parliament')) |&gt;  \n  ggplot(aes(x = trust, y = n)) + \n  geom_col() + \n  facet_wrap(~ target) + \n5  scale_x_continuous(breaks = seq(from = 0, to = 10, by = 1)) +\n  labs( y = 'Count', x = 'Trust')\n\n\n\n1\n\nThe four variables all appear in a sequence, so I can do this instead of the c(\"var name\", \"var name\") procedure from above.\n\n2\n\nI’m going to plot the number of people in each category on each measure. So I combine both variables (the one listing the variable name, target, and the one listing the respondent’s response, trust, in group_by().\n\n3\n\nI could technically use a simpler command here: tally(). This would do the same thing as this line of syntax.\n\n4\n\nUsing recode to give the different trust variables nicer names for the plot.\n\n5\n\nThis makes sure there are breaks/ticks on the x-axis for each value between 0 and 10.\n\n\n\n\n\n\n\n\n\n\n\nHere, we can see that trust seems to be slightly higher when we look at the legal institutions (legal system and police) than for the more explicitly political institutions. This is a fairly common finding in the trust literature. We can see this by looking at the means for each variable as well using the describe() function from the psych package for simplicity:5\n5 We could, of course, have done this in the long dataset as well, e.g.: ess_long |&gt; group_by(target) |&gt; summarize(mean = mean(trust, na.rm=T) , although we would have needed to include some additional lines of syntax to get the standard deviation, etc.. Pivoting is not always the best solution!\ness |&gt; \n  select(trstprl:trstplt) |&gt; \n  psych::describe()\n\n        vars    n mean   sd median trimmed  mad min max range  skew kurtosis\ntrstprl    1 1824 5.24 2.06      6    5.43 1.48   0  10    10 -0.77     0.28\ntrstlgl    2 1830 6.02 2.02      6    6.22 1.48   0  10    10 -0.85     0.56\ntrstplc    3 1835 6.40 1.83      7    6.64 1.48   0  10    10 -1.25     1.83\ntrstplt    4 1829 5.05 1.96      5    5.25 1.48   0   9     9 -0.81     0.16\n          se\ntrstprl 0.05\ntrstlgl 0.05\ntrstplc 0.04\ntrstplt 0.05\n\n\nPivoting from wide to long can be a useful strategy for your analyses. We have used it above to summarize multiple variables and then produce nice looking figures to communicate our results. Pivoting could also be useful in other contexts as well. It may be a pre-requisite for some types of analyses (e.g., the analysis of repeated measures data such as with panel data). It could also be useful as a prelude to more advanced analyses such as performing the same regression model on lots and lots of different subsets of our data (e.g., the same model for each country in a dataset) although that use is beyond the scope of this document (see here for an example discussion).",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Reshaping Datasets</span>"
    ]
  },
  {
    "objectID": "analysis_pivot.html#from-long-to-wide-pivot_wider",
    "href": "analysis_pivot.html#from-long-to-wide-pivot_wider",
    "title": "15  Reshaping Datasets",
    "section": "15.3 From Long to Wide: pivot_wider()",
    "text": "15.3 From Long to Wide: pivot_wider()\nWe can move from long to wide as well. This is perhaps less useful overall. The main use case I can think of here focuses on situations where you want to compare across different values within one of the dyad columns.\nLet’s start with a simple example involving our ess_long data from above. Let’s take a quick look at the data again to remind ourselves what it looks like:\n\nhead(ess_long, n = 10L)\n\n# A tibble: 10 × 9\n     idno eisced  agea trstprl trstlgl trstplc trstplt measure outcome\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1 600002      4    53       7       8       6       6 contplt       2\n 2 600002      4    53       7       8       6       6 wrkprty       2\n 3 600002      4    53       7       8       6       6 wrkorg        2\n 4 600002      4    53       7       8       6       6 badge         2\n 5 600002      4    53       7       8       6       6 sgnptit       2\n 6 600002      4    53       7       8       6       6 pbldmn        2\n 7 600002      4    53       7       8       6       6 bctprd        2\n 8 600003      5    41       4       5       5       2 contplt       1\n 9 600003      5    41       4       5       5       2 wrkprty       2\n10 600003      5    41       4       5       5       2 wrkorg        1\n\n\nWhy might we want to make this wider? Well, maybe we want to answer look at the cross-tabulation between contplt and wrkprty to get a sense of their inter-relationship: do people who work for political parties also tend to contact politicians?. That isn’t really easy to do with the data variables stacked on top of each other like this. Instead, we can use pivot_wider() to create a wider version of this dataset wherein the two variables are given their own columns that we can then pass into some other command. Here is how we can do that:\n\ness_wide &lt;- pivot_wider(ess_long, \n  id_cols = idno:trstplt, \n  names_from = measure, \n  values_from = outcome\n)\n\n\nid_cols = idno:trstplt\n\nThis is a very important row. id_cols tells the command which columns in the data are “id” columns - that is, which columns “uniquely identify each observation”. The idno:trstplt entry tells the command to treat all columns between those bookends (idno through trstplt) as unique identifiers. If I only put one of those variables here (e.g., id_cols = idno by itself), then the other columns would get dropped from the resulting dataset! That may or may not be an issue in a given example.\n\nnames_from = measure\n\nThis tells the command which column in our longer dataset contains the values that we want to use as the column names in our wider dataset. The measure variable contains that data in our example.\n\nvalues_from = outcome\n\nLikewise, we need to tell the command which column contains the values that should be used in these newly created columns. That column is named outcome in this example.\n\n\nLet’s take a look so you can see what this looks like:\n\nhead(ess_wide, n = 10L)\n\n# A tibble: 10 × 14\n     idno eisced  agea trstprl trstlgl trstplc trstplt contplt wrkprty wrkorg\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 600002      4    53       7       8       6       6       2       2      2\n 2 600003      5    41       4       5       5       2       1       2      1\n 3 600005      7    42       7       8       8       7       2       2      1\n 4 600011      1    87       3       4       8       3       2       2      2\n 5 600013      1    78       3       9       8       6       2       2      2\n 6 600014      2    48       5       3       0       3       2       2      2\n 7 600015      2    61       4       4       3       3       2       2      2\n 8 600017      2    21       8       6       7       7       2       2      2\n 9 600021      4    67       6       7       7       6       2       1      1\n10 600024      1    57       0       7       5       0       1       2      2\n# ℹ 4 more variables: badge &lt;dbl&gt;, sgnptit &lt;dbl&gt;, pbldmn &lt;dbl&gt;, bctprd &lt;dbl&gt;\n\n\nWe one again have a dataset wherein each row focuses on a unique observation (completely different survey respondent) complete with columns for each of our participation measures. We could then perform a simple cross-tabulation if we wanted:\n\n1with(ess_wide, table(contplt, wrkprty))\n\n\n1\n\nI am using with() here to avoid having to use the $ notation twice (e.g., instead of writing table(ess_wide$contplt, ess_wide$wrkprty).\n\n\n\n\n       wrkprty\ncontplt    1    2\n      1   33  215\n      2   31 1563\n\n\nRecall that 1 = “did the action” and 2 = “did not”. Most people took neither action. More people reported contacting a politician but not working for a party (n = 215) than doing both actions (n = 33) or working for a party but not contacting a politician (n= 31).\nLet’s look at another (slightly more complex example) using the vdem data. Here are the variables in our dataset:\n\nsjPlot::view_df(vdem)\n\n\n\nData frame: vdem\n\n\nID\nName\nLabel\nValues\nValue Labels\n\n\n1\ncountry_name\n\n\n&lt;output omitted&gt;\n\n\n2\nyear\n\nrange: 1789-2023\n\n\n3\nv2x_polybackground-color:#eeeeeehy\n\nrange: 0.0-0.9\n\n\n4\nv2x_egaldem\n\nrange: 0.0-0.9\n\n\n5\nv2x_corr\n\nrange: 0.0-1.0\n\n\n\n\n\n\nOur data focuses on a country’s level of electoral democracy (v2x_polyarchy), egalitarian democracy (v2x_egaldem) and level of corruption (v2x_corr). We have data over time for each measure - from 1789 to 2023!6\n6 Although, in practice, we have missing data for many years prior to the 20th century.One thing we might wonder is this: how stable are democracy scores? One way we could get at this is to consider the correlation between years, e.g., how strongly correlated are democracy scores in 1980 and 2000 or in 2000 and 2020? The more strongly correlated the scores, the more stable is democracy. Again, though, our data is “stacked” such that the democracy score data (either via v2x_polyarchy or v2x_egaldem) is all in one column. We thus need to widen our data such that there is a column for our democracy measure (or measures) for each separate year that we can then feed into a correlation.\nHere is how I will do this:\n\nvdem_wider &lt;- vdem |&gt; \n  filter(year %in% c(1980, 2000, 2020)) |&gt; \n  select(country_name, year, v2x_polyarchy, v2x_egaldem) |&gt; \n  pivot_wider(\n    id_cols = \"country_name\", \n    names_from = \"year\",\n    values_from = c(\"v2x_polyarchy\", \"v2x_egaldem\"))\n\n\nfilter(year %in% c(1980, 2000, 2020)\n\nI am filtering my original data such that only data from these three years is kept. If I skipped this step then my wide dataset would end up with a column for each year from 1793 to 2023! That is overkill in the present instance. This initial step may not be needed in other examples.\n\nselect(…)\n\nI am also getting rid of the corruption measure (while keeping the variables included in the select() command). I could have widen the data to include separate columns for it as well, but that is not really needed in this example. Getting rid of unnecessary columns also makes the id_cols part simpler.\n\nid_cols = \"country_name\"\n\nThis specifies the unique identifier variable. Here, the variable that uniquely identifies observations across time is country_name, which lists the, uh, country name for each observation.\n\nnames_from = \"year\"\n\nThis indicates that I want the values from the year column to be included in the names of the columns that this command creates.\n\nvalues_from = c(\"v2x_polyarchy\", \"v2x_egaldem\"))\n\nThis tells the command which columns in our long dataset contain the values we want to port over into the columns we’re creating.\n\n\nLet’s take a look:\n\nhead(vdem_wider, n = 10L)\n\n# A tibble: 10 × 7\n   country_name  v2x_polyarchy_1980 v2x_polyarchy_2000 v2x_polyarchy_2020\n   &lt;chr&gt;                      &lt;dbl&gt;              &lt;dbl&gt;              &lt;dbl&gt;\n 1 Mexico                     0.3                0.671              0.652\n 2 Suriname                   0.218              0.783              0.754\n 3 Sweden                     0.91               0.914              0.901\n 4 Switzerland                0.87               0.888              0.901\n 5 Ghana                      0.543              0.667              0.722\n 6 South Africa               0.16               0.745              0.714\n 7 Japan                      0.841              0.84               0.835\n 8 Burma/Myanmar              0.137              0.095              0.422\n 9 Russia                     0.107              0.405              0.258\n10 Albania                    0.174              0.407              0.536\n# ℹ 3 more variables: v2x_egaldem_1980 &lt;dbl&gt;, v2x_egaldem_2000 &lt;dbl&gt;,\n#   v2x_egaldem_2020 &lt;dbl&gt;\n\n\nThe first column is our ID column. We can now see that each country features once! We then get columns with our democracy data. The command has “glued” together the names from values_from with the values from year (e.g., v2x_polyarchy_1980 tells us the polyarchy scores for each country in the year 1980, while v2x_polyarchy_2000 gives us the year 2000 data and so on). The naming of variables in a pivot_wider() command can get a little bit complicated in more complicated examples and there are additional options useful on this front; see the reference page for the command.\nLet’s look at some correlations. I will use the correlation() command from the aptly named correlation package to do this because it makes it easy to calculate multiple correlations at once.\n\nlibrary(correlation)\n\nvdem_wider |&gt; \n1  select(contains(\"v2x_polyarchy\")) |&gt;\n  correlation()\n\n\n1\n\nThis is one of a variety of helpful shortcuts for selecting variables based on the contents of variable names, including ends_with() and starts_with() See this7se, pretty stable is not perfectly stable.\n\n7 See here for where I found the syntax needed for the 45 degree line.\n\n\n# Correlation Matrix (pearson-method)\n\nParameter1         |         Parameter2 |    r |       95% CI |     t |  df |         p\n---------------------------------------------------------------------------------------\nv2x_polyarchy_1980 | v2x_polyarchy_2000 | 0.70 | [0.61, 0.77] | 12.23 | 153 | &lt; .001***\nv2x_polyarchy_1980 | v2x_polyarchy_2020 | 0.68 | [0.58, 0.75] | 11.34 | 153 | &lt; .001***\nv2x_polyarchy_2000 | v2x_polyarchy_2020 | 0.83 | [0.78, 0.87] | 19.64 | 175 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 155-177\n\n\nWe could perhaps plot this data to get a better sense of matters. I will do so for the 1980 and 2020 data. I will also add a 45 degree diagonal line; points above this line will indicate countries with higher levels of democracy in 2020 than 1980, while points below the line indicate a decrease in electoral democracy over time.\n\nggplot(vdem_wider, aes(x = v2x_polyarchy_1980, y = v2x_polyarchy_2020)) + \n  geom_point(alpha = 0.2) + \n  geom_smooth(method = \"lm\") + \n1  geom_line(data = data.frame(x = c(-Inf, Inf), y = c(-Inf, Inf)), aes(x, y)) +\n2  scale_y_continuous(limits = c(0,1)) +\n  scale_x_continuous(limits = c(0,1)) + \n  labs(x = \"Electoral Democracy (1980)\", \n       y = \"Electoral Democracy (2020)\")\n\n\n1\n\nAdds the 45 degree line.\n\n2\n\nThe democracy variables have a theoretical range of 0 to 1. This extends the y- axis to cover that full range.\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe electoral democracy scores are quite stable per the correlation coefficient. Where there are deviations it tends to show up in countries with a higher democracy score in 2020 than 1980. This makes sense as the 1980s/1990s is commonly referred to as the ‘third wave of democratization’ We can also see that countries with comparatively high levels of democracy in 1980 generally have quite high levels in 2020 as well.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Reshaping Datasets</span>"
    ]
  }
]