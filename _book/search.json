[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thoughts and Advice on Writing your BAP",
    "section": "",
    "text": "Preface\nI have been teaching a Bachelor’s Project (BAP) at Leiden University for several years now. I have seen great theses, poor theses, and everything in between as both a supervisor and as a second reader on theses in other courses. This book is a collection of materials that I have been developing for the past several years aimed at giving you additional guidance for developing a good (perhaps even great) thesis.\nThere are three main sections to this book:\n\nWriting the Thesis: This part slightly puts the cart before the horse by considering the written product of the thesis - what should be in it, how to structure it, and how to communicate effectively. It also provides links to some other resources that I think can help you in this process (see the starting preface/part of each section for additional resources).\nDoing Research: This part focuses on the process of doing research. In particular, it provides some guidance on how to find relevant research to review for your project and how to think about finding relevant data to analyze (presuming you are not collecting your own).\nPractical Issues in Data Analysis: This section focuses on providing some advice on how to analyze and present data in an effective manner. It is not a full primer on statistical analyses (you should consult your Statistics I/II notes and the resources provided there for that), but seeks to bridge the gap between the abstract and the practical.\n\nWhat I write naturally reflects my own opinions and tastes. That does not make them “correct”, of course, although this information may still be of value even if it is purely idiosyncratic to me as a reader given that I’m one of the people grading your thesis! In addition, this document, by its very nature, is focused on general advice; your specific project may require deviations or alterations to what I suggest here.\nI will highlight some other resources for helping you along the way in subsequent sections as I note above. One resource that I want to highlight here is a Substack written by two political scientists–Paul Musgrave and Nicholas Davis–called “Thesis Statement” (weblink). These authors have written a host of short, but very insightful, posts on how to approach nearly every part of writing one’s senior thesis. I would strongly recommend reading through what they have written as well.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "part_writing.html",
    "href": "part_writing.html",
    "title": "Writing the Thesis",
    "section": "",
    "text": "Some Other Guides\nWhat I write in the sections to come is based on my experience supervising BAP thesis projects and assessing thesis projects as a second reader. The points below are thus potentially idiosyncratic to me as a reader. However, I have also drawn on a variety of other resources from political and social scientists in composing this advice. You can find links to some of the resources I draw upon below. I recommend giving them a quick read through as well since they are pretty short, but note that some of these documents are pitched at particular institutions or particular types of assignments, so not every point in them will be relevant.",
    "crumbs": [
      "Writing the Thesis"
    ]
  },
  {
    "objectID": "part_writing.html#some-other-guides",
    "href": "part_writing.html#some-other-guides",
    "title": "Writing the Thesis",
    "section": "",
    "text": "Thesis Statement by Paul Musgrave and Nicholas Davis\nWriting Advice by John Gerring\nAssorted Tips for Students on Writing Research Papers, Dos and Don’ts of Writing for Students, and Research Design Paper Instructions by Steven V. Miller\nThree Templates for Introductions to Political Science Articles\nCh. 12: Communicating Research in “Research Design in Political Science” by Dimiter Toshkov",
    "crumbs": [
      "Writing the Thesis"
    ]
  },
  {
    "objectID": "writing_01_structure.html",
    "href": "writing_01_structure.html",
    "title": "1  Structuring the Thesis",
    "section": "",
    "text": "In general, a BAP thesis can be divided into the following portions:\n\n\nIntroduction\nReview of Literature and Argument\nResearch Design\nAnalyses/Results\nConclusion\nAppendix/Appendices\n\nI discuss each portion of the thesis in the chapters to come. But, first, a few beginning thoughts.\nFirst, do not take the above as meaning that you need to have exactly this number of sections in your thesis. It may, and often does, make sense to divide some of these portions into separate sections or subsections. The Review of Literature and Argument portion of the thesis, for instance, is often best divided into two or more sections to avoid overwhelming the reader with too much information all at once and to signal changes in topic within the thesis. It is not a bad idea, meanwhile, to divide the Research Design section into sub-sections as I’ll discuss later on.\nRegardless of these points, there should be clear section headers to delineate where each portion of the thesis begins and where the next one ends with the exception of the Introduction which does not require a section header. I prefer, and recommend, the APA style for section headers shown here with main sections centered and bolded and then sub-section headers left-aligned and bolded. For instance:\n\nA second consideration concerns length The thesis, as a whole, should be between 7,000-8,000 words (excluding the title page and references). A natural question might then be: how long should each portion be? There is no single correct answer to this question. What follows, then, are highly general recommendations that will need to be revised in relation to your particular research question and project. I would not get too attached to the specific numbers here - they were not created via any type of scientific process beyond me trying to eyeball my own writing, and past theses, and throwing out some plausible sounding ranges. However, the ranges may give some sense of the relative importance of each section.\n\nIntroduction: ~500-1000 words\nLiterature Review and theory: ~1500-2500 words\nResearch Design: ~700-1500 words\nAnalyses/Results: ~700-1500 words\nConclusion: ~600-1500 words\n\nFinally, I have a sixth item above: Appendix or Appendices. There are two types of appendix that it makes sense to include: (1) one that gives some additional details on how control variables are measured and operationalized in your model; (2) one that goes into detail about your assumption checks. I’ll discuss these in sections to come. An important point: appendices do not count toward the word count of the thesis.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Structuring the Thesis</span>"
    ]
  },
  {
    "objectID": "writing_02_introduction.html",
    "href": "writing_02_introduction.html",
    "title": "2  The Introduction",
    "section": "",
    "text": "The introduction is a mini-preview of the paper to come. The reader should come away from this section understanding what the topic of the paper is, what the specific question you are investigating is, why these things are interesting/relevant (the stakes of the research), and have a sense of what your answer to the question is.\nHow should you go about writing this section and accomplishing these goals? Here is an outline of a paper I recently wrote that exemplifies one way of approaching these tasks.1 The bit after the colon (e.g., “Paragraph 1: blah blah blah”) indicates the general goal of the paragraph, while the bit after “Example:” indicates the specific point I was trying to make in that paragraph in this particular paper. This is an example of how to think about using an outline to help you structure your thinking and make it easier to write (see Chapter 7).\n1 Although, given the nature of time, “recently” here is receding further and further into the past.\nParagraph 1: Here is an important subject to consider\n\nExample: People follow party cues with potentially problematic normative implications for democracy.\n\nParagraph 2: Ah, but there is something we don’t know…but should!\n\nExample: Previous work hasn’t examined the effectiveness of party cues when other actors (journalists, rival politicians) allege that the party is motivated by ulterior motives despite this being a common element of political rhetoric\n\nParagraph 3: Preview of your argument\n\nExample: I argue that cues will be less effective in these circumstances because cue taking is built on trustworthiness and these messages undermine trustworthiness.\n\nParagraph 4: Preview of study and findings\n\nExample: I use an experiment where I randomly assign party cues with and without these types of messages; I find that cue taking is indeed undermined.\n\nParagraph 5: Roadmap of paper (potentially)\n\nIn the above, I first try to establish that people should care about party cue taking by highlighting findings that people actually do use them (i.e., they’re a common element of politics) and also that there is something at stake in their use. In this case, the idea is that cue taking may threaten an understanding of democracy wherein elected officials follow the wishes of the public rather than the public simply following its leaders.2 I have thus set up some broader stakes for the paper by tying it to more general normative concerns about democracy. Identifying the stakes of the paper is important; the reader should have some sense of an answer to the ‘so what’ question (i.e., why should we care about this?) before they exit the introduction. In the second paragraph, meanwhile, I tried to highlight a gap in the literature - something we don’t know about. I then gave a preview of my particular argument and what I found. This is something that BAP students sometimes fail to do when writing their thesis to their detriment. As Davis and Musgrave note, the introduction is “not a murder mystery”. You do not want to bury the lede about what you argue and ultimately find! Instead, you should give the reader a roadmap into what is coming further on in the thesis.\n2 Druckman offers a discussion of this tension from the perspective of a public opinion researcher and whether the potential “endogeneity” of public opinion to elite communicates means that the public does not hold “quality opinions”. Disch offers a discussion from the perspective of a normative political theorist and provides an alternative understanding of responsiveness and mass opinion.There are different ways of structuring introductions and, indeed, of motivating the importance of research questions. I strongly recommend reading Three Templates for Introductions to Political Science Articles by Andrew T. Little as this document provides some great ideas of how to effectively structure this section of a paper. In my rendition above, I did this by highlighting something missing from previous research on this topic. In the actual paper, I actually used something like one of the templates identified by Little wherein I suggested that different theoretical traditions in this field predict different outcomes (i.e.: There is something common in politics we don’t know about and, moreover, Theory A predicts X about it while Theory B predicts Y…which one is correct?).\nIn the outline above, I placed a final entry: “Roadmap (potentially)”. This is a reference to ‘roadmap’ paragraphs in which the author outlines the steps they will take in the paper to come (I will first talk about X; then, I will discuss why that is insufficient; then I’ll do this…). I am personally agnostic about the inclusion of such roadmaps in the introduction to a thesis paper. Most of the time the structure is pretty common so its inclusion feels unnecessary. Not everybody feels the same, so it may be sensible to include a short roadmap to cover your bases.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Introduction</span>"
    ]
  },
  {
    "objectID": "writing_03_review.html",
    "href": "writing_03_review.html",
    "title": "3  Review of Literature and Argument",
    "section": "",
    "text": "3.1 Goals\nThe next portion of your paper is the literature review and theory building portion. You should aim to accomplish several things in this section:\nYou should have already previewed these points in the Introduction. However, here is the place to unpack that shortened preview into something with more depth.\nOne issue students sometimes have in writing this portion of their thesis is an over-focus on goal 1 to the detriment of goals 2 and 3. BAP literature reviews can sometimes read as a laundry list of readings (and this person did X and then another person did Y, and then this other person did Z) without it being clear how these different readings relate to one another or what is really the problem that you will be addressing. However, everything you write in the thesis should have a reason or point for being. Every paragraph should build on the last and help you toward realizing the goal of persuading the reader that there is something we should know but which we don’t and that you have a (potential) answer to that question and hence the content of your contribution.\nOne way of thinking about this is via a discussion from an article by Siddaway, Wood, and Hedges. The goal of this manuscript is to discuss best practices when writing what is known as a ‘systematic reviews’ of a research literature. The goal of a ‘systematic review’ is to “bring together, synthesize, and critique one or more literatures to provide an overall impression of the extent, nature, and quality of evidence in relation to a particular research question, highlighting gaps between what we know and what we need to know.”\nSiddaway, et al. discuss two types of systematic review. The first is meta-analysis in which the researcher gathers together as many published and un-published research articles on the subject at hand and then merges them together into a single dataset for subsequent quantitative analysis. The second is a “narrative” literature review in which the author still systematically canvasses a literature but instead provide a more qualitative discussion of major themes in the work under discussion.\nSiddaway et al. draw a distinction between “reviewing literature” and “literature reviews” (by which they mean these systematic reviews):1\nIdeally, you should put on the hat of judge for the assigned literature review in the substantive seminar (albeit on a smaller sampling of research). In your thesis, on the other hand, compose your review of the literature more like a lawyer. You are not just telling us what others have found, but doing so in a way to scaffold your own argument and contribution.\nMusgrave and Davis make a similar point when they tell students not to write “literature reviews” in their thesis:\nThese authors have a nice discussion of some ways of thinking about narrow (but important) ways that a senior thesis can contribute to a research literature in this post.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Review of Literature and Argument</span>"
    ]
  },
  {
    "objectID": "writing_03_review.html#goals",
    "href": "writing_03_review.html#goals",
    "title": "3  Review of Literature and Argument",
    "section": "",
    "text": "Tell the reader what we already know about this subject\nTell the reader what we don’t know (e.g., that something isn’t studied, or that there is a conflict between theories, etc.)\nTell the reader what your answer to the ‘what we don’t know’ issue is and try to persuade them that it is a good answer\n\n\n\n\n\n\n1 They discuss reviewing literature in the introduction in this excerpt. The authors are psychologists and articles in psychology often feature less of a separation between the introduction section as discussed in the preceding chapter and the ‘literature review’ section as discussed here.\nBefore discussing systematic reviews and the different types of literature review, it may be instructive to dispel two common misunderstanding about [systematic] literature reviews. The first is that conducting a literature review is the same as the task of reviewing literature, which occurs when writing the introductory section of all quantitative and qualitative journal articles (including review articles). Reviewing literature involves selectively discussing the literature on a particular topic to make the argument that a new study will make a new and/or important contribution to knowledge. In contrast, literature reviews make up a distinct research design and type of article in their own right. Rather than selectively reviewing relevant literature to make a flowing rationale for a study’s existence, they provide a comprehensive synthesis of the available evidence to allow the researcher to draw broad and robust conclusions….To best achieve the purposes of a systematic review, we like Baumeister’s (2013) advice to adopt the mindset of a judge and jury rather than a lawyer. A judge and jury skeptically evaluate the evidence to render the fairest judgment possible, whereas a lawyer’s approach is to make the best case for one side of the argument. Returning to the differences between a literature review and the task of reviewing literature, the introduction section of a quantitative or qualitative article is usually written using the lawyer’s approach.\n\n\n\n\nThe “literature review” in a thesis, by contrast, has to establish what is relevant in prior arguments to situate your own work. Some of this should reflect your own reading and research, but a good deal of it should come from the suggested readings from your adviser. Your goal with the literature review is sufficiency, not novelty and certainly not mastery. All you have to do is demonstrate that your project is not adrift alone somewhere but, rather, that it is directly linked to earlier research….Your goal is to demonstrate how your contribution fits with that conversation, and to do that you will also have to show shortcomings, conflicts, or unresolved issues in some aspect of that earlier conversation. Moreover, the problems you uncover need to relate to the work you will be doing—there’s no benefit in showing that the theory is flawed if you’re writing a methods paper, and there’s no benefit in showing that methods are flawed if you’re writing a theory paper.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Review of Literature and Argument</span>"
    ]
  },
  {
    "objectID": "writing_03_review.html#specific-tips-points-and-warnings",
    "href": "writing_03_review.html#specific-tips-points-and-warnings",
    "title": "3  Review of Literature and Argument",
    "section": "3.2 Specific Tips, Points, and Warnings",
    "text": "3.2 Specific Tips, Points, and Warnings\nThe following sub-sections provide some additional thoughts on approaching this portion of the paper.\n\n3.2.1 Focus on Theory\nA key goal of your ‘theory’ section is to describe for the reader the major approaches relevant to your question; why they don’t seem to actually answer your question (or do so in some unsatisfactory manner); and, ultimately, what your answer is to the question at hand. And, of course, to do this in as efficient an amount of space as possible.\nMy general advice here is to abstract away from the particular a little bit and focus on the general or abstract. In other words, try to avoid what I’ll call a ‘listing’ approach to the literature and focus more attention on the theories that connect the reviewed research (the general) rather than detailing every detail of the specific articles (the specific).\nAs an example, let’s return to the example of a paper on whether accusations of ulterior motivation undermine the effectiveness of party cues on public opinion (a paper topic introduced in Chapter 2). I might begin by reviewing a variety of articles on party cue taking. I might write:\n\n“Cohen (2003) uses lab experiments to study this topic. He finds that cues matter irrespective of policy specifics, i.e. people choose party over ideology. [New paragraph] Bullock (2011) uses a survey experiment to show that substantive information about a policy undermines cue taking. He does this by doing X and Y. [New paragraph] And then there is Bolsen et al (2014). Bolsen et al. recruit subjects from the Bovitz online panel and randomly assign people to various conditions. They then do all these analyses and find that cues matter a lot. [New paragraph] And then there is….”\n\nThe issue here is that your reader will likely be confused and uncertain as to what the central or most important point is of all this listing. What is the main point above beyond, perhaps, that there is variation in the literature? This might be an important point, but it does not, by itself, shed light on why party cues might matter and why, in turn, accusations of ulterior motives might moderate their influence. And, even if this is the main point, it could surely be stated much more simply!For instance:\n\nResearch is divided on the influence of party cues with some finding that they dominate other considerations (Cohen 2003; Bolsen et al. 2014) but others finding that they have a limited impact in more realistic environments (Bullock 2011; Boudreau and Mackenzie 2014)\n\nInstead, consider organizing your literature review around theories and prominent answers to your question and use specific articles to elaborate on key elements of those theories as needed. In the example above, I could instead write:\n\nMany studies suggest that party cues matter, although they vary in the theoretical basis for their claims. Importantly, these theoretical claims suggest divergent reactions to party cue taking when accusations of ulterior motivations are present. [New Paragraph] On the one hand, some authors root cue taking in social identity processes. Here, partisanship is a social identity formed young in life and then reaffirmed over time due to psychological processes that ‘bias’ the reception of new information (Cohen 2003; Bolsen et al. 2014; other relevant citations). As an example, consider Bolsen et al…. Based on this perspective we might assume that Z will not matter for cue taking because…. [New Paragraph] While the social identity approach is perhaps dominant, other scholars consider partisanship from a different lens. Here, partisanship is a running tally based on performance evaluations (Fiorina 1981; Achen 1992). Party cues are followed only in certain cases, such as X and Y. For instance, Bullock (2011) shows…. Based on this perspective, we would expect reduced cue taking because…\n\nThis latter style focuses attention on what matters: the theory or theories involved in this debate, what they imply for the research question, and the evidence relevant for the paper’s argument. You can still use articles/books/book chapters here, but they are meant to elaborate on or exemplify something more general.\nThere may be exceptions to the above of course. If you theory section requires you to directly contrast two different articles, for instance, then that would be fine. But, in general, I think it wise to root yourself in the theoretical approach(es) that underlie the articles you are talking about - to make sure that these are clearly described and discussed.\nSee this post by Steven V. MIller for some additional thoughts on structuring literature reviews. Read Musgrave and Davis on “how to talk about other people”.\n\n\n3.2.2 Draw it out before you write\nIn class you will be introduced to the practice of drawing a causal diagram to think through the theoretical assumptions underlying a paper’s argument. This is a good thing to do for your thesis as well as you prepare your literature review and argument portion.\nFor instance, suppose that there is one or two key theoretical traditions in the literature that you are reviewing. In my party cues example, these might be instrumental approaches based on rational choice theory (people follow party cues as rational shortcuts based on feelings of trust) and social identity theory (people follow party cues because they identify with the party and are motivated to be good group members). It can make sense to draw out a causal diagram for each theory linking the IV and DV in an attempt to think through what the theory takes as necessary for the IV to affect the DV (i.e. what are the mechanisms) as well as whether there are potential moderators that the literature has not examined or alternatively potential confounds. In so doing, you may be able to better identify gaps in the literature and hence see the path forward to writing your literature review.\nThis can also be useful for you in thinking about the analyses that you want to perform. Suppose that you are writing a paper on the subject of political ideology and trust in the legal system. Perhaps you hypothesize that trust will be higher among those on the right-side of the ideological spectrum than the left and find data relevant to examining hypothesis. Okay…what should you control for in your model? Going through the steps of drawing out a causal diagram for yourself might help you further refine your argument about why this relationship should emerge (e.g., through the process of identifying potential mediators of the relationship) as well as helping you think about what the most important confounds of the relationship happen to be (age? education? openness to experience?). You can then discuss why you include particular variables as controls in your thesis. In addition, if there are important confounds that you cannot control for, well, you have now identified some of them and have something to discuss in your conclusion.\nNote: while I think causal diagrams like the ones we’ll use in class are good tools to use, you do not need to then include them in the final thesis.\n\n\n3.2.3 Stating your hypothesis/hypotheses\nExplicitly state clear and testable hypotheses. What you expect may be implicit, or even clear, based on what you have written. But, that isn’t always the case from experience; what is clear in your mind may not be clear to the reader. So, explicitly state your hypotheses. This is preferably done outside the confines of a paragraph, e.g. below it (see the image in Chapter 1 for a better example).\n\nHypothesis 1: Blah blah blah.\n\nYou should not include the rationale for the hypothesis in the statement of the hypothesis. For instance, you might be tempted to write: “Hypothesis: Y will increase alongside X because of these following reasons”. But, you should have already discussed those reasons in the foregoing paragraph(s) so there is no need to reiterate them in the hypothesis. Focus the hypothesis on the specific thing being tested.\nMake sure that your hypothesis/hypotheses is clear and falsifiable. Instead of “Hypothesis 1: There will be a relationship between X and Y”, one could state “Hypothesis 1: As X increases, so will Y” or something like that. This is better because it gives the specified direction of the relationship. It is clear what would falsify the expectation (i.e. a null relationship or even a negative one).\nYou do not need to state the null hypothesis. The only exception here is if the null is something other than “zero effect of the IV”.\nIf you write a hypothesis that implies multiple tests, then I think you should probably break it up. So, for instance, you might write: “Hypothesis 1: Voter turnout will increase alongside education, age, and social network size”. Now, if your intent is to consider the joint influence of these three things (i.e. you are comparing an old person with lots of education and a big network to young people with little education and a small network) then that may be fine. But, probably you are instead intending to independently test these relationships (i.e. is turnout higher among older people?; is turnout higher among the college educated?; is turnout higher among those with bigger social networks?). In that case, then separate them into distinct hypotheses (H1, H2, and H3) to be more clearly communicate your expectations.\nAs a final note, it is probably not a good idea to provide the hypotheses in a separate “Hypotheses” section (and definitely not in the Research Design section). Doing so can break the connection between argument and hypothesis and make it more difficult to understand why you’re claiming what you’re claiming.\n\n\n3.2.4 Focus on what is important\nYour analyses will involve running a regression of some type (linear, logit, etc.) with the goal of attempting to uncover the relationship between X and Y while avoiding, as far as possible, worries of omitted variable bias unless you are analyzing experimental data. In so doing, you will want to include various control variables. You will need to tell us about those variables. The trick here is giving the reader just enough detail about what is being included and why without giving so much that the reader’s attention wanders.\nYou do not need to discuss what control variables you intend to use in this portion of the thesis. Instead, focus your attention on advancing a clear and persuasive argument for why X \\(\\rightarrow\\) Y. The influence of other variables can and should be discussed here to the extent that they shed light on that relationship, e.g., moderating variables (variables that augment the relationship between X and Y).",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Review of Literature and Argument</span>"
    ]
  },
  {
    "objectID": "writing_04_design.html",
    "href": "writing_04_design.html",
    "title": "4  Research Design/Data Section",
    "section": "",
    "text": "4.1 Goals\nThis section of your thesis has a fairly straightforward goal: telling your reader why your analyses take the form that they do (e.g., why this data, why these measurements, why this model, etc.). I have noticed that students often structure this section in very…interesting ways. So, I will discuss how best to structure this area of your thesis and some other supplemental points below.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Research Design/Data Section</span>"
    ]
  },
  {
    "objectID": "writing_04_design.html#basic-structure",
    "href": "writing_04_design.html#basic-structure",
    "title": "4  Research Design/Data Section",
    "section": "4.2 Basic Structure",
    "text": "4.2 Basic Structure\nHere is a general guideline on how to structure this section:\n\nData: Where does the data come from/how was it created?\nDependent Variable: How are you operationalizing your dependent variable(s) using this data?\nMain Independent Variable(s): How are you operationalizing your main independent variable(s), i.e., the predictor variable(s) specified in your hypothesis/hypotheses? This also covers a discussion of any moderators although you should discus the main IV first and then the moderator.\nModel and Controls: How are you actually going to test your hypothesis/es? What variables are you including as controls while doing so (and why them)?\n\nThis structure is my general recommendation and it should apply to nearly all theses in my BAP. However, I could see some room for deviations in situations where multiple data sources are being used in the analyses. The above, for instance, works great if you’re using data from something like the World Values Survey or European Social Survey by itself. But, your particular project may require a combination of datasets. If this involves simply adding some data to an existing survey dataset (e.g., merging data on economic growth from the World Bank into your European Social Survey dataset), then I would keep the general structure the same as above and talk about the source of the added data when you introduce the variable in question (e.g., in the Main Independent Variables or Control variable sections).1 On the other hand, your project may require you to combine multiple databases together into a wholly new data source.2 If that were the case, then I would skip the first bit on Data and discuss the source of each variable as you introduce it.\n1 See the Statistics I R Book for how to “join” datasets (weblink).2 As an example, a student in a prior year wrote a BAP predicting hate crime prevalence in the United States based on economic conditions and various other predictors. The student combined data on hate crimes from the US Department of Justice, data on economic conditions from another source, and so on, into a new dataset.Note: It generally makes sense to have these discussions as separate subsections with bolded section headers. See Chapter 1 for an example.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Research Design/Data Section</span>"
    ]
  },
  {
    "objectID": "writing_04_design.html#some-more-specific-points",
    "href": "writing_04_design.html#some-more-specific-points",
    "title": "4  Research Design/Data Section",
    "section": "4.3 Some More Specific Points",
    "text": "4.3 Some More Specific Points\n\n4.3.1 Discuss(ing) how the data was generated\nYou are probably using a secondary data source: the American National Election Studies, the World Values Survey, and so on. Per the structure above, you should probably begin this section of the thesis by telling your reader(s) about this data source. A stronger discussion here goes beyond simply saying “I’m using ANES data” to tell the reader a little bit more about how the data was actually generated by the people running these surveys. How as the sample generated? (Probability-based sample? Convenience?) How were people interviewed? (In-person? By phone? Online?). When was the data collected specifically?\nThe answers to some of these questions may be important for understanding your results and potentially their limitations when it comes to generalization. For instance, perhaps the survey is done using a face to face interview. If so, are any of your core variables “sensitive items” that may invite (some) people to hide their true preferences due to social desirability pressures? If so, then then what does that mean for your results? Does it make it easier or harder to find evidence consistent with your hypothesis(es)?\nBuilding on this last point, you will want to discuss potential “limitations” with your data and how they may affect your results (see also, Chapter 6). Where should those discussions occur? I would generally say that it is okay to preview this discussion in this section of the paper with a fuller one in the Conclusion since you will want to incorporate into this discussion some thoughts on how the “limitation” in question might be addressed by other researchers (and what might happen if they did so).\n\n\n4.3.2 Assumption checks\nYou should check the assumptions underlying the particular model that you are using. The results of these assumption checks can be presented in an Appendix (which does not count against your overall word count) and referenced in the Model and Controls sub-section. I would recommend not bogging this discussion down with too much detail about the procedures you took and their results as you can leave that for the Appendix. Instead, you would want to clearly and efficiently communicate that you checked the assumptions and (1) they all more/less checked out so you did not need to take any additional steps or (2) you identified some issue, in which case you’d note how you updated or altered your statistical model to address it (e.g., the inclusion of a squared term, you used some other type of standard error, etc.).\nAn additional point about this Appendix: you should include an actual discussion of the results of your assumption checks. In other words, don’t just plop down some graphs or tables and leave them as is, but instead also include a short interpretation of those results. Treat the assumptions appendix much you would the analyses in text - with polished tables/graphs and clear discussions.\n\n\n4.3.3 Talking about models and control variables\nYou should tell your reader how you are analyzing your DV.\nThe first component of this discussion is a note regarding the specific type of statistical model that you are using and why you have chosen this one. This generally does not require very much elaboration as the type of analysis is going to be determined the nature of your DV (i.e., continuous DV = OLS, binary = logistic). So, a simple proviso is all most of you will need: “I use an [model] because my dependent variable is [scale]”. Of course, your specific project may use other types of data that may entail some other modeling strategy. Maybe your DV is categorical in which case you need something like a multinomial logistic model (see SECTION for more on this). Perhaps you have a “clustered” dataset, in which case you would need to do something to deal with that and make a note of what that “something” happens to be (see SECTION for more on this topic).\nYou also need to tell the reader what else (besides your main IVs of interest) is going into this model to predict the DV. In particular, you need to tell the reader what you are controlling for in your model(s) and sufficient information about how they are coded/scaled so that your reader can understand your regression output. In general, I would recommend leaving the in-depth details about the measurement (e.g. question wording) and coding of control variables to an Appendix with a reference to this appendix in the main-text. It’s still a good idea to have some mention of the direction of the variable’s coding in the main text (e.g., higher values on the variable = ?) though. This information could be a simple bullet point list like so:\n\nIdeology\n\nRespondents were asked the following question: “In general, how do you place yourself on this scale from 0-10 where 0 = left, 10 = right” (mean = X; SD = X).\n\nEducation\n\nRespondents were asked to indicate the highest level of education that they had finished with the following options: (1) less than secondary education; (2) secondary; (3) tertiary; (4) etc. I created a binary version of this variable where 0 = less than secondary (X%) and secondary and 1 = tertiary and post-collegiate degrees (X%).\n\nVariable 3\n\nDetails\n\n\nThe idea here is to give the reader sufficient information so that they can understand your model without bogging down the discussion with potentially superfluous details. The appendix is one way to thread that needle.\nAs an additional point here, it makes sense to include a short point about why you are including these particular variables as controls. That is, justify their inclusion in some way. Typically, we control for variables to reduce bias in our models - we are worried that the relationship between our main variable of interest and the DV is actual spurious in nature because this relationship occurs due the correlation between them and confounding variables. A justification on this front may highlight a reason (such as previous studies) suggesting that the control might cause both the DV and the main IV of interest in the model with a reference to relevant work to give warrant to the claim.\nI discuss additional questions and issues in relation to statistical modeling that you may encounter in the final section of this book.\n\n\n4.3.4 Don’t use SPSS/R variable names in-text\nYou may be tempted to refer in text to one of your variables by the name you gave it in R (e.g. something like “lrscale” to capture ideology). Do not do this. You are only forcing your reader to remember more things and do more work, which in the process may confuse matters for them. More work for a reader means a more disagreeable reader. Instead, refer to the concepts that these variables are trying to measure or operationalize.\nAnother version of this is the tendency to talk about the minute procedures you undertook to analyze the data, e.g. importing the data into R from an .csv file or something like that. Unless these procedures required some out of the ordinary effort this is almost certainly unnecessary as it is unlikely to convey any useful information. Don’t waste words!\n\n\n4.3.5 Provide descriptive data!\nWhen you have your data ready to analyze you will probably want to just jump straight into multivariate models. Resist this temptation. Instead, get to know your data. How much variation is there in your key variables? Are they skewed? Are there potential outliers? What are the bivariate relationships between your core variables (and particularly your main IV and the DV)? Doing so may help you identify potential issues that you will need to address as well as giving you a deeper understanding of your data that can help you better discuss you results.\nWhen you turn to writing your research design and results sections, it is a good idea to present/discuss some of these descriptive results to give the reader a sense of the underlying data, although avoid bogging the discussion down with too many minute details. At the very least, you should provide some descriptive data about your main IV(s), any moderator, and your DV. This would typically focus on their mean and measure of dispersion (e.g., standard deviation or confidence interval for continuous variables) or frequencies (for binary or categorical variables). This could also include some bivariate analyses (e.g., contingency tables, correlations), although bivariate analyses between your main variables may be more useful as the start of the analyses section.\nHow should you present this data? Simple summary statistics about a variable can often be communicated in-text. For instance, you might write something like:\n\nWe use two questions as dependent variables. First, respondents were asked about their level of democratic satisfaction (“how satisfied are you with the way democracy is working in [country name]”) with four response options (very satisfied, somewhat satisfied, not too satisfied, and not at all satisfied). We rescaled this variable to range from 0-1 with higher values indicating more democratic satisfaction (mean = 0.518 [95% CI: 0.513, 0.522]). Second, respondents were asked whether their country’s political system “needs to be completely reformed, needs major changes, needs minor changes, or doesn’t need to be changed”. We use this question as an indicator for system legitimacy. We rescaled this measure to range from 0-1 but here higher values indicate support for changing the system and hence lower legitimacy (mean = 0.599 [0.594, 0.603]).\n\nYou could alternatively provide this information in a table or graph (boxplot, histogram, etc.) as well.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Research Design/Data Section</span>"
    ]
  },
  {
    "objectID": "writing_05_analyses.html",
    "href": "writing_05_analyses.html",
    "title": "5  Results/Analyses Section",
    "section": "",
    "text": "5.1 Goals\nYou will naturally discuss the results of your analyses after discussing your data. This section is fairly straightforward in that its central goal is to tell the reader what you have found and how this relates to your hypothesis/hypotheses. What follows are some general points on the content of this section.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Results/Analyses Section</span>"
    ]
  },
  {
    "objectID": "writing_05_analyses.html#some-more-specific-points",
    "href": "writing_05_analyses.html#some-more-specific-points",
    "title": "5  Results/Analyses Section",
    "section": "5.2 Some More Specific Points",
    "text": "5.2 Some More Specific Points\n\n5.2.1 Structure in the same order as your hypotheses\nIf you have more than one hypothesis, then you should discuss them in order: analyses relevant to H1, then analyses relevant to H2, etc. It may make sense in this case to create subsections for each test, but that is not always necessary.\n\n\n5.2.2 Restate your hypotheses when you start discussing them\nRemind your reader about your expectations when you start discussing your analyses and be clear about what would fit with those expectations in the analysis/es you are showing.\nSo, to return to the running example of whether accusations of ulterior motives undermine the influence of party cues, I might begin this section by writing something like:\n\nI argued in Hypothesis 1 that receiving an in-party cue should result in greater policy support among partisans. Table 1 provides the results of a regression model where policy support is regressed receiving a in-party cue or whether they did not (baseline condition). A positive coefficient would thus be consistent with Hypothesis 1. And, indeed…\n\nThen, I might write something like this when discussing a secondary hypothesis:\n\nTable 1 shows that party cues mattered. However, in Hypothesis 2 I argued that this effect would be conditional, i.e. it would be smaller when the party was described as motivated by ulterior goals. I test this claim by including an interaction term between Variable1 and Variable2; see Model 2 in Table 1. A negative coefficient on the interaction term would be consistent with my hypothesis. And, indeed, Table 1 shows…\n\nDoing this makes sure you and your reader are on the same page.\n\n\n5.2.3 Go beyond “statistical significance”\nIn interpreting your results, you might be tempted to just say “well, there is a statistically significant (or insignificant) effect of the independent variable” and leave it at that. But this does not tell the reader very much. And, indeed, it elides the more important question of substantive significance as one could obtain an estimate that one is confident is more than statistical noise but which is unimportant.\nWhen writing up your results, discuss the direction and magnitude of results with some level of specificity. One way to do this is to interpret the direction and magnitude of the effect implied by the coefficient in your model. So, for instance, if the coefficient is 0.8, you might say: “for every one unit increase in my variable, the DV is expected to change by 0.8 [DV units] on average”.1 It if often a good idea to then supplement such a statement with predictions from the model: “Based on this model and its results, we would expect that someone at the minimum of X to score a 2 on the scale, while someone at the maximum of X would score a 6” (or something like that). One can also do this with logistic models using average marginal effect estimates and/or predicted probability estimates. Incorporating a figure to show these changes is also a wonderful idea although it is not always needed.\n1 Well, if the IV is continuous. If it is a binary or categorical variable, then you would discuss this as a difference in means test: is the mean in category A different from the mean in category B? And, of course, we would not use quite the same language when describing a logistic regression model since the coefficients of that model are on the hard to communicate log of the odds (logit) scale. A focus on average marginal effects and predicted probabilities would be more useful there.2 Moreover, a statistically significant relationship does not necessarily mean that your hypothesis is true. Maybe your DV is actually causing your IV (e.g., “reverse causality”). Or perhaps you have an omitted variable that confounds the relationship. Meanwhile, an “insignificant” result could result from measurement error or a small sample.One thorny subject here is substantive significance. The p-value associated with a regression coefficient might be small leading you to “reject” the null hypothesis. But, that doesn’t mean the effect in question is particularly important! We can precisely measure small effects with enough data after all.2 It is thus a good idea to give some consideration to whether the effect that you are observing is “meaningful” or “substantively” important. What characterizes a “substantively” important finding, however, is not always clear. This can depend on the topic in question and our prior knowledge about it. Consider a study investigating variation in the percentage of votes cast for an incumbent party that finds that a one unit change in X leads to an expected gain of 1% more votes. If the incumbent party in the context of this study typically wins elections and does so by 45-50% on average, then a 1% change due to X is probably irrelevant in the “real world”. However, if real elections are typically within 1-3% on average, then a 1% change due to X could mean a different winner and hence a represents a “substantive” or “important” effect! So, there is no straightforward answer here. This is one reason why I suggest incorporating predictions from your model into the discussion of your results as these may aid in interpreting substantiveness in light of your knowledge of this case and the literature regarding it.\nDiscussing the substantive importance of coefficient can be tricky. I discuss this point in more detail in a chapter in the final section of this book.\n\n\n5.2.4 Discussing Control Variables\nYou’ll likely have various control variables in your model (unless you are analyzing an experiment). How much attention should you give them in your discussion of results? The answer is “probably not much if any”. Recall that the standard justification for why we “control” for other variables is to avoid statistical bias when estimating the relationship between the predictor variable we care about and the DV. In other words, we have some reason to believe that the “control” variable causes both our main IV of interest and the DV such that failing to include it in the model would yield faulty conclusions about the relationship between that main IV and the DV. We are not including the control variable in the model, in other words, because we actually care about its direct relationship with Y…so why should we then spend a lot of time talking about it? Indeed, every word you spend on talking about the “controls” is a word you could be using to better set up your research question, or justify your theoretical argument, or elaborate on the implications of your results, i.e., the stuff your readers will most care about.\nThere is an additional reason why you might want to limit your attention to the control variables: they don’t necessarily tell you what you think they’re telling you (which can lead your discussions into error-prone directions) and what they are telling you may not be very interesting on its own.\nConsider the following simplified causal diagram:\n\n\n\n\n\n\n\n\n\nThe diagram above shows a set of theorized causal relationships wherein a person’s immigration attitudes (DV) are influenced by their own educational attainment (main IV) and a potential confounder (Parents Education). Parents Education is thus theorized to influence Immigration Attitudes and to do so, at least partially, because of its influence on a person’s Own Education. If our interest is on obtaining an unbiased estimate of the relationship between Own Education and Immigration Attitude, then we should include Own Education and Parents Education in our resulting statistical model.3 The coefficient for Own Education would tell us about the relationship between Own Education and Immigration Attitudes “controlling for” or “after adjusting for” Parents Education. In essence: if we compared people with different educational backgrounds but whose parents had the same educational attainment, then what is the difference in immigration attitudes that we’d expect to see?\n3 There are plausibly other confounds out there of the education/immigration relationship, but I am keeping the figure simple. Hainmueller and Hopkins provide a relatively recent review of the literature on immigration attitudes.What would the coefficient for “Parent Education” represent in such a model? Well, it would provide us with an estimate of the relationship between this variable and the DV that is unrelated to “Own Education” (e.g., the influence of this variable after adjusting for differences in Own Education). Stated differently: it is an estimate of the relationship between “Parent Education” and “Immigration Attitudes” that emerges due to other mechanisms/mediators besides “Own Education”. For instance, it might represent the influence of Parent Education on Immigration Attitudes that works through a person’s ideology, or their self-interest, etc. (unless those things are also controlled in the model!). It is thus not an estimate of the total relationship between Parents Education and Immigration Attitudes but the relationship that is left over after addressing one of the mediator’s of this variable’s relationship with the DV. This coefficient is thus a “biased” estimate of the total relationship between this variable and the DV. Specifically, it suffers from what is known as post-treatment bias. Any resulting discussion of this coefficient that does not take this fact into account would thus fall into error…but why risk error if this topic is kind of besides the point to begin with?\nSo, in general, I would tell you to focus on just the variable(s) relevant to your hypothesis/es and interpret them. There might be some reason to deviate from this rule if something really surprising or odd turned up, but that is something we would need to discuss with each other.\n\n\n5.2.5 Talking about R2\nStudents writing the BAP can sometimes seen quite preoccupied with the R2 of their model and whether it is “too low” or not. You probably don’t need to worry much about that and, even, have to talk about the R2 at all. Let me explain why.\nThe first reason why talking about R2 is not usually necessary concerns the purpose of what you’re doing. Regressions can be used to try and build strong predictive models, e.g., to build a model to try and make accurate electoral forecasts. In that context,R2 may be quite relevant as a model that doesn’t explain much variation in existing data may generate inaccurate predictions for new data.4 However, your goal is likely more focused on using a regression model to say something about a particular variable and its relationship with Y: to describe that relationship and, contingent on research design considerations and assumptions about omitted variables, to say something about whether X causes Y. R2 is not particularly relevant for that latter purpose. The more important things to focus on are the coefficients for the variables core to your hypotheses, the predictions they generate, and the (un)certainty surrounding them (e.g., confidence intervals and standard errors).\n4 Although, the reverse is also true. A highly predictive model with one set of data might not generate accurate predictions on a different data set!Second, R2 has some potential issues that have led some to question its validity as a measure of model fit to begin with. See here for a deeper dive.\n\nIt assumes a linear model, which means that you can get a very low R2 even with a very strong relationship between X and Y if the relationship is non-linear and you have failed to account for that aspect of the data.\nYou can theoretically get a “good” or “high” R2 with a silly model. For instance, if I were to regress a person’s height on the height of their parents (as measured before the parents had the child), I’d probably get a decent R2 value. Height, after all, is one of the most heritable traits we have. If I were to reverse that regression, I’d still get a good R2, even though a child’s height cannot go back in time and cause their parents’ height! Or, as an alternative example, if I were trying to explain a person’s vote choice, I could survey them five minutes beforehand and ask them whom they intended to vote for. Including that variable in a resulting model would certainly lead to a very high R2, but it wouldn’t tell me anything about why they voted for the party/candidate they voted for because the high R2 is emerging because you’ve included an IV in the model that is essentially the same as the DV!\nR2 values are inherently sample dependent. If I field a survey and estimate a model and then you field a separate survey with the same exact measures and run the same exact model we’d expect the R2 to be non-identical (although, hopefully, quite similar).\nFinally, what counts as a “high” R2 can be field specific. R2, in essence, tells you how much systematic variation in Y you are capturing with your models versus residual noise…but some Y variables are simply noisier than others. Individual human behavior is much more noisy than, say, the behavior of molecules under controlled conditions. A discussion of whether R2 is high or low must then proceed from knowledge of how much variation researchers can typically explain with their models in the domain of study rather than as a blanket statement.\n\nR2 statistics can have some value. In particular, they are helpful in comparing models using the same data. If you run one model and then another which includes an additional variable, then it can make sense to talk about how this did or did not increase the variance explained. However, to go back to point one, even that is not necessarily the relevant consideration.\nThe point is not to never talk about this statistic. Rather, you shouldn’t use it as a heuristic for judging the quality of your, or other’s research or, at least, use it by itself to make those judgments. Research quality is dependent on the clarity of theory and its relationship to existing knowledge, the nature of the sample, how variables are measured, and what goes into the model. R2, as well as other model fit statistics, can be useful in this endeavor, but only as one piece of evidence.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Results/Analyses Section</span>"
    ]
  },
  {
    "objectID": "writing_06_conclusion.html",
    "href": "writing_06_conclusion.html",
    "title": "6  Conclusion",
    "section": "",
    "text": "6.1 Structure\nIn the Introduction, your goal is to motivate a research question and then preview your answer to that question, how you tested that argument, and what you found. Something like this (although, obviously, the format here will depend on your question, argument, data source, etc.)\nYour conclusion should concisely do the above but as a jumping off point for a discussion of the limitations and broader implications of your results. A potential structure could be:",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "writing_06_conclusion.html#structure",
    "href": "writing_06_conclusion.html#structure",
    "title": "6  Conclusion",
    "section": "",
    "text": "Many people say X. But, we actually see Y. Why is that? Well, I argue that X occurs when Z is high but does not when Z is low. I tested this argument using evidence from Data Source. I first regressed Y on X and found that that Y happens, much as earlier work shows. But, when I include Z as a moderator, I found support for my argument. This has important implications for something which I discuss in the conclusion.\n\n\n\nReview of what you’ve done\n\nIt is important to understand whether parties can dominate public opinion. I argued that party influence is conditioned by X. I showed via some method that this indeed appear to be the case. While cue taking occurred when X was absent, their power diminished when X was present. In the remainder of this conclusion, I discuss limitations of this study and potential avenues for future work.\n\nLimitations and future work\n\nI used a one time experiment, but party messages are experienced over time. We could study this in this way. We might find this which would have this implication.\n\nBroader implications\n\nSome think party cue taking threatens norms of democratic representation, but that’s only a problem if people follow them unthinkingly and I’ve shown that’s not an issue. This means X for democracy.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "writing_06_conclusion.html#be-specific",
    "href": "writing_06_conclusion.html#be-specific",
    "title": "6  Conclusion",
    "section": "6.2 Be Specific",
    "text": "6.2 Be Specific\nOne thing you are trying to communicate here is that you have reflected on your project; that you have critically thought through your methodological decisions and their implications for your results and how others should relate to them. Another is to expand on why your study has some further interest for other people.\nOne important point I’d like to communicate here: be as specific as possible when you discuss the limitations of your study. Many times students say “well, I focused on X cases which might be problematic” and leave it at that. However, a comment like that is really just the start of the conversation. Indeed, it could very well be the case that the relationship between X and Y is basically the same across contexts! For instance, consider this paper which examines the generalizability of experiments commonly done in the US in a multiple of countries and basically finds that treatment effects are consistent across context (at least, for the experiments and contexts they are looking at). You may even think this is the case in your own example - in which, you should make that argument! “One potential limitation is that I only looked at Brazil. But that isn’t as concerning as we might think!”\nGo one step further and consider the specific problems that might arise (and why!) based on the theory (or theories) you have been working with and how they could be addressed in future work. Doing so shows a higher level of analytical thinking whereas simply plunking down a “eh, it’s a single case” type comment just reads as if you are ticking off a box and will almost certainly receive a “the conclusion discusses limitations but should go further” comment from me on the evaluation box.\nTo return to the example about party cue taking that I have used throughout this book, let’s say that this study used very old data for some reason. I might then write:\n\nA recurring debate in the literature on party cues is why people follow the party line. I argued that they do because of X. I then used the best available evidence to test this argument, where I found Y. In the remainder of this conclusion, I’ll talk about some potential limitations in these analyses and questions this all raises. [New Paragraph:] One potential limitation is the time frame of my study, which focuses on party cue taking in the year 1981. That is potentially problematic because political parties in the US have polarized since then and partisans have better sorted themselves into party camps. This may matter if this means that partisans instead have stronger group motivations to follow the party, which would mean that my results understate the influence of cue taking. Future studies could address this by doing X. … [New Paragraph] This study has important implications for party competition. To the extent that cue taking is reduced when Z occurs, then this means that elite partisans have little ability to influence the public given that Z is quite prevalent. As a result, worries over elite manipulation should be reduced. This is good for democracy (we think).\n\nOne reason why students sometimes do not elaborate here is because they are machine-gunning out potential issues (“and this could be an issue and this and this and this”). Two well fleshed ideas are much better than seven superficial ideas. Focus and elaborate.\nConclusions can be weirdly hard to write in no small part because you have to take a step back, critically reflect on what you have done, but then project forward to other time periods, or contexts, or experimental instruments, or whatever. So, do not rush this section! Give it the time it deserves.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "writing_06_conclusion.html#dont-only-be-negative",
    "href": "writing_06_conclusion.html#dont-only-be-negative",
    "title": "6  Conclusion",
    "section": "6.3 Don’t Only Be Negative",
    "text": "6.3 Don’t Only Be Negative\nYou should highlight the potential limitations of your study. However, remember to also remind us why this paper is interesting in the first place! What’s good or interesting about this project? Why should we care? This is something you can communicate in the first paragraph of the conclusion per the outline above (‘this study addresses an important question not fully examined by others’, etc.).",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "writing_07_general.html",
    "href": "writing_07_general.html",
    "title": "7  General Writing Advice",
    "section": "",
    "text": "7.1 Outline first\nIn general, it is a good idea to begin with an outline of what you want to write. This will let you start to feel out your argument, and flexibly revise it, without the pressure of having to compose perfect sentences.\nA potential outline might look like this (although, the contents will obviously vary based on what you are writing):\nIn filling in the outline for each paragraph, be sure to keep in mind that each paragraph should be organized around a clear, and singular, point or idea. A goal of the thesis, and indeed all writing, is to clearly communicate ideas. One writing tic that may undermine this goal is over-stuffing paragraphs with too many ideas or points. Instead, focus each paragraph on a main idea with successive paragraphs building on each other to form an overarching argument or point. Steven V. Miller’s tip’s for students page has an example of what I am talking about here that is quite useful.\nThe following is an example of what I mean. It stems from my efforts to puzzle through an idea I had that I was going to write up as a paper for a research workshop. As you can see, such an outline is a start to writing - there are places where I have placeholders (“Final Sentence”), for instance - and its contents are rather barebones. But, it provided me with a start!",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>General Writing Advice</span>"
    ]
  },
  {
    "objectID": "writing_07_general.html#outline-first",
    "href": "writing_07_general.html#outline-first",
    "title": "7  General Writing Advice",
    "section": "",
    "text": "Introduction\n\nFirst Paragraph: Main point\nSecond Paragraph: Main point\nThird Paragraph…\n\nLiterature Review & Theory Section(s)\n\nPotential Sub-section 1\n\nParagraph 1…\n\nPotential Sub-section 2\n\nMethods\n\nData Source: Where does the data come from and why this data source?\nDependent Variable: How do you measure your dependent variable?\nIndependent Variable(s): How do you measure your dependent variable?\nModel: Any relevant details on how you will analyze the data.\n\nResults\nConclusion\n\nParagraph 1…\n\n\n\n\n\nIntroduction\n\nParagraph 1: Promises aren’t believed\n\nPoliticians make promises on the campaign trail.\nThey seem to mostly keep them\nHowever, the mass public doesn’t believe this\nThe public largely distrusts politicians and believe them to be liars and not promise keepers\nCampaign statements are often non-credible\nThis poses some challenges to broader accounts of democratic representation and operation that place promissory representation at their core - how can the public hold elites accountable if they don’t believe them in the first place?\n\nParagraph 2: Moral language might increase credibility\n\nWhat would make a politician’s statements more credible?\nOne possibility highlighted in recent work is the use of moral language.\nWhat is moral language\nPoliticians and parties do sometimes use this language\nSimas and Clifford show that general moral language (of this sort) does lead to more perceived sincerity\n\nParagraph 3/4: This paper’s contribution(s)\n\nWe contribute to this work in two ways.\nFirst, we consider targeting. (maybe switch to language of matched morals rather than targeted? easier to talk about mis-matched?).\nImportant for this reason.\nSecond, we focus on competence. speaker credibility is based on two components: beliefs about the intentions/motives of the speaker (sincerity, trustworthiness) and their competence or ability to perform the action in question.\nCompetence is important for this reason.\n\nParagraph 4 or 5: This paper’s argument\n\nWe argue…\nTargeted &gt; non-targeted when it comes to sincerity & matched; targeted = more perceived similarity, so there is a halo effect (?); moral character stuff\nTargeted &lt; non-targeted when it comes to sincerity & mismatched; backlash effect (but is that what the feinberg stuff shows?)\nWhat about competence? More of an open question.\nOn the one hand, if more committed then more likely to work on it.\nOn the other hand, if more commiteed then less likely to compromise and perhaps less likely to have influence!\n\nParagraph 5 or 6: roadmap?\n\nWhat comes next.\n\n\nReview/Theory\n\nParagraph 1: Position Taking is central …\n\nCommon models of democratic politics grounds electoral competition in the programmatic offerings of parties and candidates\nFor instance, promissory theories of representation are based on the idea that parties and candidates make policy promises during election season; voters choose based on those promises; and then parties/candidates are held accountable at the next election based on whether they upheld those promises.\nThis process is iterative however with partisan elites potentially changing their positions over time in response to prior electoral performance and in an effort to anticipate the behavior of future voters (Mansbridge, Erikson, Adams etc.)..\nRegardless of whether this is a backwards or forwards process, policy is central to elections and representation.\n\nParagraph 2: But contested?\n\nParties and candidates spend a good deal of effort cultivating policy positions and publishing party manifestos documenting how they will act in office.\nAnd, yet, the actual role of policy positions on voter choices is contested.\nStudies of proximity voting suggest a role for voter positions, but one that is perhaps limited relative to more long standing concerns such as partisan predispositions or group loyalties. (Warshaw, etc.; maybe footnote Lenz – Of course, this assumes that voters have exogenous preferences to begin with, which is a quite problematic assumption - Kuklinski, Druckman, Lenz, Disch)\nChanges in party positions, on the other hand, do seem to matter but perhaps mostly over time and only to a small extent.\nPolicy is central to how we talk about elections and democratic politics, but it is not always clear whether it should be or not. (too strong?)\n\nParagraph 3: What explains this disconnect?\n\nWhat might explain the lack of more robust evidence about party positioning and voter policy positions?\nThere are a variety of potential answers to this including the (contested) possibility that only some types of issues matter (e.g., strong attitudes, issue importance, moralization; something on moralization) or that party strategies to remain ambiguous may muddy the waters (Somer-Topcu; etc.).\nAnother reason might be that the policy statements of parties and candidates may lack credibility in the eyes of audience members.\nParty position taken occurs in a context wherein many people distrust parties and politicians, believing them to only care about electoral success and willing to pander.\nSuspicion about the motives of partisan elites, perhaps stoked by other actors, may thus undermine the influence of party position taking (Adams, pandering minds, my papers).\nOn the other hand, party positions may be more influential in contexts wherein audience members believe the position to be more credible as when parties take unpopular positions as this is read as going against the party’s interests and hence serves as a costly signal about the party’s true intentions (Vazquez; see also Berinsky; Lupia?).\nFinal sentence\n\n…",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>General Writing Advice</span>"
    ]
  },
  {
    "objectID": "writing_07_general.html#write-and-edit-edit-edit",
    "href": "writing_07_general.html#write-and-edit-edit-edit",
    "title": "7  General Writing Advice",
    "section": "7.2 Write and Edit, Edit, Edit…",
    "text": "7.2 Write and Edit, Edit, Edit…\nHaving an outline in hand first helps clarify what you actually need to write…which is a handy thing to know before you start writing! However, it is, of course, just a first step. You then need to start writing and, important, editing and rewriting as well. Your first draft is almost certainly your worst draft in terms of clarity for the reader. Revisit your writing as much as possible. If possible, have someone read a draft of your thesis and ask them for feedback regarding anything that is unclear or doesn’t make sense.\nHere is some advice for the editing process. Musgrave and Davis provide additional points of value in the following blog posts: Simple Tricks for Better Prose; Show and Tell\n\nProofreading for Clarity\n\nRead the entire paper, checking for any awkward or unclear passages. For any such passage that you find, ask yourself what point you want it to convey. Write the point as if you were explaining it to a friend. Then revise to make it formal.\nBe wary of starting sentences with demonstratives that are not connected to a noun (e.g. “This results in”, “These can affect,” etc.). Sentences such as these invite the question: what is “this” again?\nAvoid using the passive voice if you can help it.\nAvoid beginning paragraphs with “However” or similar clauses. “However” implies a contrast (This thing, however this other thing)…but that would suggest combining the two points into a single paragraph. Either do that or rewrite so that you quickly reintroduce the idea you’re setting yourself up against.\nFinally, make your sentences as short as possible. Doing so will force you to write exactly what you mean. It will also prevent complicated sentence structures from obscuring the meaning of your words. One way to accomplish this task: rewrite sentences with multiple clauses separated by commas and try to eliminate as many commas as possible by rearranging the clauses.\n\nProofreading for Content\n\nPut yourself in the shoes of an intelligent reader who is unfamiliar with the topic. Make sure your argument will be clear to such a person and that you have not left any parts of the argument or evidence unstated or assumed them to be common knowledge.\nOne way to make sure you have done this is to write or revise as if you have a curious (and unusually literate) five-year-old child peering over your shoulder. Your evidence should speak to the series of “but why?” questions that this child will inevitably ask you. The logic and facts you present, what you cite from the sources you use, are the best tools you have to convince that child of your argument.\nDo not assume that you reader(s) know everything about the topic you’re writing about.\n\nProofread for Grammar and Miscellaneous Errors\n\nPolish your writing. Make sure there are no grammatical mistakes, no typos (remember to look for missing words, too), no repetitive words/wordings, and no missing or incorrect citations.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>General Writing Advice</span>"
    ]
  },
  {
    "objectID": "writing_07_general.html#some-other-questions-and-topics",
    "href": "writing_07_general.html#some-other-questions-and-topics",
    "title": "7  General Writing Advice",
    "section": "7.3 Some Other Questions and Topics",
    "text": "7.3 Some Other Questions and Topics\ncan you use first person! Yes! “This paper argues” is nonsense. You argue! Own the argument.\navoid tables that overlap pages",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>General Writing Advice</span>"
    ]
  },
  {
    "objectID": "writing_07_general.html#use-a-reference-manager-to-ease-in-text-citing-and-reference-list-building",
    "href": "writing_07_general.html#use-a-reference-manager-to-ease-in-text-citing-and-reference-list-building",
    "title": "7  General Writing Advice",
    "section": "7.4 Use a reference manager to ease in-text citing and reference list building",
    "text": "7.4 Use a reference manager to ease in-text citing and reference list building\nI recommend using a citation manager to help simplify the task of referencing as much as possible. I personally use Zotero but have experienced with Mendeley which also works fine. Either tool can help you insert citations in-text and automatically populate bibliographies for your essays.",
    "crumbs": [
      "Writing the Thesis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>General Writing Advice</span>"
    ]
  },
  {
    "objectID": "part_research.html#footnotes",
    "href": "part_research.html#footnotes",
    "title": "Doing Research",
    "section": "",
    "text": "Well, four is probably more accurate since you also have to write and edit the thing. See the first section of this collection: “Writing the Thesis”.↩︎",
    "crumbs": [
      "Doing Research"
    ]
  },
  {
    "objectID": "doing_01_research.html",
    "href": "doing_01_research.html",
    "title": "8  Finding Literature",
    "section": "",
    "text": "8.1 Strategies for Finding New Sources\nIf the substantive seminar is a tree trunk of knowledge on which you need to build, then that just raises the question of how you should go about doing that. Here, I’ll discuss three strategies for gathering new sources of research for potential inclusion in your thesis. These strategies should not be thought of as exclusive to one another. Rather, each might be useful at different points in the research process and can be used profitably together.",
    "crumbs": [
      "Doing Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Finding Literature</span>"
    ]
  },
  {
    "objectID": "doing_01_research.html#strategies-for-finding-new-sources",
    "href": "doing_01_research.html#strategies-for-finding-new-sources",
    "title": "8  Finding Literature",
    "section": "",
    "text": "8.1.1 Open Search\nFirst, one could begin with a somewhat open-ended search. This is what Miller talks about as “to [just] ‘Google it’”. (I discuss Google Scholar as a research tool below.)\nSuppose that you are interested in understanding public opinion about reactions to terrorist attacks, perhaps guided by a nascent question such as “do terrorist attacks lead people to support their national government more”. One could go to the university library’s website (or, per below, Google Scholar) and enter in a keyword related to that topic (“terrorism”, “terrorist attacks”, etc.) and see what comes up.\nThis can be especially useful at the very beginning of your research endeavors when you’re just trying to get a sense of what might be out there. However, it may pose a problem of surplus - that is, you are very likely to find LOTS AND LOTS of things come up that may not be especially well tailored to your particular interests. That surplus can, paradoxically, be a negative for you at the beginning of the search process since you may feel like you’ve been thrown into the middle of an ocean without knowing which way to swim.\nFor instance, here are the first several entries I found on Google Scholar when using the phrase “terrorist attacks” as my search term:\n\nThe second article by Leonie Huddy seems a relevant, albeit now somewhat old, resource. Huddy is a prominent public opinion researcher and the article clearly concerns public opinion on the subject.1 However, the other articles (save maybe the first one) look less relevant to my original question.\n1 This article is actually a specific type of research article that is published by the journal Public Opinion Quarterly, in which the authors canvass existing survey data to give a descriptive view of public polling concerning a topic.One thing I could do to better my odds of finding relevant work is to refine my search in various ways (choosing a different term, adding more keywords, asking for results from particular journals, etc.). Indeed, I would recommend doing just that if and when you engage in this type of research strategy: start with a keyword but limit your search to selected journals relevant to our class (see Section 8.3 below). For instance, here I add “public opinion” to my query to try and weed out less relevant work and restrict my attention to the American Political Science Review:\n\nThese results are much more helpful. I get a couple of articles not on the subject of terrorism (Hopkins, Chong and Druckman), but which may still be relevant for thinking about its effects in some way as they focus on how messages in the media influence public opinion. The other articles in this snapshot are much more directly focused on research relevant to my starting question.\nAn open search of this type can turn up relevant work and, indeed, quite a lot of it if you’re focused on a ‘hot’ topic. Of course, this strategy has its limitations. An open-ended search is a quite iterative process that may take some work to get you where you want to go (e.g., finding the right keywords, or searching through multiple relevant journals). It may thus be better at the very beginning (to get started) and towards the end (to see if you’re missing anything) strategy.\n\n\n8.1.2 Working Backwards, Working Forwards\nThere are two other, more directed, strategies that one might choose here.2 In both cases one would start with an existing piece of research (a journal article, a book, a book chapter) that one finds interesting and on whose topic one might wish to work for the thesis. From this article one could work backwards: start with the reading, look at what it cites (i.e., its references), and then start reading what looks interesting/relevant. An alternative strategy is to work forwards: start with the reading, look at what cites it, and then start reading what looks interesting/relevant.\n2 Or in combination with an open search strategy. Perhaps you begin with an open search, find an article or two that seem interesting/relevant and then branch out from there.Both strategies speak to the potential problem of relevance noted above by using the judgment of established researchers (those that published the original article in the first case, those publishing subsequent research in the latter) to narrow down the teeming mass of published research into a list of studies that are likely to be relevant to the topic you are interested in. As we’ll discuss in the substantive seminar, relying on the advice or actions of those we trust can sometimes be a sound route to effective decision making.\nBoth of these strategies are sound methods for getting into the research literature on a particular topic. However, there are some issues to think about.\nFirst, the backwards strategy could bias your literature review toward old (and potentially less relevant) research if your starting point is itself somewhat older. For instance, if you pick an article on reactions to terrorist events that was published in, say, 1985 then the research it is citing will likely involve cases that may be less relevant for understanding public reactions to more recent events. And, of course, it may lead you to ignore more recent theoretical and empirical advances on the topic. This need always be the case, but it is a potential risk; see Miller’s’s discussion on focusing research on relatively newer research.\nSecond, it says nothing about either the mechanics of actually going out and finding this literature or about how to sift through the likely large array of resources either strategy will produce. The next two sections consider these topics.",
    "crumbs": [
      "Doing Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Finding Literature</span>"
    ]
  },
  {
    "objectID": "doing_01_research.html#two-useful-tools",
    "href": "doing_01_research.html#two-useful-tools",
    "title": "8  Finding Literature",
    "section": "8.2 Two Useful Tools",
    "text": "8.2 Two Useful Tools\n\n8.2.1 Google Scholar\nGoogle Scholar is a search engine for academic research (you may already have some familiarity with it from your other classes). You can search for a keyword (“immigration”, “populism”, etc.) as well as for particular articles and books. It is thus a powerful tool for all three strategies above. You can then access these materials either directly (in some cases researchers make a copy available) or via the Leiden University library. See below for information on how to set up Google Scholar with access to the library.\nWhen you look up an article you will see something like the image below (staying with the terrorism example, but dropping the specific focus on the American Political Science Review):\n\nThe first entry that comes up is an article by Davis and Silver that focuses on public opinion concerning the trade off between civil liberties and security introduced by new public policy passed into law after the September 11 terrorist attacks in the United States. There are two particularly relevant links here that I want to draw your attention to: “Related Articles” and “Cited by”.\nRelated articles is a list of articles Google has decided are related to this one. This may be a first way to delve into a topic in greater detail. In practice, you are relying on Google’s algorithm to sort articles into more/less relevant…which may work fine in most cases. Indeed, in this case, Google lists a variety of other articles on this particular topic that all strike me as highly relevant for someone doing research on public opinion concerning terrorist attacks (albeit work with a decidedly US skew), as this snippet shows:\n\nAlternatively, one can click on “Cited by”, which will naturally take you to a list of books and articles that cite the original research. Here, is a snippet of that screen for this article:\n\n“Cited by” is a powerful method for researching “forward”. However, it may also pose some issues when starting with highly cited articles such as this one: lots of people cite it even when not working on terrorism! One can, of course, simply start skimming titles to find the relevant stuff, but that can be a bit time consuming if the originating article in question has a lot of citations.\nGoogle Scholar has an additional trick up its sleeve that may help here owing to the fact that this is Google we’re talking about: search. As the left hand column in the image shows, one can restrict this list of citations to a particular date range to help manage a potential excess of resources. In addition, if you click on the “Search within citing articles” box, then you can perform a second search on this list of articles using keywords that may be more relevant for your interests, e.g., by searching for particular countries, topics, etc. Perhaps I am particularly interested in the case of Germany; I might then click that box, add Germany to the search bar, and see what comes up:3\n3 I also added “public opinion” to further refine the search. Of course, I could have added something even more specific here depending on my interests, e.g., “tolerance”, or “prejudice”, or “voter turnout” or whatever.\nNow I have a little bit more to work with. I have the original article and some leads on research that may be more relevant for my own research (assuming, here, that I may want to understand something about German public opinion in particular).\n\n8.2.1.1 Connecting Google Schlar to Leiden University’s Library Resources\nOne can use Google Scholar to not just find research materials but also to access them. There are two ways to do this. First, journals and researchers may provide links to the materials themselves. For instance, in the most recent image, the link “[PDF] sagepub.com” would bring me to the pdf of the article provided by the journal that published the research in question. Researchers themselves may also provide links to articles hosted on their own websites, although note that sometimes these are ‘pre-print’ editions (i.e., versions that were submitted for publication and which might not exactly match the final published version). It should go without saying, though, that you should be careful when clicking on random-looking links on the web!\nThe second route is via Leiden University itself. You may have noticed in the images above the “GetIt@Leiden” option. Clicking on that link will take you to Leiden University’s library page for the article/resource in question and, from there you can access the material.\nTo take this second route you must first add the university as a resource in the settings in Google Scholar. First, access the website’s settings page via three horizontal lines in the upper top-left corner of the website. Then go to “Library Links”, search for “Leiden” in the search box, click on Leiden and select the box for “Universiteit Leiden”. Hit save and then everything should be good to go.\n\n\n\n\n8.2.2 Connected Papers\n[Note: I originally prepared this information prior to the Fall 2021 version of the BAP. Since then Connected Papers has added an account system. One can create two of the graphs discussed below without an account per month and five per month with a free account. It appears that one can largely bypass these limitations via the use of alternative web browsers and/or private/incognito modes, although I’m not sure if all functionalities will be offered without an account. This may limit the tool’s usefulness for you somewhat, but I still think it’s a cool resource that could be used sparingly to flesh out your research efforts.]\nGoogle Scholar, much like Google itself, is an ocean of information. It may be somewhat difficult to find what you want as a result. One new resource that may help with this is Connected Papers.\nMuch like Google Scholar, you can search for a given article (using its title or its digital object identifier (DOI)). What Connected Papers then does is search through articles to try and identify those that are on the same topic based on similarity in each article’s references. The logic is that two articles that cite the same things will likely be on the same topic.\nLet’s check out the Davis and Silver paper that we turned up in the section above. This is as easy as typing in the name and then double clicking on the article.\n\n\n\n\n\nHere is what the output page looks like. You can double click on this image to zoom in on it. The graph can be directly accessed via this link.\n\nThe middle part provides a graphical display of research in the form of a network. Here is how to read the graph:\n\nEach circle represents a different piece of research with our starting article seen in the middle or so of the graph. The size of the circle is related to how many times the piece of research has been cited: larger circle = more highly cited.\nThe shading of the graphs indicates when the research was published - darker shaded articles were published more recently.\nThe circles are connected by lines. The proximity of the circles to one another indicates the similarity in the paper’s citations and, presumably, topical focus.\n\nThere look to be three or so clusters of studies centered around Davis and Silver 2004 (our starting paper). The papers to the north of it focus on attitudes regarding political tolerance and civil liberties. The papers very very close to it (Sullivan 2009, Hutchison 2007) are focused specifically on the relationship between civil conflict/terrorism and tolerance, while those further north are about tolerance more generally. The other two clusters focus on the concepts of authoritarianism (the cluster to the right with Lubbers 2008 in it) while the cluster to the bottom focuses on terrorism and threat; these are both concepts that come up in Davis and Silver. Not only has Connected Papers given me some potential sources to look into on the very specific topic I began with (terrorism and civil liberties) but it has also given me a path toward exploring related concepts and ideas that I might need to make reference to in building up my ideas about this topic. It is also pretty easy to investigate those papers; as the right-hand part of the figure shows, for instance, I can obtain the abstract for the paper, and Google Scholar links, by clicking on the circle for the paper in question.\nConnected Papers also provides two additional areas: “Prior Works” and “Derivative Works”. The former tab provides an overview of the works that are highly cited by works displayed in the graph and which might thus be counted as “seminal works” within this field.\n\nWe also have Derivative Works: papers not in the graph but which cite many of those that are.\n\nThe nice thing about Connected Papers, in my opinion, is that it can reveal to you in one go a big (but manageable) chunk of a broader research literature. If I were writing a paper on terrorism and civil liberties (or, perhaps, one of those component concepts), for instance, most of the articles represented in the image with the graph above would indeed be the core of my literature review as they represent some of the most important work on this topic over the past two decades. I would not want to stop with them, but they would nevertheless provide me with a solid core with which to work. That might not be the case in other examples, of course. In this example I chose a highly influential article, for instance. Nevertheless, Connected Papers can help one get into a research literature on a topic quite quickly.",
    "crumbs": [
      "Doing Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Finding Literature</span>"
    ]
  },
  {
    "objectID": "doing_01_research.html#sec-research-focus",
    "href": "doing_01_research.html#sec-research-focus",
    "title": "8  Finding Literature",
    "section": "8.3 Okay…but what should I focus on?",
    "text": "8.3 Okay…but what should I focus on?\nOkay, so you’ve begun researching via one of the methods above. In doing so, you have likely turned up a large or large-ish number of things to read. How do you pick what to read and focus on? There is likely no “right” answer here, but let’s talk about some strategies.\nFirst, one could try and read everything you have found. This would certainly give you a deep knowledge of the literature. However, nobody can do that and certainly not someone working on your timeline. Thus, out of necessity, you will have to use some heuristics (a concept we’ll cover in the course) to try and cut through the noise, albeit with the risk of creating bias in your search.\nOne natural shortcut is algorithmic and has been the topic of the sections above: use Google or Connected Paper’s algorithm to identify a smaller subset of articles and then read through them. This isn’t a bad idea. However, we don’t necessarily know what goes into Google’s decision making in deciding what is relevant. In addition, one might still need to use judgement to parse out relevant and irrelevant resources as I did in the example above regarding Connected Papers.\nA second cue you can use is prestige. This is a natural cue as humans use prestige all the time to decide whom to pay attention to and learn from. There are two relevant signals of prestige that one can look to here. First, one can look at citation counts. The Davis and Silver article from above, for instance, has been cited nearly 900 times, which is quite a bit for a single paper. That signals that many other researchers think this is an important article. This may create some potential biases though. The natural bias is temporal; recent articles will be cited less simply because they are more recent, so a citation based judgement rule may lead you away from important recent work. That bias, at least, can be rectified by going back to Google Scholar and looking for more recent work on the topic.\nA second signal of prestige is the journal in which the article is published. The blog post by Miller linked to at the beginning of this document discusses this strategy in some more detail. Here are the journals I’d think to be quite relevant for our class (with acronyms in parentheses):\n\nGeneral Interest Journals\n\nGeneral interest journals are those that publish research from across the various sub-fields of political science. These are typically the most prestigious journals in the field.\nThe Big Three: American Political Science Review (APSR), American Journal of Political Science (AJPS), and the Journal of Politics (JOP)\nOthers of particular relevance: British Journal of Political Science (BJPS), Political Research Quarterly (PRQ), Perspectives on Politics\n\nJournals that focus more explicitly on public opinion, communication, and behavior\n\nPublic Opinion Quarterly (POQ), Political Behavior, Political Psychology, Political Communication, Comparative Political Studies, Electoral Studies, Journal of Experimental Political Science, International Journal of Public Opinion Research\n\nNon-political science journals that may be relevant\n\nIn Psychology: Journal of Personality and Social Psychology, Psychological Science\nIn Communication Studies: Journal of Communication, Communication Studies\nIn Sociology: American Journal of Sociology, American Sociological Review, Social Forces\nIn Economics: American Economic Review\n\n“Super” general interest journals\n\nThere has been some very interesting and important work published by political scientists in journals that cover “science” more generally (i.e., journals that also publish research in biology, physics, etc.). Hence, my calling them “super” general interest journals.\nExamples: Science, Proceedings of the National Academy of Science of the United States (PNAS), Nature: Human Behavior\n\n\nThe above should not be taken as meaning that journals not on this list are irrelevant or bad. Indeed, there are likely some not listed above that may be especially relevant for your research question. Nor should it be taken to mean that the research featured in the above journals is necessarily better or even good; that is a judgment that you have to make. Nevertheless, it isn’t a bad idea to focus your attention on work published in these journals first.",
    "crumbs": [
      "Doing Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Finding Literature</span>"
    ]
  },
  {
    "objectID": "doing_02_data.html",
    "href": "doing_02_data.html",
    "title": "9  Finding and Evaluating Data",
    "section": "",
    "text": "9.1 Study Design and Interpretation\nThe books, book chapters, and articles that you read during the seminar and in preparation of your thesis project will often feature both descriptive claims (e.g., claims about the extent of political knowledge in a particular country) and causal or explanatory claims as well (e.g. that political knowledge causes political tolerance). One important task is to consider the validity of these claims; you should not presume that a reading on a course syllabus, or one published by an academic journal or publishing house, is the final word on the subject as science is cumulative, research is fallible, and new conditions or contexts may require revision of what existing research leads us to expect.\nIn thinking about the claims made by a piece of empirical research you should think about the following questions:\nHere are some resources to aid you in thinking about these questions. Of course, you can also ask me for my thoughts as well!\nThe foregoing focuses on the logic of research design - on why researchers might collect data in the way they do and the potential consequences of research design on the findings that result. Understanding a study’s findings also depends on reading the regression tables and figures present in the article as well, something which may feel overwhelming at times. Here are some useful resources for helping you interpret research:",
    "crumbs": [
      "Doing Research",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Finding and Evaluating Data</span>"
    ]
  },
  {
    "objectID": "doing_02_data.html#study-design-and-interpretation",
    "href": "doing_02_data.html#study-design-and-interpretation",
    "title": "9  Finding and Evaluating Data",
    "section": "",
    "text": "Is the underlying theory logically coherent?\nAre the specific empirical claims made by the resource plausible given this theory and other things we now about the world?\nHow was the data generated? (who was surveyed? what experimental conditions are involved? how are key constructs measured?)\nWhy did the researchers opt for these methods (and, crucially, what choices did they make that could have been made otherwise?)\nHow confident should we be about an author’s findings given these design choices?\n\n\n\nEvidence in Governance and Politics (EGAP)\n\nThis website features several short and accessible guides for thinking about research design and causal inference. Their particular focus is on experimental work, but much of what they discuss is relevant in non-experimental settings. See their Methods Guides section.\nSome pages of particular relevance:\n\n10 Things to Know About Causal Inference\n10 Strategies for Figuring Out if X Caused Y\n10 Things to Know About External Validity\n\n\nResearch Design in Political Science by Dimiter Toshkov\n\nThis is a very nice book about the nitty gritty of doing research - on how to think about concepts, the nature of causal inference, and how to do empirical research (and what problems might arise when doing so). It thus goes a bit deeper than the shorter EGAP guides and covers all steps in the research process as well.\nThe following chapters are particularly relevant: “Theory in the Research Process”, “Concepts and Operationalizations”, “Measurement and Description”, “Experimental Designs”, and “Large-N Designs”\n\nSurvey Research\n\nMuch of what we know about how ordinary people think about politics comes from survey evidence (sometimes paired with an experiment, oftentimes not).\nThe political scientist Adam Berinsky provides a nice discussion of some of the important choices that survey researchers must make when designing and implementing surveys and how these choices may influence what can learn about politics in this article.\nPew Research is a major polling firm that performs surveys around the world (although more so in the United States). Not only might they be a valid source of data for your project, they also provide some resources for learning more about survey methods. Their Methodological Research talks about specific issues in survey research (e.g., debates about how best to measure gender or the difficulty of asking questions about religion in China) that may be interesting although not necessarily directly relevant for you. On the other hand, their YouTube page provides a nice series of short videos on survey methods that may be helpful in understanding survey research more generally.\n\nExperiments\n\nYou may be interested in experiments as a method for understanding the world and perhaps even as a method to be employed in your own project.1\nIf you want to learn more about experiments, then I heavily recommend reading through the Toshkov chapter on the subject and skimming through the book Experimental Thinking: A Primer on Social Science Experiments by James Druckman to better understand the possibilities on offer.\n\n\n1 Students in my BAP have, in the past, fielded survey experiments for their projects. I am certainly open to this as a method. If you are interested in this, then I’d ask you to keep in mind the issue of time and feasibility. Collecting your own data does add some steps to the thesis writing process - you will need to come up with the experimental protocols, create the survey (using, for instance, Qualtrics), field the survey, and then analyze it. This can be doable in the short time span available to you (especially if you are replicating/extending an existing experimental design), but does require you to be a bit more ‘on the ball’ in terms of time management. If you’re interested in doing this, then you should talk with me as soon as possible to discuss your ideas.\n\nReading a Regression Table: A Guide for Students by Steven V. Miller\n10 Things to Know about Reading a Regression Table by EGAP",
    "crumbs": [
      "Doing Research",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Finding and Evaluating Data</span>"
    ]
  },
  {
    "objectID": "doing_02_data.html#data-resources",
    "href": "doing_02_data.html#data-resources",
    "title": "9  Finding and Evaluating Data",
    "section": "9.2 Data Resources",
    "text": "9.2 Data Resources\nYou will need data for your thesis projects. The type of data required is determined by your research question and theory. However, I suspect that many of you will make use of existing data sources such as existing surveys and administrative data. How can you identify (and then access) useful data for your projects?\nThere are three resources you can use for figuring out what data to use for your thesis project.\nThe first are the various readings you have done in the course of your review of the literature related to your topic. They are a natural first place to begin as they are, of course, on the same topic as your own thesis. As you read and research, it may make sense to create a spreadsheet to keep track of the various data sources used in the myriad articles you read and what appears to be in them. This may have the benefit of also revealing limitations in existing work (or, at least, the work you have reviewed). For instance, if you notice in your spreadsheet that most of the data comes from European data sources, then this could help you motivate the use of non-European data as part of your answer to the ‘so what’ question for your thesis.\nA second resource are the various websites linked below in the “Data Repositories” sub-section. ICPSR and GESIS are both data repositories wherein researchers have deposited their data. Each enables you to perform a keyword search that may enable you to find datasets connected with your thesis topic. The “dataset with political datasets” webpage, meanwhile, is a very helpful one-stop shop for finding links to a wide array of relevant data sources in political science.\nFinally…you have me! If you’re having trouble finding something, ask me for advice. I may be able to guide you to something that is of relevance to your project, although no guarantees are provided here!\n\n9.2.1 Data Repositories\n\nA Dataset with Political Datasets: This webpage provides links to a huge array of datasets, including both mass level public opinion surveys as well as data on elites and institutions. You can even download the list as a spreadsheet that includes additional data about the data (meta!), such as what regions the resource covers, what time frame, and so on. This is a great place to begin. Scroll down to “Citizens” and take a look.\nGesis : A data repository run by the Leibniz Institute for the Social Sciences. Particularly relevant if you are using Eurobarometer data (see Section 9.2.3 section below) or European Election Studies data.\nICPSR : Another data repository, this one run by the University of Michigan\nPew Research: A leading public opinion firm with polling data available worldwide. See here on how to access their data.\n\n\n\n9.2.2 Some acronyms to know…\nSome surveys tend to come up quite a lot in our discussions. I may refer to them by their acronym due to their seeming ubiquity. Here are some examples:\n\nANES: American National Election Studies.\nWVS: World Values Survey\nEVS: European Values Survey\nESS: European Social Survey\nCSES: Comparative study of Electoral Systems\nEES: European Election Studies\n\n\n\n9.2.3 A Note on the Eurobarometer\nYou may be interested in public opinion within Europe. One useful resource might then be the Eurobarometer, which provides access to a series of surveys in EU member states going back quite a long time. For instance, you might be interested in media use and EU attitudes and head to their website to search for relevant surveys. Let’s say you turn up their Media & News Survey (2023) and think it would be a perfect fit for your project. Scrolling down that page, you might find this link:\n\nFantastic - you’ve found a survey and its data! Surely, all you have to do is follow that link and you can download the relevant data. … Well, not quite. Following that link will give you access to a variety of excel spreadsheets with data in them:\n\nHowever, those data files are not the ones that you will want. These data files are Excel spreadsheets that provide summary statistics for the survey, e.g., the % of people within a particular country and age range that give a particular answer:\n\nThis is almost certainly not what you want as it’s not the underlying raw individual-level data. Even if you are interested in explaining variation at the country-level (e.g., why might the percentage saying yes to a question vary between countries rather than between individuals), the files provided via the link above would be a pain in the ass for doing that because data is organized into separate files for each country and the files themselves are badly organized for use in R (or some other statistical program).\nIf you want to use Eurobarometer data, then you instead need to go to the GESIS data repository and search there for the specific survey that you’re interested in (or, perhaps, simply for “eurobarometer” - although from there you’ll need to find the one you want from the multitude of Eurobarometer surveys). Here is the page for the survey used in this example. It provides access to the raw data, questionnaires, and other documents (under Downloads). Note that you need to create a free account to access these files.\n\nSo, you Eurobarometer interested folk have been warned…\n\n\n9.2.4 R Packages for Accessing Data\nOne way to obtain data is to go to the website for a survey (or, perhaps, a repository that contains its data), download the relevant data file(s) to you computer, load them in to R, and begin working. However, some data sources can be directly accessed via R packages. Here are some relevant examples:\n\nvdemdata\n\nInstalling this package will download the most recent Varieties of Democracy (V-Dem) data files.\nYou can find a vignette on how to use this package here\n\nWDI\n\nYou might need/want to access data from the World Bank (e.g., data on a country’s unemployment rate). This package enables you to query the World Bank’s databases from R and then download the data directly without needed to visit their webpage. (In practice, I’d recommend using the World Bank’s website to help you search for the relevant indicator and then using the package to download it.)\nSteven V. Miller provides a helpful vignette on how to use this package.\n\nOur World in Data\n\nThis package enables access to the data provided by Our World in Data resource.\n\ngesisdata\n\nA resource for accessing data stored on the Gesis repository.\nFrederick Sold provides a vignette discussing how to set up and use the package.",
    "crumbs": [
      "Doing Research",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Finding and Evaluating Data</span>"
    ]
  },
  {
    "objectID": "part_analysis.html",
    "href": "part_analysis.html",
    "title": "Practical Issues in Data Analysis",
    "section": "",
    "text": "Other Resouces\nThe ensuing set of chapters is not a guide to statistics and does not cover all elements of data analysis (e.g., assumptions testing). If you need a refresher on the statistics element, then you should consult Andy Field’s “Discovering Statistics Using IBM SPSS Statistics” and/or OpenIntro Statistics as well as your class notes for Statistics I and II. SPSS users can find videos on how to use SPSS for data analysis at this Kaltura page (and additional written instruction via the .pdf file linked to above). R users can find the videos and overviews from Statistics I and II on this Brightspace page. The Statistics I R overviews have now been collected into a single book here. The Statistics I R overviews are also being collected into a single online resource and I will update this document with a link to it when it is ready. You can, of course, also consult me to supplement these materials.\nThe resources above do not exhaust the potential guides and pedagogical resources available for working with R/R-Studio. Here are some additional resources that you may find useful.",
    "crumbs": [
      "Practical Issues in Data Analysis"
    ]
  },
  {
    "objectID": "part_analysis.html#other-resouces",
    "href": "part_analysis.html#other-resouces",
    "title": "Practical Issues in Data Analysis",
    "section": "",
    "text": "Data Visualization\nYou were introduced to the ggplot package for data visualization in Statistics I/II. This was very much an introduction as the helper materials did not go into everything this series of co[^1]mmands can do. The R Graphics Cookbook provides a much more thorough walkthrough of how to create, and augment, nearly any other type of plot possible with ggplot . Cédric Scherer’s A ggplot2 Tutorial for Beautiful Plotting in R, meanwhile, provides an overview of many of the aesthetic options available in ggplot for creating eye-catching graphics. Finally, Professor Kieran Healy’s book Data Visualization is a great resource for learning about what makes a plot an effective tool for communication in the first place.1\n1 His Bluesky account is a fun read as well.\n\nR Programming\nYou were introduced to the tidyverse in Statistics I/II. One of the most influential resources for working with these tools, and with R in general, can be found in the book R for Data Science by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund. Wickham is one of the progenitors of this library of R packages and on the shortlist for most influential people in the broader R universe. This book is for those who want to push their knowledge of R further and perhaps particularly for those that might be thinking of going into data science or something similar in their post-graduate life.\n\n\nR Markdown\nYou may very well write your thesis using two programs: a word processor (e.g., Microsoft Word or [shudder] Google Docs) for the actual writing and R-Studio for the analyses. However, you could very well write the whole thing in R-Studio as an R Markdown document. The book R Markdown Cookbook by Yihui Zie, Christophe Dervieux, and Emily Riederer is resource is for those bold few who are thinking of doing that as it delves into the wide array of options available to you in doing so.\nOne note here: if you’re writing your thesis in R Markdown, then I recommend that you also install and use Zotero as a citation manager. I recommend using a citation manager in general, but Zotero can be used with R-Markdown documents, making the inclusion of in-text citations and the (automatic) generation of a bibliography section much easier. See this blog entry on how to add citations in R Markdown using Zotero.",
    "crumbs": [
      "Practical Issues in Data Analysis"
    ]
  },
  {
    "objectID": "analysis_01.html",
    "href": "analysis_01.html",
    "title": "10  Analysis Steps",
    "section": "",
    "text": "10.1 What Do You Want to Know?\nThe first step above is not “perform your analyses” but instead “What Do You Want to Know?”. This is because everything depends on this step - what data you should look for and try to use, what types of analyses you perform, and so on.\nWe can think of this step as involving finding an answer to these two questions:\nAnswering these questions is what will primarily occupy your attention in the first 8-11 weeks of the thesis project (i.e., the seminar, proposal writing stage, and post-proposal feedback stage).",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analysis Steps</span>"
    ]
  },
  {
    "objectID": "analysis_01.html#what-do-you-want-to-know",
    "href": "analysis_01.html#what-do-you-want-to-know",
    "title": "10  Analysis Steps",
    "section": "",
    "text": "What is the question?\n\nWhat is your research question? What is it that you want to know about the world?\nFor instance: do young people punish politicians for democratic violations as much as old people?; or: does partisanship promote voter turnout?; or: does social contact reduce prejudice?\nSee Musgrave and Davis on how to ask the “right questions”.\n\nWhat is the claim you want to test?\n\nYou will advance a claim about the world in your thesis that you then seek to test. This claim is your hypothesis (or hypotheses).\nFor instance: young people are less likely to punish non-democratic behavior by politicians; or: partisans are more likely to vote than non-partisans; or: social contact with out-group members reduces prejudice.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analysis Steps</span>"
    ]
  },
  {
    "objectID": "analysis_01.html#pre-analysis-planning",
    "href": "analysis_01.html#pre-analysis-planning",
    "title": "10  Analysis Steps",
    "section": "10.2 Pre-Analysis Planning",
    "text": "10.2 Pre-Analysis Planning\nThe second step is “Pre-Analysis Plan”. This step involves figuring out how to translate your answer to “What Do You Want to Know” into practice. We can also break this down a bit further:\n\nFind a Reasonable Data Source\n\nWhat data or type of data do you need to test your empirical claims and hence answer your question? Can you actually access this data? If you cannot obtain the ‘ideal’ data to test your hypothesis(es), then can you obtain a reasonable alternative?\nThis is commonly a social survey for students in my BAP, but could also be (or laso include) administrative data or an experiment.\n\nIdentity Relevant Variables and Decide How You’ll Treat Them\n\nYou will formulate some hypothesis or hypotheses that you will then test. This test will likely involve a multiple regression of some type in which you predict a dependent variable with an independent variable and some “control” variables. You both need to find a data source with reasonable measurements for the concepts relevant to this statistical model and also figure out how they should be handled in the analyses (e.g., do you need to convert anything into a factor variable, do you need to rescale a variable, etc.).\n\nIdentify Relevant Methods of Analysis\n\nWhat type of statistical model is most appropriate for analyzing your dependent variable? The answer here generally follows from the nature of the dependent variable (e.g., continuous \\(\\rightarrow\\) OLS model, binary \\(\\rightarrow\\) logistic model), but there may be complications along the way (e.g., your DV may be categorical, or you may have clustered/nested data, etc.).\n\nThink about limitations\n\nAll data sources are “limited” in some fashion, e.g., they only focus on some observations rather than all possible observations, the measurement of some key construct may not exactly match the underlying concept in some way, etc. You should give some thought as you go to the potential issues in your data and how they might specifically affect your results. Does the use of this particular data source (etc.) make it easier or harder to find evidence consistent with your expectations? If so, why and how could we avoid this issue in future work? If not, why not? This is perhaps most relevant for the writing stage of the thesis and particularly discussions in the conclusion.\n\n\nWork on this step should begin during the “What Do You Want to Know” portion but may extend further (i.e., you might not really start focusing on this until Weeks 6-7, say, and not have a full answer until a little bit after this). However, you should have a good idea of the data source that you’ll use by mid-November or so and a sense of what you’ll need to do to translate this raw data into your analyses shortly thereafter so that you can, well, begin the analysis portion. The first ‘feedback’ deadline in my BAP is timed for around this point and I recommend/expect a reasonably clear research design section that communicates these points (and especially the data source and nature of the main variables) by this time frame.\nI have an arrow pointing from this box back to “What Do you Want to Know”. You should not decide your research question or hypotheses based on the data available to you. At the same time, feasibility is an important element of a good thesis. A common saying in graduate school is “the only good dissertation is a finished dissertation”. You can identify an interesting and novel question, and advance a plausible argument/hypothesis regarding it, but if you cannot test it all then the thesis process will grind to a halt. It thus could be a case that you’ll need to revisit some of your answers in the “What Do You Want to Know” step (e.g., reframe the question, change the IV, etc.) as you begin hammering out the details here. Ideally this would be done in consultation with me so that I can provide you guidance in this process.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analysis Steps</span>"
    ]
  },
  {
    "objectID": "analysis_01.html#data-cleaning-and-creation",
    "href": "analysis_01.html#data-cleaning-and-creation",
    "title": "10  Analysis Steps",
    "section": "10.3 Data Cleaning and Creation",
    "text": "10.3 Data Cleaning and Creation\nAfter you have decided on what data you need and what variables from this dataset you’ll use (and how), then you can start cleaning the data in preparation for subsequent analyses. By “data cleaning”, I mean removing missing value codes (if necessary), recoding variables, creating new ones, etc. One thing to keep in mind: this will probably be the most time consuming element of the actual analyses. A regression model can be run in seconds on a modern computer…but cleaning the data could take much longer.\nIf you need to create your own dataset in some form (e.g., merging multiple databases together, collecting data for an experiment), then this would also happen at this point in time followed by data cleaning.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analysis Steps</span>"
    ]
  },
  {
    "objectID": "analysis_01.html#descriptive-analyses",
    "href": "analysis_01.html#descriptive-analyses",
    "title": "10  Analysis Steps",
    "section": "10.4 Descriptive Analyses",
    "text": "10.4 Descriptive Analyses\nYou might then be tempted to jump into a statistical model…stop! You should first get to know these variables and especially the ones core to your hypothesis(es).1 In other words, begin with some univariate analyses (mean + measure of dispersion [SD, confidence interval] for continuous variables, tabulations/frequency table for binary/category variables, etc.) to better understand your data. This might suggest to you the need to make some type of transformation or recoding of the variable. Beyond that, it may help you turn up errors in the data cleaning process before you progress to a more fully realized model and commit yourself to an interpretation of its output. And, finally, this step is important because you should provide this information somewhere in your thesis (see Chapter 4).\n1 Some of this may come about in the Data-Cleaning and Creation stage. For instance, you may need to factorize a categorical variable and, in so doing, decide which category will be the reference group for your analyses. If the variable is related to a clear hypothesis, then the reference category will likely depend on what comparison(s) makes most sense for testing the hypothesis. For instance, if you have a four-category region variable (North, West, East, South) and are arguing that your DV will be lower in the South than everywhere else, then the South would be the most relevant reference group. (In this situation it might even make sense to make this into a single variable [South vs. Non-South], but hopefully the point is made). Otherwise, you might want to avoid using a category with very few observation in it, but to do that you’d need to do a quick tabulation to check.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analysis Steps</span>"
    ]
  },
  {
    "objectID": "analysis_01.html#statistical-modelinginterpretation-writing",
    "href": "analysis_01.html#statistical-modelinginterpretation-writing",
    "title": "10  Analysis Steps",
    "section": "10.5 Statistical Modeling…Interpretation & Writing",
    "text": "10.5 Statistical Modeling…Interpretation & Writing\nAfter getting to know your data, proceed to bivariate and multi-variate analyses. If your main variables are continuous, perform a correlation and then your fuller statistical model (i.e., a linear regression) (etc.). After you perform your main statistical model, check its assumptions and, if necessary, make changes to the statistical model (e.g., use a different standard error, transform a variable, etc.). Finally, interpret your model and write up the results.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analysis Steps</span>"
    ]
  },
  {
    "objectID": "analysis_02.html",
    "href": "analysis_02.html",
    "title": "11  Some Practical Questions",
    "section": "",
    "text": "11.1 What should I control for?\nYour goal in the thesis is probably going to be to test some hypothesis. For instance, you might hypothesize that partisan animosity (“affective polarization”) will be greater among people with extreme ideologies than centrist ideologies. In other words, people with extreme ideologies might dislike people who identify with a different political party as themselves more than people with less extreme ideologies dislike people who identify with a different political party as themselves. You will naturally want to find data on your DV and IV and fit some type of regression model. A natural question then arises: what else should go into my model? Or: what should I “control” for?\nWe can answer this question by taking a step back and thinking about what we’re trying to do when we include a “control” variable in our model.1 Suppose we perform some bivariate test predicting partisan animosity and affective polarization and find the relationship we expect to find (i.e., more extremity = more animosity). The challenge before us is that ideological extremity is not randomly distributed in society. Extremists probably differ from non-extremists in all sorts of ways - they might have different education levels, social networks, personality characteristics, etc. This consideration raises the possibility that the relationship we observe (more dogmatism = more animosity) is better explained by one or more of these other characteristics that differentiates extremists and non-extremists (e.g., more dogmatism = more animosity & more extremity?). We include “control” variables in an attempt to statistically account for alternative explanations for the relationship we care about. We should thus “control” for any and all potential confounder variables that we can, i.e., variables that we have good reason to think cause both our main IV of interest and the DV.2\nFigure 11.1: A Relationship with Two Confounders\nPer above, you should control for things that you believe cause both your main IV of interest and your DV. You can justify these beliefs in relation to prior evidence and theory. Per Chapter 10, one important part of the thesis writing process (and of doing your analysis) is first doing the hard work of figuring out how you think the world works in relation to your phenomenon of interest as this will help you identify relevant control variables.\nOne thing you should avoid doing is simply dumping variables into your model. This is because you can get in trouble by leaving out relevant variables (as above) but also by including unnecessary variables in your model as well! In particular, you should try to avoid including what are alternatively known as “post-treatment” or “mediating” variables in your model.3 A post-treatment variable is a variable that is caused by your main IV and in turn causes your DV. For instance:\nFigure 11.2: A Relationship with a Confounder and a Mediator\nWhat is the problem here, exactly? Let’s say that our main IV is ideological extremity and our DV is partisan animosity. We might think that some of this relationship is explained by a common source of both variables (dogmatism or cognitive rigidity) and include that in our model. However, we think extremity should still predict animosity (the relationship is not only due to dogmatic thinking). In particular, we theorize that this relationship should emerge because holding an extreme ideology facilitates a variety of reasoning processes that will lead extremists to hold inaccurate beliefs about the other side’s political views.4 Extremists, for instance, might come to see the other side as holding more extreme beliefs than it actually does, feel threatened by this (somewhat imagined) out-group, and hence express more animosity toward the out-group.\nFigure 11.3: Probably Not a Good Idea to Control for Beliefs about Out-Group Beliefs!\nIf we include variables pertaining to a person’s extremity and their beliefs about the out-group in a model predicting their level of animosity toward the out-group, then we will be estimating the “effect” of extremity “holding constant” beliefs about the out-group, i.e., after adjusting for differences in beliefs about the out-group. However, we will be producing a biased estimate of the total relationship between extremity and animosity because we are essentially “controlling away” (part of) the effect we care about in the first place! Including the mediator in our model accounts for part of (perhaps even the entire) reason why extremity matters for understanding animosity thereby leading to a smaller effect estimate than actually exists - our model can only tell us whether there is a relationship between extremity and animosity after adjusting for differences in the mediator. Indeed, if the only reason why extremists differ from non-extremists is because of the effect of extremity on beliefs about the out-group, then our estimate for the effect of extremity in this model might even be 0 (statistically insignificant)!5 You can see this point in practice via Case 4 in this explainer. The upshot is that you should try and avoid including post-treatment variables in your model…if you can. The difficulty is that what counted as a confounder (causes X and Y) versus a post-treatment variable (caused by X, causes Y) may be ambiguous or contested. At a certain level, we may be stuck with assumptions about how the world works to justify our decisions on this front.\nThere can be contexts in which we would want to include the post-treatment variable in the model although they may not apply to your thesis. First, we might actually be interested in whether there is any left-over relationship between our main IV and the DV after accounting for potential mediators. For instance, we might be investigating wealth biases in college admissions.6 We might examine this question by comparing admission rates for students with rich parents vs. those with non-rich parents and find higher rates among the former group than the latter. A skeptic may say that this is not evidence that colleges are biased toward the rich - perhaps family wealth leads to better academic performance (higher grades, higher test scores) and that is what colleges are paying attention to. In this instance, it might make sense to also include measures of performance to see if there is still a relationship between family background and admissions after adjusting for differences in performance (what we might call a “direct” effect of family background since it is not being mediated by performance). Second, we might be interested in testing for mediation: how much of the relationship between our main IV and the DV is accounted for by a mediator? This is difficult to do in practice, and especially so with observational data, but typically would involve including the post-treatment/mediating variable in the model at some point. However, in the context of your study this type of variable is almost certainly a nuisance variable that you do not want to include in the model because doing so will essential ‘control away’ some of the influence of your main IV by controlling for one of its effects, which is probably not what you want to do!\nTwo final thoughts here. First, the foregoing applies when your data is non-experimental. However, if you are working with an experiment then you don’t generally need any control variables in your model. You can simply predict your DV with an indicator for which treatment group the observation has been randomly assigned to. This is the case because random assignment is handling the task of ruling out alternative explanations. Second, the foregoing focuses on situations where the two predictor variables are causing one another (in someway) and also causing Y. What about if the two IVs are unrelated to one another while both predicting the DV? Including both predictors in the model would be a good idea in this scenario but for other reasons than discussed above. Instead of reducing bias in our estimates, doing so would reduce variance in our estimaets (i.e., it would lead to more certainty); see Case 5 in this explainer. However, this is mainly of interest in experimental studies wherein we know for sure that our main IV (assignment to different experimental groups) is unrelated to other variables (due to random assignment and measuring the other variables prior to assignment).",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Some Practical Questions</span>"
    ]
  },
  {
    "objectID": "analysis_02.html#what-should-i-control-for",
    "href": "analysis_02.html#what-should-i-control-for",
    "title": "11  Some Practical Questions",
    "section": "",
    "text": "1 The focus here is on the inclusion of additional predictors in a context where we are trying to generate an unbiased estimate of the relationship between a particular IV and a DV. We could also be building a statistical model for the sake of prediction, e.g., to construct an accurate forecasting model. The considerations about what to include in such a model would be different as the focus would be on what best reduces predictions errors from the model as a whole rather than what reduces bias in one particular estimate.2 Of course, there may be (likely are) important confounders out there that we cannot measure or perhaps simply do not have a measure of to include in our model. We should thus be careful not to over-interpret statistical models of observational data as definitively telling us that our IV of interest causes the DV - it is better to talk about these relationships as associations between the variables.\n\n\n3 There is yet another potential source of bias from improper variable inclusion: collider bias. Let’s say we have this model: Y = X1 + X2. Collider bias emerges when one of the variables on the right hand side of the equation (X2, for instance) is caused by the other two variables (i.e., X2 is caused by Y and X1). Collider bias can lead to a spurious association much like omitted variable bias when a relevant confound is excluded.\n\n4 Inaccurate “meta-perceptions” (or, beliefs about others beliefs) are indeed an important predictor of group animosity (see, for instance, Moore-Berg et al.) and perhaps also support for political violence (see, for instance, Braley et al.)\nper Moore-Berg a\n\n5 See this article for additional discussion of this type of situation.6 This is a real-life example based on the following news article.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Some Practical Questions</span>"
    ]
  },
  {
    "objectID": "analysis_02.html#if-a-control-variable-is-not-statistically-significant-should-i-remove-it-from-my-model",
    "href": "analysis_02.html#if-a-control-variable-is-not-statistically-significant-should-i-remove-it-from-my-model",
    "title": "11  Some Practical Questions",
    "section": "11.2 If a control variable is not statistically significant, should I remove it from my model?",
    "text": "11.2 If a control variable is not statistically significant, should I remove it from my model?\nNo.\nYou should use theory and evidence to justify what variables go into the model. A “statistically insignificant” variable does not mean the variable does not ‘cause’ the DV or even that it doesn’t matter for you. Insignificant predictors could emerge, for instance, due to poor survey wording (measurement error) or low sample size even in a case where there is a ‘real’ relationship between the two variables. And, per above, the rationale for including the variable in your model is that it is a plausible confounder of a/the variable you actually care about. If those assumptions are correct then we may very well expect to see a null relationship between (some of) the controls and the DV since you are controlling for its potential mechanisms (i.e., your main IV)!",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Some Practical Questions</span>"
    ]
  },
  {
    "objectID": "analysis_02.html#how-many-variables-should-i-can-i-include-in-my-model",
    "href": "analysis_02.html#how-many-variables-should-i-can-i-include-in-my-model",
    "title": "11  Some Practical Questions",
    "section": "11.3 How many variables should I, can I, include in my model?",
    "text": "11.3 How many variables should I, can I, include in my model?\nThere is a basic mathematical ‘basement’ here for OLS models: n &gt; k. n is the number of observations you have while k is the number of coefficients. A bivariate model has two coefficients (intercept and slope) while multiple regression models would have as many added slope terms as there are additional predictors. The number of observations you have must be greater than the number of variables you’re including in the model.\nOkay, but probably you’ll have anywhere from 25 to 10,000 observations in your dataset. So, is it okay to include anywhere from 20 to 9500 variables? Not necessarily. Too many variables for too few observations can lead to an issue known as overfitting. There are some general rules of thumb here though that you may see suggesting that you should have approximately 10-15 observations per model term. A constant + 2 IVs? You’d want 30-45 observations at least. Of course, rules of thumb are not always iron clad.\nThe general aim should probably be for parsimony: “Everything should be as simple as it can be, but not simpler”.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Some Practical Questions</span>"
    ]
  },
  {
    "objectID": "analysis_02.html#what-should-i-do-with-ordinal-data",
    "href": "analysis_02.html#what-should-i-do-with-ordinal-data",
    "title": "11  Some Practical Questions",
    "section": "11.4 What should I do with ordinal data?",
    "text": "11.4 What should I do with ordinal data?\nStatistics II focuses on OLS regression and logistic regression. The former is meant for the analysis of continuous (interval/ratio) DVs, while the latter is meant for binary DVs. What about ordinal data though? These impose some complications.\nConsider age in years as a variable. The difference between 18 years old and 19 years old is the same as the difference between 80 years old and 81 years old: 1 year. Each unit increment in age in years cover the same amount of change. Now, consider a standard left-right ideology measure ranging from 0 (“left”) to 10 (“right”). A person who rates themselves a 1 on the scale is less left-leaning than someone who rates themselves a 0. A person who rates themselves a 10 on the scale is likewise less left-leaning (more right-leaning) than someone who gives themselves a 9. The difference in each case is again a single unit on our scale. But do these 1 unit differences really capture the same amount of ideological difference between the groups? This is not quite as clear cut as with the age variable given the abstractness of the scale.\nThe foregoing ambiguity can create a problem for ordinary least squares regression. OLS models assume that the relationship between the IV and the DV is linear: that moving from 18 years to 19 years will bring with it the same degree of change in Y as moving from 80 to 81 years. Or, that moving from 0 to 1 on the left-right measure brings with it the same degree of change in Y as moving from 9 to 10. However, ordinal variables can violate, or at least potentially violate, that assumption insofar as the difference between levels of the variable doesn’t capture the same degree of change across the range of the IV. So, what should you do?\nIf your DV is ordinal, then my recommendation is to simply run an OLS model. The alternative to an OLS model here is an ordered/ordinal logistic model. I’ll discuss fitting such a model in CHAPTER. However, while this is indeed the technically more appropriate model: you were not trained on how to run it, it is more difficult to interpret, there are debates about statisticians about whether it is really a better model, and it usually leads to the same basic conclusions. In running the OLS model, you will be implicitly assuming that the spacing between categories on the DV is indeed equivalent/identical. If you are concerned about that assumption, then I’d recommend that you present the results of both an OLS model and an ordered logistic model; discuss the results of the OLS model; and then note the similarity with (and/or differences from) the ordered logit model. This is a common tactic. I’ll discuss this more in CHAPTER.\nA perhaps trickier business is what to do with ordinal independent variables. One can do two things here. First, one could treat the variable as a categorical variable: create a dummy variable for each level of the variable and then include all but one in the resulting model. This is the safest tactic as it does not involve making any further assumptions about the data (i.e., that the change between categories is equivalent). However, it throws away information about the variable (the fact that it is ordered in nature) and only enables you to formally test the statistical significance of the difference between the included categories and the common baseline category. Second, one could simply assume that the categories are equivalently spaced and treat the variable as ‘continuous’ in nature. This perhaps enables easier interpretations, but does involve that additional assumption and you know what they say about assumptions.\nIn practice you will see both things done and sometimes even in the same model! Here, I would recommend paying some attention to how other researchers use the variable in question. The 7-pt party identity measure in the US is almost always treated as an interval/continuous scale, for instance.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Some Practical Questions</span>"
    ]
  },
  {
    "objectID": "analysis_02.html#what-should-i-do-if-my-dv-is-categorical",
    "href": "analysis_02.html#what-should-i-do-if-my-dv-is-categorical",
    "title": "11  Some Practical Questions",
    "section": "11.5 What should I do if my DV is categorical?",
    "text": "11.5 What should I do if my DV is categorical?\nIf my DV had multiple categories that do not admit of an obvious ordering (e.g.: do you support the Labour Party, Conservative Party, or the Liberal Democrat Party), then a multinomial logit model would be most appropriate. A multinomial logit model is basically just a logit model but one designed for categorical variables with more than two categories. I’ll give an example in Chapter 19 .\n\n\n\nFigure 11.1: A Relationship with Two Confounders\nFigure 11.2: A Relationship with a Confounder and a Mediator\nFigure 11.3: Probably Not a Good Idea to Control for Beliefs about Out-Group Beliefs!",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Some Practical Questions</span>"
    ]
  },
  {
    "objectID": "analysis_rscript.html",
    "href": "analysis_rscript.html",
    "title": "12  R Script Files",
    "section": "",
    "text": "12.1 Suggested Structure\nI would recommend structuring the R Script file with different sections that build on one another. The logic behind this ordering is based on the discussion in Chapter 10. For instance:",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>R Script Files</span>"
    ]
  },
  {
    "objectID": "analysis_rscript.html#suggested-structure",
    "href": "analysis_rscript.html#suggested-structure",
    "title": "12  R Script Files",
    "section": "",
    "text": "Header Information\n\nIt is nice to start with some header information indicating what the contents of the file are for. You are probably going to have a single script file for your project so this can be as simple as providing a short description such as “BAP - Data Analysis Script by NAME”. If you had multiple files (e.g., one for data cleaning, one for one type of analysis, one for another) then you could help keep things straight for yourself and others by having that information in the header (e.g., “Data Cleaning Script” or “Regression Model Analyses”).\nSee below for an example.\n\nPackages\n\nThe various packages you’ll need to complete your analysis.\nThe main thing to watch out for are package conflicts. The car package, for instance, has a conflict with the dplyr package within tidyverse - both car and dplyr have a command called recode(). If you load car before tidyverse, then R will use the recode command from the latter package, but if you load tidyverse before car then R will use the recode command from car instead.\n\nData\n\nLoad your data or datasets.\nIf you are loading multiple different datasets, then it might make sense to have a big section that loads the first dataset and cleans it as needed, then another one that loads the second one and cleans it, and then one for joining them, before progressing onto the analyses.\n\nData Cleaning\n\nThen a section where you clean your data as needed (e.g., recoding variables, creating factor variables, etc.).\nThis section might also include syntax for summary statistics and looking at the attributes of the variables as this may be relevant for deciding how to clean the data.\n\nData Analysis with subsections\n\nThen a section for the various analyses you’ll end up doing with sub-sections for different steps in that process (e.g., running the model, checking assumptions, obtaining/plotting predicted values, creating the regression table, etc.).\nThe sample script file that I provide has this set up slightly differently with one section for descriptive and/or bivariate analyses and then one for the regression model and subsequent steps. The specifics here are not super mission critical, you just want to set things up in a clear and logical manner so that you know what is going on and to help others decipher what you’re doing (e.g., if you ask me for help then I’ll want clarity!).",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>R Script Files</span>"
    ]
  },
  {
    "objectID": "analysis_rscript.html#separating-sections",
    "href": "analysis_rscript.html#separating-sections",
    "title": "12  R Script Files",
    "section": "12.2 Separating Sections",
    "text": "12.2 Separating Sections\nHow can you keep things neat and divided? The # symbol is your friend here. The # symbol defines a comment area which R will not try and evaluate as code. For instance:\n\n# Here is a comment - R will ignore the stuff in this greyed out line\n\n## Here is another comment\nsummary(mtcars$mpg)  #and another\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.40   15.43   19.20   20.09   22.80   33.90 \n\n\nHere is a snapshot of the beginning of the example script file:\n\nI have tried to delineate different sections by using the # commenting tool. The very first part, for instance, is the Header (in the terminology from above) - I would replace “Project Information” and “Author” with relevant information for my project/file. I then have a section for Packages and another for Data and so on. I have used multiple hashtags to try and make different sections stand out from one another a bit more.\nA handy tool here is the Outline section. You can see an Outline of the contexts of the script file on the right side of the image; this can be toggled by clicking on the button next to “Source”. The Outline is nice because clicking on an entry within it will automatically take you to that place in the script file thereby saving you from having to scroll around looking for things. You can add something to the Outline by including four dashes (----) at the end of the comment text. You can remove something from the Outline by including a single dash there instead; I do this for the long row of hashtags used in separating sections because otherwise they’d be added to the Outline but listed as “Untitled” thereby cluttering everything up. You can nest sections within one another by using one, two, or three hashtags (see here as well):\n\n# : Section\n##: Sub-Section\n### Sub-subsection",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>R Script Files</span>"
    ]
  },
  {
    "objectID": "analysis_discussion.html",
    "href": "analysis_discussion.html",
    "title": "13  Discussing Your Model Results",
    "section": "",
    "text": "13.1 Example Model and Some Initial Thoughts\nLet’s consider an initial model:\nShow the code\n# OLS Model \nm1 &lt;- lm(v2x_polyarchy ~ Women2003 + GDP2006 + regime_type_2006, \n         data = demdata)\n\n## Table (See Chapter 'Regression Table Formatting Suggestion' \n## for reasoning behing this syntax)\n## I do not specify a Title here because I am doing that behind \n## the scenes in creating this html document\n\ndem_models &lt;- modelsummary(m1, \n             estimate = \"{estimate}{stars}\\n({std.error})\", \n             statistic = NULL, \n             gof_map = c(\"nobs\", \"r.squared\", \"adj.r.squared\"), \n             coef_rename = c(\n               '(Intercept)' = 'Intercept', \n               'Women2003' = '% Women MPs (2003)', \n               'GDP2006' = 'GDP Per Capita (2006)', \n               'regime_type_2006Autocracy (2006)' = 'Autocracy in 2006? (1 = Yes, 0 = Democracy)'), \n             notes = list(\"Notes: OLS or Logit Coefficients with standard errors in parentheses\", \n                          \"* p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\"), \n1             fmt = fmt_term(GDP2006 = \"%.3e\"),\n             output = 'flextable' )\n\ndem_models &lt;- dem_models |&gt; \n  hline(i = nrow_part(dem_models) - 3) |&gt;  \n  align(i = 1:nrow_part(dem_models), j = 2:ncol_keys(dem_models), \n        align = 'center') |&gt;  \n  align(align = 'center', part = 'header') |&gt; \n2  autofit()\n\ndem_models\n\n\n\n1\n\nThe coefficient for GDP will be very small given its coding. This bit of syntax handles the rounding of values for this specific coefficient value. The coefficient is small enough that the default would be to simply print “0.000***” absent this line. See here for more.\n\n2\n\nThis makes the table fit the columns of the page. It works well with html output and okay when exporting to Word.\n\n\n\n\n\n\nTable 13.1: Predicting Democracy Levels\n\n\n\n (1)Intercept0.475***(0.027)% Women MPs (2003)0.005**(0.002)GDP Per Capita (2006)7.736e-06***(1.466e-06)Autocracy in 2006? (1 = Yes, 0 = Democracy)-0.310***(0.035)Num.Obs.152R20.550R2 Adj.0.541Notes: OLS or Logit Coefficients with standard errors in parentheses* p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\nThe DV in the model is a country’s score on the v2x_polyarchy measure of electoral democracy in the year 2020 provided by the Varieties of Democracy (V-Dem) project. The IVs include the following predictor variables: (1) the % of female members of parliament in the country in the year 2003; (2) the country’s gross domestic product per capita in the year 2006; and (3) a binary measure concerning whether the country was considered a democracy (=1) or an autocracy (=0) in the year 2006.\nWhat do the coefficients tell us about the relationship between the predictor variables and the DV?\nOkay…but are these differences actually meaningful? The numbers look small in some cases…but we’re dealing with continuous variables that may vary quite greatly, so it’s not clear whether the overall impact is meaningless or not. Consider the following descriptive data and histograms of the two continuous predictors:\nShow the code\n# Descriptive Data\ndemdata |&gt; select(GDP2006, Women2003) |&gt; \n  psych::describe()\n\n\n          vars   n    mean       sd median trimmed     mad min     max   range\nGDP2006      1 165 7148.13 10635.06 2172.0 4847.22 2765.05  93 54779.0 54686.0\nWomen2003    2 152   14.13     9.60   11.3   13.22    8.38   0    45.3    45.3\n          skew kurtosis     se\nGDP2006   1.92     3.15 827.94\nWomen2003 0.85     0.20   0.78\n\n\nShow the code\n# Plots\np1 &lt;- ggplot(demdata, aes(x = GDP2006)) + \n  geom_histogram(fill = 'white', color = 'black') + \n  labs(title = \"GDP Per Capita (2006)\")\n\np2 &lt;- ggplot(demdata, aes(x = Women2003)) + \n  geom_histogram(fill = 'white', color = 'black') + \n  labs(title = \"% Female MPs in 203\")\n\np1 + p2\n\n\n\n\n\n\n\n\nFigure 13.1: Continuous Predictor Variables - Distribution\nOkay, so a 1 dollar increase in GDP yields a very small increase in the democracy score. But…is a change of 1 dollar a meaningful comparison to make or care about? The GDP variable ranges from 93 to around 54,779 with a standard deviation of 10,635 after all. A dollar increase in GDP may be meaningful in some instances, but is it what best communicates the influence of country wealth in this one? Meanwhile, the % Female MPs measure also varies quite a bit more than by 1% albeit not as greatly as GDP (e.g,. SD = 9.6 vs. 10,635!). The point here is that our unstandardized coefficient tells us something relevant (e.g., the slope of a regression line when the predictor variable is continuous), but we may not want to rely solely on it to convey the importance of the changes on offer.2\nMaybe we can turn to standardized coefficients then?\n# DV and IV are standardized by 1 SD\nstandardize_parameters(m1)\n\n# Standardization method: refit\n\nParameter                           | Std. Coef. |         95% CI\n-----------------------------------------------------------------\n(Intercept)                         |       0.27 | [ 0.15,  0.40]\nWomen2003                           |       0.20 | [ 0.08,  0.32]\nGDP2006                             |       0.32 | [ 0.20,  0.44]\nregime type 2006 [Autocracy (2006)] |      -1.22 | [-1.48, -0.95]\nThe standardized coefficients fro Women2003 and GDP2006 tell us how many standard deviations we expect the DV to change when the IV in question changes by 1 standard deviation. The standardized coefficient for the regime type factor variable, meanwhile, indicate the difference between the two categories of the factor variable (Autocracy in 2006 vs. Democracy in 2006) in standard deviations. We could perhaps now turn to some ‘rules of thumb’ about effect sizes (such as those reported in the Statistics I materials for interpreting Cohen’s d) to make claims about the size of these relationships. The two continuous variables might thus be described as having a somewhat small effect, while the regime comparison yields a large one. However…have we really clarified things for ourselves or for those to whom we are communicated our results? What does a 0.2 or 0.32 or 1.22 SD change really mean in our given context? It seems like a lot…but are we talking about a change from a complete autocracy to a complete democracy? From an autocracy to a slightly less autocratic (but still not democratic) country? Something else?3 Moreover, what if our results were from a logistic model? Technically we can use an odds ratio there, but those aren’t particularly intuitive in their own right either.\nSaying something about the substantive effect of a variable can be difficult as discussed in Chapter 5. My recommendation here is to use your model. In particular, use the model to estimate predicted values, i.e., what value we would expect the DV to be, on average, given this model for some important values of your main IV(s). You can then integrate that information into your written reports directly and/or in relation to figures. Doing so will help flesh out what your coefficient is really implying about the relationship and, hence, give greater substance to your discussion.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Discussing Your Model Results</span>"
    ]
  },
  {
    "objectID": "analysis_discussion.html#example-model-and-some-initial-thoughts",
    "href": "analysis_discussion.html#example-model-and-some-initial-thoughts",
    "title": "13  Discussing Your Model Results",
    "section": "",
    "text": "% Women MPs\n\nCountries with more female MPs in the year 2003 are, on average, more democratic in the year 2020 than countries with fewer female MPs in 2003.\nWe expect democracy scores to increase by 0.005 scale points with each one unit (here, 1 %) increase in female MPs after adjusting for differences in country wealth and regime status.1 This relationship is statistically significant (p &lt; 0.01).\n\nGDP Per Capita (2006)\n\nRicher countries are, on average, more democratic in the year 2020 than poorer countries. We expect democracy scores to increase by approximately 7.7359493^{-6} scale points, on average, when a country’s GDP per capita increases by 1 USD after adjusting for differences in female representation and 2006 regime status. This relationship is statistically significant (p &lt; 0.001).\n\nAutocracy in 2006?\n\nCountries that were autocratic in 2006 remain less democratic in 2020 than countries that were democracies in 2006 after adjusting for differences in female representation and country wealth. If we compared countries with the same wealth in 2006 and same female representation in 2003, we’d expect the average democracy score in 2020 to be around -0.31 scale points lower in 2006 autocracies than 2006 democracies.\n\n\n1 If the goal of the model were to examine this variable in particular, then the two “control” variables here might be problematic given that they are based on measurements taken three years after the Women MPs measure. The main question we’d have to consider is whether it is plausible that having more/less female MPs causes a country to have a higher GDP and/or to influence its subsequent regime status (and particularly so on such a small time scale). The latter possibility may be particularly problematic given that our DV is…regime status (e.g., level of democracy)! If the % of female MPs in 2003 does in fact cause variation in 2020 democracy scores and does so in part because of its influence on wealth and/or 2006 regime status, then our estimate for % of female MPs in 2003 will be “biased” - it will almost certainly be smaller than it should be. See the discussion in Chapter 11. I am just using these variables because I have them laying around, but if I were actually writing this as a paper then I’d be worried.\n\n\n2 As I’ll discuss below, the coeficient for a binary/categorical variable is perhaps more directly relevant here since these types of variables involve comparisons between two (and only two) groups in the first place.\n\n\n3 The DV is also being standardized here (e.g., 1 SD change in X yields some amount of change in SDs of Y). We could omit that by including include_response = F in our syntax. That might make our discussions easier to follow in situations where the scale of the DV is easier or more intuitive to understand.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Discussing Your Model Results</span>"
    ]
  },
  {
    "objectID": "analysis_discussion.html#two-questions-to-think-about-for-substantive-significance",
    "href": "analysis_discussion.html#two-questions-to-think-about-for-substantive-significance",
    "title": "13  Discussing Your Model Results",
    "section": "13.2 Two Questions to Think About for Substantive Significance",
    "text": "13.2 Two Questions to Think About for Substantive Significance\nThe foregoing is a recommendation about the form of how you discuss your results: remind the reader about what your hypothesis, indicate what evidence would be consistent with it in your model, tell us whether that evidence shows up (e.g., is the coefficient in the right direction and statistically significant, and then use predicted values to flesh out the meaning of that coefficient and particularly so in contexts where the coefficient is statistically significant). However, this still elides the question about whether the patterns you see are meaningfully or substantively significant or not. This is a difficult to answer question because the answer may be research question and contingent (e.g., a “small” effect could be meaningful in some contexts but not others). However, I think you should consider two questions on this front to try and structure your discussion and interpretations.\nFirst, what comparison in terms of X values do you want to call our attention to? In other words, if you’re calculating predicted values or creating a predicted values plot, then what would a meaningful difference in X even be in your context?\nThis question is pretty straight forward to answer if your main IV is a factor variable: the comparison of interest is between the two categories of your binary variable or the reference group and the other categories (or, potentially one or two particular categories) of your categorical variable.\nThe answer to this question is perhaps less clear cut with a continuous IV. Per above, the coefficient for a continuous variable tells us about change in Y given a one unit change in X, but one unit may or may not be meaningful or important on its own right in particular examples. A 1 dollar difference in GDP seems somewhat trivial…but should we instead be comparing countries that differ by 100 USD, 1000 USD, 50,000 USD? You can (maybe should) provide a plot showing predicted values across the full range of GDP. That may nicely demonstrate the extent of change when moving from the minimum of X to its maximum…but maybe those values are outliers…so, is that the most important comparison to make? This is a question that you’ll need to give consideration in the context of your particular project and data. I’ll discuss some options below.\nSecond, once you have thought through what a meaningful comparison would be and provided some evidence about the difference in Y in that comparison, you then have to answer the question: “okay, but is the amount of difference important?”. Is it a “big” difference or not? This is a difficult question to answer since the answer can be question and data contingent. Some differences may be prima facie large: for instance, if the difference between “low” and “high” values of X is 8 scale ponits on a variable that ranges from 0 to 10 then, yeah, that’s probably a really large effect. Most of the time, however, you’ll probably not be finding differences of this magnitude. Here are three other ways of considering this question:\n\nPercent change and intuition\n\nOne could use a calculator (or R as a calculator) to calculate the percentage change represented by a given comparison.\nThe predicted democracy score for a country with 0 female legislators is 0.53 while it is 0.77 for a country with the maximum observed value on this variable. That yields a percentage change of: \\(\\frac{0.77 - 0.53)}{0.77} * 100 = 31.2\\%\\)\nIt’s up to intuition, perhaps, from there: does a 31.2% change seem large? It does to me actually…although it’s possible that the difference I’ve brought attention to here (maximum IV - minimum IV) might be unrepresentative. The third quartile for the Women MP variable is 19.5, for instance, so the maximum value seems like an outlier.\n\nVariation in the DV\n\nOne could alternatively look at the difference in comparison to the variation in the DV.\nOne way to do this is to compare the change to the length or range of the DV. For instance, Malhotra, Margalit, and Mo consider whether experiencing an economic threat from immigration is associated with greater opposition to immigration. They note on p. 401 of their manuscript that the difference in means on their DV between those not threatened and those threatened ranges from 0.06 to 0.12. Their DV is scaled from 0-1. They thus write: “These differences are also substantially meaningful, representing about 6 to 12% of the length of the scale.”\n\nThis type of discussion can be aided by first converting all non-factor variables used in an OLS model to a 0-1 scale as these authors do. This can be most easily done using the rescale() function in the scales package. For instance: mutate(newvar = scales::rescale(oldvar, to = c(0,1))). If that is done, then the resulting coefficients automatically compare the difference in the expected mean of Y between those at the maximum of the IV (=1) and those at the minimum (=0). One could then multiply by 100% to get to a similar discussion as the authors above.\n\nAlternatively, one could use the standard deviation of the DV to give some context here. Suppose that our DV ranges from 0-100 and that we compare observations at the maximum and minimum of an IV where we find an expected difference of 45.8 scale points. Suppose that the standard deviation of the DV is 30.24. The difference between predicted values at the maximum and minimum values of the IV is thus something like 1.5 standard deviations ( \\(\\frac{45.8}{30.24}=1.51\\)). That seems like a pretty sizable difference to me!\n\nThis is similar to the standardized coefficients above, but we are choosing what values of the IV to compare.\n\nOf course, one must also try and interpret such results as best one can as to whether it is ‘large’ or ‘meaningful’ in the context of the study.\n\nChange in Status\n\nA more qualitative approach. Does the difference in predictions show a change in status of some type? For instance, does moving across the range of the IV lead to a change in the nature of attitudes (e.g., from support to oppose?; from satisfaction to dissatisfaction?; from being unlikely to vote for Candidate A to being more likely than not to vote for Candidate A?).\n\n\nThe remaining sections walk through a few examples and discuss these issues further. They also introduce a command from the marginaleffects package that may help you on the way: the comparisons() function.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Discussing Your Model Results</span>"
    ]
  },
  {
    "objectID": "analysis_discussion.html#ols-model-factor-predictor",
    "href": "analysis_discussion.html#ols-model-factor-predictor",
    "title": "13  Discussing Your Model Results",
    "section": "13.3 OLS Model, Factor Predictor",
    "text": "13.3 OLS Model, Factor Predictor\nLet’s begin with perhaps the simplest example: your main IV is a factor variable (i.e., a binary variable or a categorical variable converted into a factor in R).4 We’ll use the example from above that features a binary indicator for whether a country was considered an autocracy in the year 1984 or a democracy in that year.\n4 If you’re using SPSS, then you need to manually create the separate dummy variables and include the appropriate number of them in the model. You need to include k-1 dummies where k = the number of categories on the original variable. A variable with four categories (e.g., North, West, South, and East) would necessitate the inclusion of three dummy variables (e.g., north (0 = not North, 1 = North), west (0 = not West, 1 = West), and south (0 = not South, 1 = South).\n\n (1)Intercept0.475***(0.027)% Women MPs (2003)0.005**(0.002)GDP Per Capita (2006)7.736e-06***(1.466e-06)Autocracy in 2006? (1 = Yes, 0 = Democracy)-0.310***(0.035)Num.Obs.152R20.550R2 Adj.0.541Notes: OLS or Logit Coefficients with standard errors in parentheses* p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\nLet’s say that we have the following hypothesis: “Hypothesis 1: Countries considered autocratic in 2006 remain less democratic in 2020 than countries considered democratic in 2006, all else equal”. We might begin our discussion as so:5\n5 References to table/figure numbers is, of course, a bit arbitrary here.\nI argued in Hypothesis 1 that countries considered autocratic in 2006 would remain less democratic in 2020 than countries considered democratic in 2006. Table 2 shows the results of my regression model where a negative coefficient for 2006 regime type would represent evidence consistent with H1. And, indeed, we see a negative and statistically significant (p &lt; 0.001) coefficient emerge for regime type. Autocratic countries in 2006 are expected to score 0.31 scale points lower on V-Dem’s polyarchy measure than 2006 democratic countries with equivalent levels of female representation and GDP per capita. The evidence in Table 2 is thus consistent with my expectations.6\n6 A note here: don’t say that you have “proven” your hypothesis. We don’t really do that. We may falsify them, but evidence consistent with our expectations do not mean that we have definitely proven our claims. Maybe our model is garbage, for instance. Maybe it only applies to our given context.\nThe foregoing is a very nice discussion: it clearly delineates what the expectations are, what evidence would be consistent with them, and whether that is the case. However, I think we can go a bit further to flesh this out per earlier discussions. In particular, we might use the predictions() command to calculated average predicted values of the DV and then integrate them into our written discussion either directly or by a reference to an additional figure we create.\nHere are the predicted values for the two categories of this variable:\n\npredvals &lt;- predictions(m1, \n            by = \"regime_type_2006\", \n            newdata = \"mean\") \n\npredvals\n\n\n regime_type_2006 Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n Democracy (2006)    0.604     0.0160 37.80   &lt;0.001  Inf 0.573  0.636\n Autocracy (2006)    0.294     0.0303  9.73   &lt;0.001 71.9 0.235  0.354\n Women2003 GDP2006\n      14.1    7067\n      14.1    7067\n\nColumns: rowid, regime_type_2006, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, Women2003, GDP2006, v2x_polyarchy \nType:  response \n\n\nAnd, here is them plotted out:\n\nggplot(predvals, aes(x = regime_type_2006, \n                     y = estimate)) + \n  geom_pointrange(aes(ymin = conf.low, \n                      ymax = conf.high)) + \n  geom_text(aes(label = round(estimate, 2)), \n            hjust = -0.5) + \n  scale_y_continuous(limits = c(0,1)) + \n  labs(x = \"Regime Type (2006)\", \n       y = \"Predicted 2020 Democracy Score\") + \n  theme_minimal(16)\n\n\n\n\n\n\n\n\nThese values may help us further communicate the substantive importance of this difference. On the one hand, the difference seems quite large when we consider it in relation to the theoretical range of the DV: 0-1. The difference between the two categories is thus around \\(\\frac{1}{3}\\) of the DV’s scale.7 Or, we could compare this difference to the standard deviation of the DV (0.25): \\(\\frac{0.31}{0.25} = 1.2\\). What about qualitatively? The V-Dem scale ranges from 0-1 with higher values = more democratic regime, but the specific values (0.3, 0.7, etc.) are a bit abstract as to what they actually represent.8 Democracy score data from Freedom House and elsewhere is often measured on an interval scale and then categorized. Lührmann, Tannenberg, and Lindberg discuss doing so for the V-Dem measure used here and suggest the regimes must have at least a 0.5 on this v2x_polyarchy variable to be considered democratic. Based on this, we might say that 2006 classifications remain pretty stable or consistent: countries considered autocratic in 2006 remain autocratic (on average) while countries considered democratic in 2006 remain democratic (on average).\n7 In practice, this variable ranges from 0.015 to 0.908 since there are no perfect (non)democracies.8 You can read more about V-Dem’s methodology here.We might thus update our discussion to make some of these points salient. (Note: it might make sense to divide this into two paragraphs.)\n\nI argued in Hypothesis 1 that countries considered autocratic in 2006 would remain less democratic in 2020 than countries considered democratic in 2006. Table 2 shows the results of my regression model where a negative coefficient for 2006 regime type would represent evidence consistent with H1. And, indeed, we see a negative and statistically significant (p &lt; 0.001) coefficient emerge for regime type. The difference between these countries is not just statistically significant, but also substantively meaningful. A difference of 0.31 scale points is around 1/3rd of the scale of the dependent variable. Figure 1, meanwhile, shows the predicted democracy score for each set of countries: 0.29 [95% CI; 0.24, 0.34] in 2006 autocracies and 0.60 [0.57, 0.64] in 2006 democracies. Lührmann, Tannenberg, and Lindberg recommend a score of 0.5 as a cut off point separating democracies from non-democracies on this measure. The results in Table 2 and Figure 1 thus indicate not just a different in degree, but also a difference in kind with previously autocratic countries remaining autocratic, on average, and previously democratic countries doing the same. Ultimately, the evidence in Table 2 is thus consistent with my expectations.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Discussing Your Model Results</span>"
    ]
  },
  {
    "objectID": "analysis_discussion.html#ols-model-continuous-predictor",
    "href": "analysis_discussion.html#ols-model-continuous-predictor",
    "title": "13  Discussing Your Model Results",
    "section": "13.4 OLS Model, Continuous Predictor",
    "text": "13.4 OLS Model, Continuous Predictor\nOkay, let’s consider a situation where our IV is continuous in nature such as the % Women MPs measure in this example:\n\n\n (1)Intercept0.475***(0.027)% Women MPs (2003)0.005**(0.002)GDP Per Capita (2006)7.736e-06***(1.466e-06)Autocracy in 2006? (1 = Yes, 0 = Democracy)-0.310***(0.035)Num.Obs.152R20.550R2 Adj.0.541Notes: OLS or Logit Coefficients with standard errors in parentheses* p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\nLet’s say that we have the following hypothesis: “Hypothesis 1: Countries with greater female representation are more democratic than countries with less female representation, all else equal”. We might begin our discussion as so:\n\nI argued in Hypothesis 1 that countries female representation would be positively associated with democracy levels. Table 2 shows the results of my regression model where a positive coefficient for female representation would represent evidence consistent with H1. And, indeed, there is a positive and statistically significant (p &lt; 0.01) coefficient emerge for this variable. Each one percentage increase in female representation is associated with an increase of 0.005 scale points, on average, after adjusting for differences in country wealth and prior regime status. The evidence in Table 2 is thus consistent with my expectations.\n\nAgain, not bad. But, 0.005 seems awfully small. Is this actually a meaningful comparison?\nOne place we could begin is by plotting expected democracy scores across the range of our continuous predictor variable. We could then produce a plot showing these results in our paper to anchor our discussions. We’ll first use the predictions() command to obtain sensible values of the IV to obtain predictions for and then return to the command to get the average predictions for each level:9\n9 We could just do summary(demdata$Women2003) to get these values. The risk here concerns missing data - if our model is only done on a subset of observations in our full dataset due to missing data on either the DV or one of the other IVs, then the minimum and maximum values might not match those used in fitting the model.\n# Range of IV via predictions() among observations in model\npredictions(m1) |&gt; \n  as_tibble() |&gt; \n  select(Women2003) |&gt; \n  summary()\n\n   Women2003     \n Min.   : 0.000  \n 1st Qu.: 7.675  \n Median :11.300  \n Mean   :14.130  \n 3rd Qu.:19.550  \n Max.   :45.300  \n\n#Get the predicted values\npredvals_cont &lt;- predictions(m1, \n                             newdata = datagrid(Women2003 = c(0, 7.7, 14.1, 19.6, 45.3)))\n\npredvals_cont\n\n\n Women2003 Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 % GDP2006\n       0.0    0.529     0.0276 19.2   &lt;0.001 269.6 0.475  0.584    7067\n       7.7    0.570     0.0190 30.0   &lt;0.001 654.3 0.533  0.608    7067\n      14.1    0.604     0.0160 37.8   &lt;0.001   Inf 0.573  0.636    7067\n      19.6    0.634     0.0182 34.9   &lt;0.001 881.7 0.598  0.669    7067\n      45.3    0.770     0.0520 14.8   &lt;0.001 162.2 0.668  0.872    7067\n regime_type_2006\n Democracy (2006)\n Democracy (2006)\n Democracy (2006)\n Democracy (2006)\n Democracy (2006)\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, GDP2006, regime_type_2006, Women2003, v2x_polyarchy \nType:  response \n\n#Plot them\nggplot(predvals_cont, \n       aes(x = Women2003, y = estimate)) + \n  geom_line() + \n  geom_ribbon(aes(ymin = conf.low, \n                  ymax = conf.high), alpha = 0.1) + \n  scale_y_continuous(limits = c(0,1)) + \n  theme_minimal(16) + \n  labs(y = \"Predicted Democracy Score (2020)\", \n       x = \"% Female MPs (2003)\")\n\n\n\n\n\n\n\n\nThe figure suggests a pretty sizable change in the DV moving across the full range of the IV. Our expectation among countries with 0% female legislators and mean/modal values on the control variables is 0.53 [0.48, 0.58], right around the 0.5 cut off discussed above. The expected values at the observed maximum of the IV is 0.77 [0.67, 0.87], more clearly democratic in nature. The difference between those values represents a 31% change, or a difference equivalent to around 1/3rd of the range of the DV, or a difference of around 0.96 standard deviations or so on the DV. This would indicate a pretty sizable difference.\nIncluding a plot like the one above into our paper would be a great way of fleshing out the meaning of our IV. However, is drawing attention to the difference between minimum and maximum values of this IV really the best comparison in this example? Consider again the distribution of the variable:\n\n\n\n\n\n\n\n\n\nThe maximum value here seems like a clear outlier. There is only one country at the maximum and the next closest values are at around 37 or so on the scale. The focus on the maximum - minimum might thus overstate the importance of this variable a bit.\nComparing values at the minimum and maximum of a continuous IV may be an intuitive way of conveying the cumulative or total effect of the variable. But, per above, it might not always be appropriate if this change is uncommon or represents differences between outliers. An alternative tactic that you may observe in researcher is to focus on other indicators for “low” and “high” values on a scale that stop short at the very extremes. Common examples include:\n\nThe 10th and 90th percentiles of the variable’s distribution\nThe 25th and 75th percentiles of the variable’s distribution\n1 standard deviation below and 1 standard deviation above the mean\n\nThere isn’t really a “right” answer as to which to use. Alternative numbers could be useful as well. Could we imagine a policy intervention, for instance, that leads to a change from 4% (10th percentile) female MPs to 28% female MPs? If not, then maybe a narrower interval would be more useful. Then again, perhaps gender quotas might have such an effect! Or, maybe the comparison of moving from 0% female MPs to average levels might be even more useful. There are no easy answers here but to think about your RQ, the data before you, and what might be particularly useful to draw attention to.\nLet’s return to our data. We can find the 25th and 75th quartiles via the summary() command. We can use the quantile() command, meanwhile to obtain percentiles at any other value we might care about.\n\n# summary()\nsummary(demdata$Women2003)\n\n# percentiles\nquantile(demdata$Women2003, \n1         probs = c(0.1, 0.2, 0.25, 0.75, 0.8, 0.9),\n         na.rm = T)\n\n\n1\n\nPut the percentiles here, but as decimals (e.g., 10th percentile \\(\\rightarrow\\) 0.1, etc.).\n\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  0.000   7.675  11.300  14.130  19.550  45.300      27 \n   10%    20%    25%    75%    80%    90% \n 3.830  6.040  7.675 19.550 21.480 28.300 \n\n\nLet’s say we think drawing attention to a “low” value of the 10th percentile (3.8% female MPs) and the 90th percentile (28.3%) would best convey the importance of our variable. I would still provide the corresponding full figure of predicted values from above, but I would supplement the text discussion with predicted values at these levels.\n\npredictions(m1, \n            newdata = datagrid(Women2003 = c(3.83, 28.3)))\n\n\n Women2003 Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 % GDP2006\n      3.83     0.55     0.0229 24.0   &lt;0.001 419.5 0.505  0.595    7067\n     28.30     0.68     0.0276 24.6   &lt;0.001 443.0 0.626  0.734    7067\n regime_type_2006\n Democracy (2006)\n Democracy (2006)\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, GDP2006, regime_type_2006, Women2003, v2x_polyarchy \nType:  response \n\n\n\nI argued in Hypothesis 1 that countries female representation would be positively associated with democracy levels. Table 2 shows the results of my regression model where a positive coefficient for female representation would represent evidence consistent with H1. And, indeed, there is a positive and statistically significant (p &lt; 0.01) coefficient emerges for this variable such that each one percentage increase in female representation is associated with an increase of 0.005 scale points, on average, after adjusting for differences in country wealth and prior regime status. The evidence in Table 2 is thus consistent with my expectations.\nHow important is the relationship between female representation and subsequent democracy scores? Figure 1 plots the democracy score we expect to observe across the range of female representation. The difference between predicted democracy scores at the maximum observed value for female representation (45.3%) and the observed minimum (0%) is quite substantial in nature (0.24 scale points or approximately one quarter of the range of the DV). However, the the maximum here is bit unrepresentative as only one country in the model has a female representation value above 40%. If we instead compare countries at the 90th (28.3% female representation) and 10th (3.83%) percentiles, then we’d expect to see a difference in democracy scores of 0.13 scale point on average (10th percentile: 0.55 [95% CI: 0.51, 0.60], 90th percentile: 0.68 [0.63, 0.73]). This still represents a fairly important difference. Lührmann, Tannenberg, and Lindberg, for instance, recommend using 0.5 as the cut off between democratic and non-democratic countries. Moving from a country with very little female representation to one with a comparatively high level (but equivalent values on the other predictor variables) thus represents a change from countries straddling the democracy/non-democracy line to ones that are more consistently democratic in nature.\n\nThere is one final tool that may be relevant here. The discussion above notes the predicted value and its corresponding uncertainty. It also mentions the difference between predicted values. However, this discussion lacks any mention of uncertainty. We can use the comparisons() command in the marginaleffects package to accomplish this end (see here for a longer discussion).\n\nexample_comparison &lt;- comparisons(m1, \n            variables = list(Women2003 = c(3.83, 28.3)), \n            newdata = 'mean')\n\n\ncomparisons(m1,\n\nThe name of the command is comparisons(). We then provide the name of the saved model object (m1 in this example.\n\nvariable = list(Women2003 = c(3.83, 28.3)),\n\nWe can then specify which variable the comparison should be done for and the values on that variable we want to compare. In essence, the command will use this information to calculated predicted values at these two levels of the IV and then calculate the difference between them.\n\nnewdata = 'mean')\n\nThis tells the command to hold the other predictor variables constant at their mean (continuous variable) or mode (categorical). Omitting this portion of the syntax would not really change things if the model is a linear regression like here - the slope of a linear regression line for a given IV in the model is the same regardless of the specific values of the other predictors. However, this is not the case with a logistic model.\n\n\nLet’s take a look at the results:\n\nexample_comparison\n\n\n      Term    Contrast Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n Women2003 28.3 - 3.83     0.13     0.0389 3.34   &lt;0.001 10.2 0.0536  0.206\n Women2003 GDP2006 regime_type_2006\n      14.1    7067 Democracy (2006)\n\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, Women2003, GDP2006, regime_type_2006, v2x_polyarchy \nType:  response \n\n\n\nTerm: This indicates the variable in the comparison.\nContrast: This indicates the type of comparison being offered. Here, we specified that we wanted to look at the difference in predicted value between cases with a value of 28.3 on the Women2003 IV and those with a value of 3.83, hence “28.3 - 3.83”.\nEstimate: The difference between the predicted values. 0.13 much as above.\nStd. Error, z, Pr(&gt;|Z|), 2.5%, and 97.%: The standard error, test-statistic, p-value for the comparison, and confidence intervals. You can ignore the “s” column.\n\nThe difference in predicted values here is 0.13 [95% CI: 0.05, 0.21] and statistically significant (p &lt; 0.001). This might be helpful information to convey in the written report:\n\nI argued in Hypothesis 1 that countries female representation would be positively associated with democracy levels. Table 2 shows the results of my regression model where a positive coefficient for female representation would represent evidence consistent with H1. And, indeed, there is a positive and statistically significant (p &lt; 0.01) coefficient emerges for this variable such that each one percentage increase in female representation is associated with an increase of 0.005 scale points, on average, after adjusting for differences in country wealth and prior regime status. The evidence in Table 2 is thus consistent with my expectations.\nHow important is the relationship between female representation and subsequent democracy scores? Figure 1 plots the democracy score we expect to observe across the range of female representation. The difference between predicted democracy scores at the maximum observed value for female representation (45.3%) and the observed minimum (0%) is quite substantial in nature (0.24 scale points or approximately one quarter of the range of the DV). However, the the maximum here is bit unrepresentative as only one country in the model has a female representation value above 40%. If we instead compare countries at the 90th (28.3% female representation) and 10th (3.83%) percentiles, then we’d expect to see a difference in democracy scores of 0.13 [95% CI: 0.05, 0.21] scale points on average (10th percentile: 0.55 [95% CI: 0.51, 0.60], 90th percentile: 0.68 [0.63, 0.73]). This still represents a fairly important difference. Lührmann, Tannenberg, and Lindberg, for instance, recommend using 0.5 as the cut off between democratic and non-democratic countries. Moving from a country with very little female representation to one with a comparatively high level (but equivalent values on the other predictor variables) thus represents a change from countries straddling the democracy/non-democracy line to ones that are more consistently democratic in nature.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Discussing Your Model Results</span>"
    ]
  },
  {
    "objectID": "analysis_discussion.html#logistic-model",
    "href": "analysis_discussion.html#logistic-model",
    "title": "13  Discussing Your Model Results",
    "section": "13.5 Logistic Model",
    "text": "13.5 Logistic Model\nThe foregoing all goes for logistic regression but perhaps double so since logistic coefficients are basically uninterpretable on their own. We thus should be especially keen to use our model by calculated predicted probabilities and average marginal effects (i.e., the average change in the probability that Y = 1 given a one unit change in X). At the same time, a discussion of predicted probabilities might be helped here by the fact that 0.5 is a clear change in status: from less likely than not that Y= 1 to more likely than not.\n\n\nShow the code\n# Data management\ndemdata &lt;- demdata |&gt; \n  mutate(\n    dem_decrease = factor(dem_decrease, \n                          levels = c(0, 1), \n                          labels = c(\"Stable/Increased Democracy\", \n                                     \"Decreased Democracy\")), \n    cvoting = factor(compulsory_voting, \n                     levels = c(0,1), \n                     labels = c(\"No Compulsory Voting\", \n                                \"Compulsory Voting\")))\n\n# Model\nm2 &lt;- glm(dem_decrease ~ Facebook + cpi + cvoting, \n          data = demdata, \n          family = \"binomial\")\n\n# Table (including Nagelkerke R2)\n\n# adding Nagelkerke R2\ndem_model_2 &lt;- modelsummary(m2, \n1                            output = 'modelsummary_list')\n\n2dem_model_2$glance$r2.nagelkerke &lt;- performance::r2_nagelkerke(m2)\n\n\n# Creating the table\n3dem_tab_2 &lt;- modelsummary(dem_model_2,\n  estimate = \"{estimate}{stars}\\n({std.error})\", \n  statistic = NULL, \n4  gof_map = c(\"nobs\", \"r2.nagelkerke\"),\n  coef_rename = c(\n               '(Intercept)' = 'Intercept', \n               \"Facebook\" = \"Facebook Members (%) in 2013\", \n               \"cpi\" = \"Corruption Perception Index\", \n               \"cvotingCompulsory Voting\" = \"Compulsory Voting System\"),\n             notes = list(\"Notes: Logit Coefficients with standard errors in parentheses\", \n                          \"* p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\"), \n             output = 'flextable' )\n\ndem_tab_2 &lt;- dem_tab_2 |&gt; \n  hline(i = nrow_part(dem_tab_2) - 2) |&gt;  \n  align(i = 1:nrow_part(dem_tab_2), j = 2:ncol_keys(dem_tab_2), \n        align = 'center') |&gt;  \n  align(align = 'center', part = 'header') |&gt; \n  autofit() \n\ndem_tab_2\n\n\n\n1\n\nThe Nagelkerke R2 is not added by default. The first step in getting around this is this line. output = modelsummary_list tells the command to simply export the underlying data that will be used in creating the table. We can then add new data before creating the table. No other formatting options should be used yet.\n\n2\n\noutput = modelsummary_list creates a list with two tables: (1) $tidy which contains information about the coefficients, standard errors, etc. and (2) $glance which contains information about model fit. This line adds a new column to that table called “r2.nagelkerke” that will contain the output of the r2_nagelkerke command from the performance package. The naming here is important: it needs to be “rs.nagelkerke” for the table to use the correct output.\n\n3\n\nWe then create the table but here we use the name of the object we created using modelsummary_list and which has the extra column from step 2.\n\n4\n\nAnd we include the Nagelkerke R2 like so.\n\n\n\n\n\n\nTable 13.2: Predicting Democracy Levels: Logit Analyses\n\n\n\n (1)Intercept1.078*(0.481)Facebook Members (%) in 20130.046**(0.015)Corruption Perception Index-0.033*(0.014)Compulsory Voting System0.103(0.540)Num.Obs.155R2 Nagelkerke0.10Notes: Logit Coefficients with standard errors in parentheses* p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\nTable 13.2 is our example model here. The DV again focuses on democracy score data, but here dichotomized such that countries with a score of 1 saw a decrease in their democracy score between 2010 and 2020 and those with a score of 0 saw either an increase or no change at all. Three IVs are included: (1) the % of people using Facebook in the country as of 2013, (2) the corruption perception index for the country (higher = less perceived corruption, and (3) a binary indicator for whether the country has a compulsory voting system (=1) or not (=0).\nThe coefficients here are on the log of the odds scale. We can use them to discuss the direction of the relationship but not the substance.\nLet’s start with the compulsory voting measure. It has a positive coefficient, which indicates that countries with a compulsory voting system were more likely to see a decrease in democracy from 2010 to 2020, but this difference is not statistically significant so we cannot rule out the possibility of no difference on the DV between these categories. We could perhaps stop there as it were, i.e., simply note that we do not find a significant difference. Sometimes that is just fine in the paper. However, I think we should still calculate the predicted probability that DV = 1 (democratic decline) for both categories and their associated uncertainty in this instance. We only have 155 observations in the data. Meanwhile, very few countries are recorded as having a compulsory voting system of some type.10 Perhaps there is a meaningless expected difference between countries…or perhaps there is a large difference with lots of uncertainty.\n10 This is a slight understatement. The data here comes from the v2elcomvot in the V-Dem data. This variable divided countries into four categories: No CV (n = 156), CV but without sanctions or sanctions are not enforced (n = 12), CV with enforced but minimal sanctions (n = 10), and Sanctions with considerable enforced sanctions (n = 1). Really, there are perhaps 11 countries, or perhaps even 1, with a true compulsory voting system.\ntable(demdata$compulsory_voting)\n\n\n  0   1 \n156  23 \n\n\nLet’s take a look at the predicted probabilities and the average marginal effect:\n\n# Predicted Probability\npredictions(m2, \n            by = \"cvoting\", \n            newdata = \"mean\")\n\n\n              cvoting Estimate Pr(&gt;|z|)   S 2.5 % 97.5 % Facebook  cpi\n Compulsory Voting       0.657  0.18972 2.4 0.420  0.835     19.8 44.2\n No Compulsory Voting    0.633  0.00421 7.9 0.543  0.715     19.8 44.2\n\nColumns: rowid, cvoting, estimate, p.value, s.value, conf.low, conf.high, Facebook, cpi, dem_decrease \nType:  invlink(link) \n\n# AME\navg_slopes(m2, variables = \"cvoting\")\n\n\n    Term                                 Contrast Estimate Std. Error     z\n cvoting Compulsory Voting - No Compulsory Voting   0.0223      0.116 0.192\n Pr(&gt;|z|)   S  2.5 % 97.5 %\n    0.848 0.2 -0.205  0.249\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nYeah, it’s not an uncertainty issue. The AME is 0.02 or a .2 percentage point difference. The predicted probabilities bring this home: Pr(Decline given compulsory voting system) = 0.66 [0.42, 0.84], Pr(Decline given non-compulsory system) = 0.63 [0.54, 0.72]. There is a lot of uncertainty around those estimates, particularly for the compulsory voting prediction, but this seems like a case where statistical insignificance is likely associated with substantive insignificance (at least, based on this model).\n\n\nShow the code\npredictions(m2, \n            by = \"cvoting\", \n            newdata = \"mean\") |&gt; \n  ggplot(aes(x = cvoting, y = estimate)) + \n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) + \n  geom_text(aes(label = round(estimate, 2)), hjust = -0.5) + \n  scale_y_continuous(limits = c(0,1)) + \n  labs(x = \"Compulsory Voting?\", \n       y = \"Predicted Probability of Democratic Decline\")\n\n\n\n\n\n\n\n\n\nHow might we write this up?\n\nI argued in Hypothesis 1 that countries with a compulsory voting system should be less likely to experience democratic erosion. Table 1 provides the results from my logistic model testing this proposition where a negative coefficient for the compulsory voting indicator would be consistent with Hypothesis 1. Ultimately, I do not find evidence in line with my expectations. The coefficient for compulsory voting is positive rather than negative. It is also statistically insignificant (p = 0.85). The predicted probability of democracy scores decreasing is nearly the same in both sets of countries when covariates are set at their mean or mode (Compulsory Voting: 0.63 [95% CI: 0.54, 0.72]; No Compulsory Voting: 0.66 [0.42, 0.84]). The resulting average marginal effect of the variable is rather trivial in nature (0.02 [-0.22, 0.26]). Compulsory voting thus appears unrelated to democratic declines, a point that I discuss in further in the Conclusion.\n\nLet’s turn to a continuous measure: the % of people using Facebook in the country as of 2013. Its coefficient is positive and statistically significant indicating that countries with more Facebook users were more likely to see a decrease in their democracy scores than countries with fewer Facebook users. We should be highly careful about saying this is a causal relationship of course - there are likely omitted variables here and, meanwhile, the DV is a change score from 2010 to 2020 and the IV is measured in 2013, so we likely cannot rule out the possibility that decreases in the 2010-2012 region led to more Facebook use!\nThis is a continuous variable. Here, I’d recommend including a plot of the predicted probabilities and then supplementing with a discussion in text, perhaps calling attention to “low” vs “high” comparisons of relevance/interest.\n\n# Range of variable among observations in model \npredictions(m2) |&gt; \n  as_tibble() |&gt; \n  select(Facebook) |&gt; \n  summary()\n\n# Predicted Values\npredvals_facebook &lt;- predictions(m2, \n            newdata = datagrid(Facebook = c(0, 3, 19.83, 35, 68)))\n\npredvals_facebook\n\n# AME\navg_slopes(m2, variables = \"Facebook\")\n\n#Plot\nggplot(predvals_facebook, aes(x = Facebook, y = estimate)) +\n  geom_line() + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.1) + \n  scale_y_continuous(limits = c(0,1)) + \n  labs(x = \"% Facebook Membership (2003)\", \n       y = \"Predicted Probability of Democratic Decline (2010-2020)\") + \n1  ggpubr::theme_pubclean()\n\n\n1\n\nUses a ggplot() theme contained in the ggpubr package. That package would need to be installed for the syntax to work. Just showing some different options here.\n\n\n\n\n\n\n\n\n\n\n\n    Facebook    \n Min.   : 0.00  \n 1st Qu.: 3.00  \n Median :15.00  \n Mean   :19.83  \n 3rd Qu.:35.00  \n Max.   :68.00  \n\n Facebook Estimate Pr(&gt;|z|)    S 2.5 % 97.5 %  cpi              cvoting\n      0.0    0.407  0.23428  2.1 0.270  0.560 44.2 No Compulsory Voting\n      3.0    0.441  0.39851  1.3 0.314  0.577 44.2 No Compulsory Voting\n     19.8    0.633  0.00421  7.9 0.543  0.715 44.2 No Compulsory Voting\n     35.0    0.777  &lt; 0.001 12.3 0.643  0.871 44.2 No Compulsory Voting\n     68.0    0.942  &lt; 0.001 10.7 0.768  0.988 44.2 No Compulsory Voting\n\nColumns: rowid, estimate, p.value, s.value, conf.low, conf.high, cpi, cvoting, Facebook, dem_decrease \nType:  invlink(link) \n\n     Term Estimate Std. Error   z Pr(&gt;|z|)    S   2.5 % 97.5 %\n Facebook   0.0101    0.00297 3.4   &lt;0.001 10.5 0.00427 0.0159\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nThe probability of a democratic decline given a one unit change in the Facebook measure is around 0.01 probability points, on average. The predicted values (plot) shows that the degree of change when moving a significant degree of the range of the IV is also pretty substantial. The probability when we’re at the minimum of the IV is around 0.41 [0.27, 0.56] but 0.94 [0.77, 0.99] at its maximum. Of course, that maximum value might not be the best one to focus on much like earlier.\n\n#Uses predictions() to just get observations in the model\npredictions(m2) |&gt; \n  ggplot(aes(x = Facebook)) + \n  geom_histogram(color = 'black', fill = 'white') + \n  scale_y_continuous(breaks = seq(from = 0, to = 25, by = 1)) + \n  scale_x_continuous(breaks = seq(from = 0, to = 70, by = 5))\n\n\n\n\n\n\n\npredictions(m2) |&gt; \n  select(Facebook) |&gt; \n  summary()\n\n    Facebook    \n Min.   : 0.00  \n 1st Qu.: 3.00  \n Median :15.00  \n Mean   :19.83  \n 3rd Qu.:35.00  \n Max.   :68.00  \n\n\nWhile there are lots of countries in the data that had no Facebook users in 2013, there is only one at the very maximum. This country is also a bit extreme when compared to the next closest ones which are at around 50-55 on the variable. Much as above, we can perhaps think about some other comparisons that might be more informative:\n\n# 10th and 90th percentile\nquantile(demdata$Facebook, probs = c(0.1, 0.9), \n         na.rm = T)\n\n10% 90% \n  1  47 \n\npredictions(m2, newdata = datagrid(Facebook = c(1, 47)))\n\n\n Facebook Estimate Pr(&gt;|z|)    S 2.5 % 97.5 %  cpi              cvoting\n        1    0.419    0.278  1.8 0.284  0.566 44.2 No Compulsory Voting\n       47    0.859   &lt;0.001 11.6 0.695  0.942 44.2 No Compulsory Voting\n\nColumns: rowid, estimate, p.value, s.value, conf.low, conf.high, cpi, cvoting, Facebook, dem_decrease \nType:  invlink(link) \n\ncomparisons(m2, \n            variables = list(Facebook = c(1, 47)), \n            newdata = 'mean')\n\n\n     Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 % Facebook\n Facebook   47 - 1    0.441      0.117 3.76   &lt;0.001 12.5 0.211   0.67     19.8\n  cpi              cvoting\n 44.2 No Compulsory Voting\n\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, Facebook, cpi, cvoting, dem_decrease \nType:  response \n\n# SD &lt; Mean vs. SD &gt; Mean\n# Stores the values first so I don't have to look them up and manually add them\nmean_below &lt;- mean(demdata$Facebook, na.rm = T) - sd(demdata$Facebook, na.rm = T)\nmean_above &lt;- mean(demdata$Facebook, na.rm = T) + sd(demdata$Facebook, na.rm = T)\n\npredictions(m2, newdata = datagrid(Facebook = c(mean_below, mean_above)))\n\n\n Facebook Estimate Pr(&gt;|z|)    S 2.5 % 97.5 %  cpi              cvoting\n     1.94    0.429    0.329  1.6 0.298  0.571 44.2 No Compulsory Voting\n    38.07    0.801   &lt;0.001 12.1 0.658  0.894 44.2 No Compulsory Voting\n\nColumns: rowid, estimate, p.value, s.value, conf.low, conf.high, cpi, cvoting, Facebook, dem_decrease \nType:  invlink(link) \n\ncomparisons(m2, \n            variables = list(Facebook = c(mean_below, mean_above)), \n            newdata = 'mean')\n\n\n     Term                            Contrast Estimate Std. Error    z Pr(&gt;|z|)\n Facebook 38.0687601647005 - 1.94397868880272    0.372      0.109 3.41   &lt;0.001\n    S 2.5 % 97.5 % Facebook  cpi              cvoting\n 10.6 0.158  0.586     19.8 44.2 No Compulsory Voting\n\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, Facebook, cpi, cvoting, dem_decrease \nType:  response \n\n# 0% vs. Median\npredictions(m2, newdata = datagrid(Facebook = c(0, 15)))\n\n\n Facebook Estimate Pr(&gt;|z|)   S 2.5 % 97.5 %  cpi              cvoting\n        0    0.407   0.2343 2.1 0.270  0.560 44.2 No Compulsory Voting\n       15    0.580   0.0841 3.6 0.489  0.665 44.2 No Compulsory Voting\n\nColumns: rowid, estimate, p.value, s.value, conf.low, conf.high, cpi, cvoting, Facebook, dem_decrease \nType:  invlink(link) \n\ncomparisons(m2, \n            variables = list(Facebook = c(1, 15)), \n            newdata = 'mean')\n\n\n     Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 % Facebook\n Facebook   15 - 1    0.161     0.0524 3.08  0.00208 8.9 0.0586  0.264     19.8\n  cpi              cvoting\n 44.2 No Compulsory Voting\n\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, Facebook, cpi, cvoting, dem_decrease \nType:  response \n\n\nWe might write:\n\nI argued in Hypothesis 1 that countries with greater Facebook usage would be more likely to see a decline in their democracy scores in the years between 2010 and 2020. Table 1 provides the results from my logistic model testing this proposition where a positive coefficient for the Facebook variable would be consistent with H1. And, indeed, a positive and statistically significant (p &lt; 0.01) coefficient emerges. The corresponding average marginal effect for the Facebook variable is 0.01 [95% CI: 0.004, 0.02], which indicates that the probability of seeing a decrease in a country’s democracy score increases by 0.01 probability points with each 1% increase in Facebook membership. This expected change adds up as seen in Figure 1 which plots the predicted probability of a democratic decline (holding the other predictors at their mean or mode) across the range of the Facebook variable. The amount of change is quite substantial. For instance, the predicted probability of democratic erosion among countries one standard deviation above the mean on the Facebook variable (38.1%) is 0.372 probability points greater than among countries one standard deviation below the mean of this variable (1.94%; difference in probability: 0.37 [0.16, 0.59]). Table 1 and Figure 1 thus provide consistent evidence in favor of H1.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Discussing Your Model Results</span>"
    ]
  },
  {
    "objectID": "analysis_03.html",
    "href": "analysis_03.html",
    "title": "14  Regression Table Formatting Suggestion",
    "section": "",
    "text": "14.1 Something slightly ugly looking\nThis example will focus on producing a regression table showing the results from three regression models. The IVs are the same in each model, but each one focuses on a different dependent variable. Here is the syntax needed to perform the regressions and create a regression table.\n## Run the models\n#DV = electoral democracy index\nmodel1 &lt;- lm(v2x_polyarchy ~ HDI2005 + Women2003 + Facebook, data = demdata)\n#DV = liberal democracy index\nmodel2 &lt;- lm(v2x_libdem ~ HDI2005 + Women2003 + Facebook, data = demdata)\n#DV = egalitarian democracy index\nmodel3 &lt;- lm(v2x_egaldem ~ HDI2005 + Women2003 + Facebook, data = demdata)\n\n## Create a list\nmodel_list &lt;- list(\n1  \"Electoral Democracy\" = model1,\n  \"Liberal Democracy\" = model2, \n  \"Egalitarian Democracy\" = model3\n)\n\n##Create a table\nmodelsummary(model_list, \n2             stars = T,\n3             gof_map = c(\"nobs\", \"r.squared\", \"adj.r.squared\"),\n4             coef_rename = c(\n               \"(Intercept)\" = \"Intercept\", \n               \"HDI2005\" = \"Human Development Index (2005)\", \n               \"Women2003\" = \"Percentage Female Legislators (2003)\", \n               \"Facebook\" = \"Facebook Use (proportion of population)\"), \n             notes = \"Notes: OLS Coefficients with SEs in Parentheses\")\n\n\n1\n\nProviding a name for the model is a good idea when we have different dependent variables.\n\n2\n\nIncludes asterisks for statistical significance\n\n3\n\nOnly include N, R2 , and Adjusted R2 in the table as goodness of fit statistics\n\n4\n\nGive informative labels for your variables\n\n\n\n\n \n\n  \n    \n    \n    tinytable_9fjgrfr35ri72ugqk0aa\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                Electoral Democracy\n                Liberal Democracy\n                Egalitarian Democracy\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\nNotes: OLS Coefficients with SEs in Parentheses\n        \n                \n                  Intercept                              \n                  0.116  \n                  -0.057  \n                  -0.115  \n                \n                \n                                                         \n                  (0.099)\n                  (0.097) \n                  (0.083) \n                \n                \n                  Human Development Index (2005)         \n                  0.371* \n                  0.415*  \n                  0.505***\n                \n                \n                                                         \n                  (0.166)\n                  (0.162) \n                  (0.139) \n                \n                \n                  Percentage Female Legislators (2003)   \n                  0.006**\n                  0.007***\n                  0.007***\n                \n                \n                                                         \n                  (0.002)\n                  (0.002) \n                  (0.002) \n                \n                \n                  Facebook Use (proportion of population)\n                  0.004* \n                  0.004** \n                  0.003*  \n                \n                \n                                                         \n                  (0.002)\n                  (0.002) \n                  (0.001) \n                \n                \n                  Num.Obs.                               \n                  142    \n                  142     \n                  142     \n                \n                \n                  R2                                     \n                  0.384  \n                  0.463   \n                  0.522   \n                \n                \n                  R2 Adj.                                \n                  0.371  \n                  0.451   \n                  0.511\nThe table above provides the coefficients from each model and then, underneath them, the coefficient’s standard error. Here is my pet peeve: modelsummary() does not include the coefficient and SE in the same cell of the table, which can lead to situations where you get extra spacing between the coefficient and standard error. We can see this above with the “Percentage Female Legislators (2003)” and “Facebook Use (proportion of population)” variables.\nThere is nothing wrong about the output above. I just think it looks kind of ugly. We could perhaps avoid it by specifying a shorter label for the two variables…but that would involve some trial and error to get things right and shortening the label might make it harder to understand the table. But, thankfully, there is another way!",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression Table Formatting Suggestion</span>"
    ]
  },
  {
    "objectID": "analysis_03.html#solution",
    "href": "analysis_03.html#solution",
    "title": "14  Regression Table Formatting Suggestion",
    "section": "14.2 Solution",
    "text": "14.2 Solution\nWe can avoid the scenario above by changing what modelsummary() puts in the estimate cell. In addition, we’ll have modelsummary() use a different backend for outputting the table to make sure this looks good when we export to Word. Here is some updated syntax:\n\n# The alternative table-formatting package we'll use\nlibrary(flextable)\n\n#Revised table syntax\nmodelsummary(model_list, \n             estimate = \"{estimate}{stars}\\n({std.error})\", \n             statistic = NULL, \n             gof_map = c(\"nobs\", \"r.squared\", \"adj.r.squared\"), \n             coef_rename = c( \n               \"(Intercept)\" = \"Intercept\", \n               \"HDI2005\" = \"Human Development Index (2005)\", \n               \"Women2003\" = \"Percentage Female Legislators (2003)\", \n               \"Facebook\" = \"Facebook Use (proportion of population)\"), \n             notes = list(\"Notes: OLS Coefficients with SEs in Parentheses\",\n                          \"* p &lt; 0.05; ** p &lt; 0.01; *** p &lt; 0.001\"), \n             output = 'flextable')\n\n Electoral DemocracyLiberal DemocracyEgalitarian DemocracyIntercept0.116(0.099)-0.057(0.097)-0.115(0.083)Human Development Index (2005)0.371*(0.166)0.415*(0.162)0.505***(0.139)Percentage Female Legislators (2003)0.006**(0.002)0.007***(0.002)0.007***(0.002)Facebook Use (proportion of population)0.004*(0.002)0.004**(0.002)0.003*(0.001)Num.Obs.142142142R20.3840.4630.522R2 Adj.0.3710.4510.511Notes: OLS Coefficients with SEs in Parentheses* p &lt; 0.05; ** p &lt; 0.01; *** p &lt; 0.001\n\n\nHere is what changed:\n\nlibrary(flextable)\n\nI first load the flextable package.1 I do this for two reasons: (1) I like the formatting of the table a bit better than the default output from modelsummary and (2) I ran into some issues when exporting the table to Word with the other modifications that using this command avoids.\n1 Normally this would be done at the start of the R script with the other packages.\nestimate = \"{estimate}{stars}\\n({std.error})\"\n\nThis is the big change. The default behavior of modelsummary is to present the coefficients from the regression model, which are stored in a column named estimate. Here, we are overriding that default behavior by explicitly specifying what should be in the cell - this is done via the information in the curly brackets. We are telling the command to first plot the coefficient estimate ({estimate}), and then symbols for statistical significance ({stars}). We then tell the command to create a new line in the cell (\\n) and to provide the standard errors surrounded by parentheses (({std.error})). This makes sure that everything is presented in the same cell, which will make sure that we do not get any spacing between the coefficient and standard error, regardless of how long the variable labels happen to be.\n\nstatistic = NULL\n\nThe statistic = option handles whether to plot the standard error or confidence interval for the coefficient (default option = standard error) in a row below the coefficient. We have already printed that statistic via {std.error}, so we tell modelsummary not to print anything here.\n\nnotes = list(….)\n\nThere is a change here: an added line that indicates what the asterisks refer to. We omitted the stars = TRUE option from this syntax because it would end up printing the asterisks next to the standard errors rather than next to the coefficient. However, manually asking for the stars to be printed as above (e.g., {estimate}{stars}) means that modelsummary() won’t automatically print information about what the different asterisks refer to. Hence, this bit of syntax to get around this issue.\n\noutput = 'flextable'\n\nmodelsummary() can use a variety of different table making “back-ends” to create the table; see this page. This tells the command to use the flextable package. This influences the general look of the table as you can probably notice. It also makes sure that the linebreak between the coefficient and standard error is used when we export the table to a Microsoft Word document - other table packages simply printed “\\n” rather than creating a new line. The use of this package also has implications for how we export this table to a .docx file, as discussed below.\n\n\nThere is one other step I would take here. It is probably not noticeable above, but the columns listing the coefficients are not centered. We can, of course, export the table to Word and then center it there, but I’m going to handle this all in one go via syntax. In addition, the horizontal line separating “Num.Obs” and the coefficients above it does not always get printed out when exporting to Word, so I’ll add some syntax to make sure that happens. These steps are optional since you could do them manually later.\n\n# Save the modelsummary results to an object\n\nreg_table &lt;- modelsummary(model_list, \n             estimate = \"{estimate}{stars}\\n{std.error}\", \n             statistic = NULL, \n             gof_map = c(\"nobs\", \"r.squared\", \"adj.r.squared\"), \n             coef_rename = c( \n               \"(Intercept)\" = \"Intercept\", \n               \"HDI2005\" = \"Human Development Index (2005)\", \n               \"Women2003\" = \"Percentage Female Legislators (2003)\", \n               \"Facebook\" = \"Facebook Use (proportion of population)\"), \n             notes = list(\"Notes: OLS Coefficients with SEs in Parentheses\",\n                          \"* p &lt; 0.05; ** p &lt; 0.01; *** p &lt; 0.001\"),\n             output = 'flextable')\n\n# Add some formatting - all of these commands\n# come from the flextable package - it needs to be loaded \n# before they are used\n\nreg_table &lt;- reg_table |&gt; \n1  hline(i = nrow_part(reg_table) - 3) |&gt;\n  align(i = 1:nrow_part(reg_table), j = 2:ncol_keys(reg_table), \n2        align = 'center') |&gt;\n3  align(align = 'center', part = 'header')\n\n\n1\n\nMakes sure a line is printed separating the coefficients from the goodness of fit statistics.\n\n2\n\nMakes sure the columns with coefficients are centered\n\n3\n\nMakes sure the column titles/labels are also centered\n\n\n\n\nI first create the table and store it as an object (named here as reg_table). Then I use some syntax to format the table. This syntax requires the flextable library to be loaded first:\n\nhline(i = nrow_part(reg_table) - 3) |&gt;\n\nThis line makes sure there is a horizontal line (hline) between the last standard error and the part of the table with the number of observations and R2 statistics. The nrow_part… stuff handles where the line is drawn. One can specify a specific number here; for instance, i = 5 would tell the command to draw a line after the fifth row. However, that means you have to create the table and count, which is annoying. Instead, this asks the command to calculate the total number of rows in the table (nrow_part(reg_table)) and then draw the line after whatever row = total number - 3. I use 3 here because I have three rows in the goodness of fit area (Num.Obs, R2, and Adj. R2). If I had 4 things in that area, then I’d use 4 instead of 3. This line of syntax can be used in your own examples - you’d just need to update the regression table name (e.g., nrow_part(reg_table) to nrow_part(name of your table).\n\nalign(i = 1:nrow_part(reg_table), j = 2:ncol_keys(reg_table),  align = 'center')\n\nThis handles column alignment. The logic here is similar to above: instead of creating the table, counting the number of columns that need centering, and using those specific numbers, I am using some commands in flextable to handle these steps for me. i = 1:nrow_part(): I want each row from row 1 to the final row to be centered. j = 2:ncol_keys(): I want columns 2 through however many columns there are in the table to be centered. This makes sure the column with the variable labels is not centered - just those with the coefficients/standard errors. Again, this syntax can be used freely.\n\nalign(align = 'center', part = 'header')\n\nThis makes sure the column header information (e.g., the model names) is also centered. This could stay the same.\n\n\nLet’s take a look:\n\n# Let's take a look\nreg_table\n\n Electoral DemocracyLiberal DemocracyEgalitarian DemocracyIntercept0.1160.099-0.0570.097-0.1150.083Human Development Index (2005)0.371*0.1660.415*0.1620.505***0.139Percentage Female Legislators (2003)0.006**0.0020.007***0.0020.007***0.002Facebook Use (proportion of population)0.004*0.0020.004**0.0020.003*0.001Num.Obs.142142142R20.3840.4630.522R2 Adj.0.3710.4510.511Notes: OLS Coefficients with SEs in Parentheses* p &lt; 0.05; ** p &lt; 0.01; *** p &lt; 0.001\n\n\nYou’re likely writing in Word or Google Docs, so you need to export the table to a .docx file so that you can copy and paste the table into your report. We need to use a command from the flextable library to do this since we changed the table-making back-end above.\n\n# How to export to word\n\nsave_as_docx(reg_table, \n             path = \"regression_table.docx\")\n\nAll you have to do here is update the name of the table in the command (e.g., from “reg_table” to whatever you have named your table object).\nHere is what this looks like in Word:\n\nThe one remaining negative: things look a bit “scrunched up”, i.e., the table is not using all available horizontal space in Word. However, this can be fixed in Word by clicking using the “Autofit” option in Word.2\n2 I believe this would be handled by the Format \\(\\rightarrow\\) Table \\(\\rightarrow\\) Distribute Columns option in Google Docs. There is an autofit() option in the flextable package that will try and do this for us. We could have added this line |&gt; autofit() as the final line when making the alignment changes above to use it. However, the command is perhaps a bit too aggressive when exporting to Word - instead of taking too little space, it tends to take a little bit too much. We’d then have to manually fix things anyways.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression Table Formatting Suggestion</span>"
    ]
  },
  {
    "objectID": "analysis_04.html",
    "href": "analysis_04.html",
    "title": "15  Combining Plots",
    "section": "",
    "text": "15.1 Example Using patchwork\nLet’s say that our paper was focused on investigating the relationship between inequality and democracy scores. Democracy, of course, is a complex concept. We can see this from the name of one of the most important sources of data on regime type available to researchers (emphasis mine): the Varieties of Democracy (V-Dem) project. The V-Dem dataset, for instance, includes v2x_polyarchy which refers to the extent of electoral democracy, v2x_libdem which focuses on the extent of liberal democracy, and v2x_egaldem which focuses on the extent of egalitarian democracy.1 Perhaps we plan on analyzing all three indicators as DVs in our paper. We would naturally want to describe the nature of these variables at some point in our paper, i.e., provide some indication about their central tendency and how much they vary. We could do this rather simply in-text by telling the reader about the mean and standard deviation of each variable. (“My first dependent variable is a measure of electoral democracy from V-Dem. This variable measures blah blah blah (mean = X, SD = X). My second….”) We could also provide a nice figure showing the degree of variation in these variables as well to supplement these discussions with a histogram making a lot of sense given the nature of these variables. For example:\n# Plot 1\npoly_plot &lt;- ggplot(demdata, aes(x = v2x_polyarchy)) + \n1  geom_histogram(color = \"black\", fill = \"white\") +\n  labs(title = \"Electoral Democracy\", \n2       x = NULL, y = \"Count\") +\n3  scale_x_continuous(limits = c(0,1))\n\n# Plot 2\nlib_plot &lt;- ggplot(demdata, aes(x = v2x_libdem)) + \n  geom_histogram(color = \"black\", fill = \"white\") +  \n  labs(title = \"Liberal Democracy\", \n       x = NULL, y = \"Count\") + \n  scale_x_continuous(limits = c(0,1)) \n\n\n# Plot 3\negal_plot &lt;- ggplot(demdata, aes(x = v2x_egaldem)) + \n  geom_histogram(color = \"black\", fill = \"white\") +  \n  labs(title = \"Egalitarian Democracy\", \n       x = NULL, y = \"Count\") + \n  scale_x_continuous(limits = c(0,1)) \n\n#The output\npoly_plot\n\n\n1\n\nThis alters how the histogram bars are presented. fill= covers the interior color, while color concerns the color of the borders.\n\n2\n\nThe title provides information of what the x-axis is showing, so I removed the x-axis label (x = NULL) to avoid redundancy.\n\n3\n\nSets the “limits” or range of the x-axis. Not strictly needed, but made sense here since I’m plotting different variables with the same theoretical range but potentially different observed minimums and maximums.\n\n\n\n\n\n\n\n\n\n\nlib_plot\n\n\n\n\n\n\n\negal_plot\nThis is nice - we can see how much variation there is on the variables and also some differences between them (e.g., more countries at the higher levels of electoral democracy than liberal or egalitarian democracy). In terms of the paper, however, it might be a bit awkward to provide each histogram as its own figure - it’s not wrong per se, but it would take up a lot of space in the paper.\nInstead of providing each plot separately, we can combine these different histograms into a single figure and include that in our thesis. We can then refer to that figure as we go. The patchwork package is one very useful package for doing this (see their webpage for all the different things you can do with this package).2\nHere is an example:\n#Load the package\n1library(patchwork)\n\n#Combine the plots\npoly_plot + lib_plot + egal_plot\n\n\n1\n\nNormally we would load all of our packages at the very beginning of our script file.\nThe simplest way to combine plots with patchwork is by using a + sign as here. This may not be the best looking version of our figure though given the number of plots we’re combining. They are currently a bit scrunched up since there is only so much horizontal space in the figure. We can use patchwork to combine the different figures a bit differently to avoid this issue (if we think it is a problem). For instance, we could stack each plot on top of each other (use more vertical space); this is generally done by using the / divider rather than a + sign. We could mix and match the + and \\ signs as in the second plot below. Or, we could explicitly specify how many rows there should be as in the third option.3\n#One stacked on top of the other\npoly_plot / lib_plot / egal_plot\n\n\n\n\n\n\n\n#One plot on top, two side by side\npoly_plot / (lib_plot + egal_plot)\n\n\n\n\n\n\n\n#Setting the number of rows or columns\npoly_plot + lib_plot + egal_plot + plot_layout(nrow = 2)\nIn this instance, I think either the first or third options works the best. I kind of like the first one because it best facilitates a comparison across the different variables. If that were not of interest (e.g., if one plot was a histogram of a continuous variable and another was a bar-plot of a categorical variable), then specifying the number of columns/rows might be better.\nWe can export this figure by storing the output as an object and then using ggsave() to save the image as a .png file. I will take one final step here when storing the final plot: I’ll add a ‘tag’ (“A”, “B”, “C”) to each plot. This makes it easier to reference the different components of the plot when writing about them (e.g,. “Plot A in Figure 1 shows….Meanwhile, Plot B…). 4\n# Store\nfinal_plot &lt;- poly_plot / lib_plot / egal_plot + \n  plot_annotation(tag_levels = 'A')\n\n#Save\nggsave(\"final_plot.png\", \n       plot = final_plot, \n       height = 8, width = 12)\nThe ggsave() command works as follows:\nThe example above focused on combining histograms. This can of course be used with other types of plots, with combinations of types, and with coefficient plots. It is not always needed, but can make sense in some instances.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Combining Plots</span>"
    ]
  },
  {
    "objectID": "analysis_04.html#example-using-patchwork",
    "href": "analysis_04.html#example-using-patchwork",
    "title": "15  Combining Plots",
    "section": "",
    "text": "1 To say nothing of v2x_partipdem (participatory democracy) and v2x_delibdem (deliberative democracy)!\n\n\n2 We could also “reshape” our data to accomplish these ends as well; see Chapter 16 .\n\n\n3 We could also specify the number of columns (ncol = 2) .\n\n\n4 Tagging, and the various ways one could modify these tags, is discussed on this page of the patchwork website.\n\n\n\"final_plot.png\"\n\nYou begin by specifying the name of the output file. You can control the format here as well. I save the file as a .png file, which is a common figure output file type. There are other options, but this one should be sufficient for the thesis.\n\nplot = …\n\nI then tell the command what object should be saved in the figure. If this line is missing then ggsave will save the most recently produced plot…which may not be the one you want to save!\n\nheight = ….\n\nI then specify the dimensions of the figure. These are in inches by default, but can be changed to other units as well (e.g., including units = 'cm' would allow you to control the size in centimeters). This is usually a trial and error approach - save the image, open it up and see if it is distorted in some way, and then update as needed (make wider, make shorter, etc.).",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Combining Plots</span>"
    ]
  },
  {
    "objectID": "analysis_pivot.html",
    "href": "analysis_pivot.html",
    "title": "16  Reshaping Datasets",
    "section": "",
    "text": "16.1 Wide vs. Long Datasets\nLet’s first get a handle on what is meant by “wide” and “long” datasets.\nThe following dataset is a simplified version of the European Social Survey:\n#Import our data\ness &lt;- import(\"./data/ess_small.rda\")\n\n#A snapshot\nhead(ess)\n\n    idno contplt wrkprty wrkorg badge sgnptit pbldmn bctprd eisced agea trstprl\n1 600002       2       2      2     2       2      2      2      4   53       7\n2 600003       1       2      1     2       1      2      1      5   41       4\n3 600005       2       2      1     2       2      2      2      7   42       7\n4 600011       2       2      2     2       2      2      2      1   87       3\n5 600013       2       2      2     2       2      2      2      1   78       3\n6 600014       2       2      2     2       1      2      2      2   48       5\n  trstlgl trstplc trstplt\n1       8       6       6\n2       5       5       2\n3       8       8       7\n4       4       8       3\n5       9       8       6\n6       3       0       3\nThe columns are different variables. idno, for instance, is a unique and anonymized ID number for each respondent in the survey. The variable between contplt and bctprd provide data on whether the respondent indicated that they had completed some type of political behavior (e.g, wrkprt = 1 if the respondent reported working for a political party and 2 if not). The remaining columns provide data on education (eisced), age (agea), and trust in various institutions (trstprl through trstplt).\nThe foregoing is what we would call a “wide” dataset. The first column of a dataset is usually an identifier.1 A wide-formatted dataset is one wherein the values in this identifier column do not repeat. Each row in the dataset above focuses on a different survey respondent (as seen from the unique values in idno).\nCompare what is shown above to the following dataset, which focuses on data from the Varieties of Democracy (V-Dem) project:\n#Import our data\nvdem &lt;- import(\"./data/vdem_small.rda\")\n\n#A snapshot\nhead(vdem, n = 10L)\n\n   country_name year v2x_polyarchy v2x_egaldem v2x_corr\n1        Mexico 1789         0.028          NA     0.68\n2        Mexico 1790         0.028          NA     0.68\n3        Mexico 1791         0.028          NA     0.68\n4        Mexico 1792         0.028          NA     0.68\n5        Mexico 1793         0.028          NA     0.68\n6        Mexico 1794         0.028          NA     0.68\n7        Mexico 1795         0.028          NA     0.68\n8        Mexico 1796         0.028          NA     0.68\n9        Mexico 1797         0.028          NA     0.68\n10       Mexico 1798         0.028          NA     0.68\nThis is a “long” formatted dataset. The first column is again an identifier. In this case, the variable is named country_name and provides the name of each country in the dataset. The values here repeat. We can think of the observations in a “long” formatted dataset as a “dyad”.2 In this example, the dyad would be a “country-year” - our data includes observations on countries by year. A dyad could refer to something else though. For instance, suppose we ask respondents to evaluate multiple political parties; we could organize our dataset in a “long” format wherein each row provides a respondents evaluation of a distinct party (e.g., a respondent-party dyad).\nMost students in my BAP will use survey data. This type of data will generally be in a “wide” format unless, perhaps, the survey is a panel survey (e.g., the respondent is surveyed at multiple time points). In general, it will make the most sense to keep the data in that format. However, there are some situations wherein it may be useful to “reshape” the data (e.g., convert a wide dataset to a long dataset). This can facilitate some types of descriptive analyses and may be required for others. Likewise, you may have need to reshape a long dataset into a wider format. The remainder of this document will show you how to do this using two commands from the tidyverse: pivot_longer() and pivot_wider().",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reshaping Datasets</span>"
    ]
  },
  {
    "objectID": "analysis_pivot.html#wide-vs.-long-datasets",
    "href": "analysis_pivot.html#wide-vs.-long-datasets",
    "title": "16  Reshaping Datasets",
    "section": "",
    "text": "1 This is not always true. Big social surveys like the ESS or ANES will often starts with lots of columns focused on survey-related variables (e.g., the country of the respondent, when the respondent answered the survey, what mode was used, weights, etc.) before getting to the actual survey responses.\n\n\n2 We could perhaps have data with an even more complicated unit of analysis. Instead of countries, or country-years, perhaps we have party-country-years or something.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reshaping Datasets</span>"
    ]
  },
  {
    "objectID": "analysis_pivot.html#from-wide-to-long-pivot_longer",
    "href": "analysis_pivot.html#from-wide-to-long-pivot_longer",
    "title": "16  Reshaping Datasets",
    "section": "16.2 From Wide to Long: pivot_longer()",
    "text": "16.2 From Wide to Long: pivot_longer()\nLet’s consider a first use case where it may make sense to reshape our data as an example of how to do so. The ESS dataset above includes columns (variables) pertaining to whether the respondent reported performing various political actions. Here is an overview:\n\n\nShow the code\ness |&gt; \n  select(contplt:bctprd) |&gt; \n1  sjPlot::view_df()\n\n\n\n1\n\nI am using the view_df() command from the sjPlot package to take a look at the variable’s attributes. This set up allows me to use the command without loading the package first, which sometimes can be handy.\n\n\n\n\n\n\nData frame: select(ess, contplt:bctprd)\n\n\n\n\n\n\n\n\n\nID\nName\nLabel\nValues\nValue Labels\n\n\n1\ncontplt\nContacted politician or government official last\n12 months\n1\n2\nNA(b)\nNA(c)\nNA(d)\nYes\nNo\n\n\n2\nwrkprty\nWorked in political party or action group last 12\nmonths\n1\n2\nNA(b)\nNA(c)\nNA(d)\nYes\nNo\n\n\n3\nwrkorg\nWorked in another organisation or association last\n12 months\n1\n2\nNA(b)\nNA(c)\nNA(d)\nYes\nNo\n\n\n4\nbadge\nWorn or displayed campaign badge/sticker last 12\nmonths\n1\n2\nNA(b)\nNA(c)\nNA(d)\nYes\nNo\n\n\n5\nsgnptit\nSigned petition last 12 months\n1\n2\nNA(b)\nNA(c)\nNA(d)\nYes\nNo\n\n\n6\npbldmn\nTaken part in lawful public demonstration last 12\nmonths\n1\n2\nNA(b)\nNA(c)\nNA(d)\nYes\nNo\n\n\n7\nbctprd\nBoycotted certain products last 12 months\n1\n2\nNA(b)\nNA(c)\nNA(d)\nYes\nNo\n\n\n\n\n\n\nLet’s say we were writing a paper where we are investigating the relationship between education and political participation. We might hypothesize that those with a university degree will be more likely to take action than those without one.\nWe should first begin by getting to know our variables. For instance, we might first ask whether rates of participation vary between these different behaviors. This may help us provide context for the results of the statistical model that we report later on in our paper.\nOne way we can do this is by creating a figure that plots the proportion of respondents who reported taking each action. However, our dataset is not very well organized for this goal given that each participation measure is a separate column in the dataset. Reshaping the data so that it is in a longer format can get around this problem. Specifically, we’ll do the following:\n\nReshape the data into a respondent-behavior dyad\nUse group_by() and summarize() to create a data object with the proportions (see Section 3.2 in the Statistic I R book for a refresher on these tools)\nPlot our data\n\nLet’s begin by reshaping the data. Here is the syntax:\n\ness_long &lt;- ess |&gt; \n  pivot_longer(\n    cols = c(\"contplt\", \"wrkprty\", \"wrkorg\", \"badge\", \n             \"sgnptit\", \"pbldmn\", \"bctprd\"), \n    names_to = \"measure\", \n    values_to = \"outcome\" )\n\nHere is the data produced by this command. Seeing it will help illuminate what I did in the syntax above:\n\nhead(ess_long, n = 10L)\n\n# A tibble: 10 × 9\n     idno eisced  agea trstprl trstlgl trstplc trstplt measure outcome\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1 600002      4    53       7       8       6       6 contplt       2\n 2 600002      4    53       7       8       6       6 wrkprty       2\n 3 600002      4    53       7       8       6       6 wrkorg        2\n 4 600002      4    53       7       8       6       6 badge         2\n 5 600002      4    53       7       8       6       6 sgnptit       2\n 6 600002      4    53       7       8       6       6 pbldmn        2\n 7 600002      4    53       7       8       6       6 bctprd        2\n 8 600003      5    41       4       5       5       2 contplt       1\n 9 600003      5    41       4       5       5       2 wrkprty       2\n10 600003      5    41       4       5       5       2 wrkorg        1\n\n\nThe first column is idno, our respondent identifier. We can see now that the values here now repeat - each respondent is now represented multiple times in the data. We then get columns for education, age, and trust. These still provide the values for these variables for each respondent. We can see that they do not vary within respondents (i.e., the same values are provided for each row associated with respondent 600002).3 We then see a column named measure - the contents of this column indicate which participation measure the particular row tells us about. Row 1, for instance, tells you how respondent 600002 answered the “contplt” question, while row 2 tells you how respondent 600002 answered the “wrkprty” question, and so on. Finally, outcome gives the specific value provided on the participation measure associated with the respondent. A 2 on these measures indicate that the respondent did not take the action in question, while a 1 indicates that they did. Respondent 600002 did not report taking any of the actions in question.\n3 In practice, I would probably have a precursor line in my syntax that selects only the variables I want in the reshaped dataset to avoid including unnecessary elements. For instance: ess |&gt; select(contplt, wrkprty, wrkorg, badge, sgnptit, pbldmn, bctprd) |&gt; pivot_longer(…)Let’s unpack the syntax now:\n\ncols = c(\"contplt\", …)\n\nThis tells the command which columns should be part of the reshaping process. I included the variable names (in parentheses) within c(). I could have made this much simpler in the present instance by instead using: cols = contplt:bctprd. (Quotation marks not needed here.) This would have told R to include all of the columns between contplt and bctprd when reshaping to the longer data object. This is a generally easier thing to type out, but would have been problematic had there been a variable in that sequence that I did not want to reshape by.\n\nnames_to = \"measure\"\n\nThis is how we indicate the name for the new column that provides information on the columns we’ve made the dataset longer by. If this omitted, then pivot_longer() would use “name” as the, uh, name of this column.\n\nvalues_to = \"outcome\"\n\nThis is how we indicate the name for the new column that provides information on the responses/observed values for each variable we’ve included in the reshaping syntax. If this is omitted, then pivot_longer() would use “value” as the name of the column.\n\n\nWe now have our data in a longer format. We can now use group_by() and summarize() to get the proportions we want to plot (or other types of statistics, such as the mean or median, if that was more relevant). However, we need to first recode our outcome variable into a 0/1 variable (0 = did not take the action, 1 = did take the action). We could have done this before reshaping the data, but that would have involved seven different commands so this is comparatively simpler.\n\ndescriptives &lt;- ess_long |&gt; \n  mutate(\n1    outcome_recode = case_when(\n      outcome == 1 ~ 1, \n      outcome == 2 ~ 0)) |&gt; \n2  group_by(measure) |&gt;\n  summarize(beh_prop = mean(outcome_recode, na.rm = T)) |&gt; \n3  ungroup()\n\n\n1\n\nI use case_when() here to make it a bit easier to handle observations with missing values on this variable. I create a new variable here because if I overwrite the original variable but make a mistake while doing so, then I need to re-run the pivoting command.\n\n2\n\nBasically tells R to separate (or group) the data by each unique value in the measure column and then do the next step to each grouping\n\n3\n\nThis tells R to undue the grouping. Not taking this step can lead to issues when we follow group_by() with mutate(). It isn’t a problem to not include this syntax after summarize() (see the discussion here), but I include it to be consistent.\n\n\n\n\nLet’s take a look:\n\ndescriptives\n\n# A tibble: 7 × 2\n  measure beh_prop\n  &lt;chr&gt;      &lt;dbl&gt;\n1 badge     0.0407\n2 bctprd    0.137 \n3 contplt   0.135 \n4 pbldmn    0.0298\n5 sgnptit   0.221 \n6 wrkorg    0.252 \n7 wrkprty   0.0347\n\n\nWe now have a data object with two variables: one indicating the behavior measure (measure) and one indicating the proportion of respondents with a value of 1 (i.e., performed this action) on the measure. We can next make a plot either for our own edification or for inclusion in our paper:\n\nggplot(descriptives, aes(x = measure, y = beh_prop)) + \n  geom_col() + \n  geom_text(aes( label = round(beh_prop, 2)), vjust = -0.4)\n\n\n\n\n\n\n\n\nThis is not too bad. However, if we were going to include this in our paper, then we should rename “badge”, etc., into something the consumers of our plot will understand. We might also want to reorder the data such that the bars progress largest to smallest or vice versa. For instance:\n\n\nShow the code\ndescriptives |&gt; \n1  mutate(measure = recode(measure,\n                          'badge' = 'Worn Badge', \n                          'bctprd' = 'Boycott Products', \n                          'contplt' = 'Contact Politician', \n                          'pbldmn' = 'Public Demonstration', \n                          'sgnptit' = 'Signed Petition', \n                          'wrkorg' = 'Worked in Organization', \n                          'wrkprty' = 'Worked in Party')) |&gt; \n2  ggplot(aes(x = reorder(measure, -beh_prop), y = beh_prop)) +\n  geom_col() + \n  geom_text(aes(label = round(beh_prop, 2)), vjust = -0.4) + \n3  scale_x_discrete(labels = scales::label_wrap(10)) +\n  labs(x = \"Participation Measure\", \n       y = \"Proportion\")\n\n\n\n1\n\nWe can give the different behaviors more informative names by recoding the contents of measure\n\n2\n\nreorder(measure, -beh_prop) handles the ordering. If we dropped the - sign, then the bars would ascend rather than descend.\n\n3\n\nThis makes sure the labels warp around so that they do not overlap. The number can be changed to give more/less space. Note: this requires that the scales package is installed.\n\n\n\n\n\n\n\n\n\n\n\nYou can see this discussion for a longer discussion of the different ways of reordering axes in a ggplot. You can learn more about how to add the automatic line break to value labels here.\nThe set up for this example suggested a hypothesis: people with a university degree participate more than those without a degree. We could also break down participation by education via this same basic process. We just need to add our education variable to the group_by() command.4 In the present case we will first want to create a binary variable for education (no degree vs. university degree) although one could imagine doing this for more than two categories as well.\n4 In practice, we might want to combine these different participation measures into a single scale. For instance, we could recode each measure into a 0/1 binary and then sum them up into a scale ranging from 0 (no actions taken) to 7 (all actions taken) and then use that as our DV.\ndescriptives_univ &lt;- ess_long |&gt; \n  mutate(\n    outcome_recode = case_when(\n      outcome == 1 ~ 1, \n      outcome == 2 ~ 0), \n1    univ = case_when(\n      eisced %in% c(1:5, 55) ~ \"No Degree\", \n      eisced %in% c(6:7) ~ \"Degree\")) |&gt; \n2  group_by(measure, univ) |&gt;\n  summarize(beh_prop = mean(outcome_recode, na.rm = T)) |&gt; \n  ungroup()\n\n\n1\n\neisced is my education variable. It takes on values between 1 (less than lower secondary education) to 7 (higher tertiary) with another category numbered 55 for “Other”. The c(6:7) type notation works here because all categories are whole numbers; if eisced could take on a value of 6.7 (for instance), then observations with this value would not get recoded when using c(6:7).\n\n2\n\nI am now telling R to separate (group) data by each combined value of measure and univ (e.g., measure = “badge” & univ = 0, measure = “badge” & univ = 1, …) and then do the next stuff.\n\n\n\n\n`summarise()` has grouped output by 'measure'. You can override using the\n`.groups` argument.\n\nhead(descriptives_univ, n = 10L)\n\n# A tibble: 10 × 3\n   measure univ      beh_prop\n   &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt;\n 1 badge   Degree      0.0502\n 2 badge   No Degree   0.0376\n 3 badge   &lt;NA&gt;        0     \n 4 bctprd  Degree      0.260 \n 5 bctprd  No Degree   0.0960\n 6 bctprd  &lt;NA&gt;        0     \n 7 contplt Degree      0.232 \n 8 contplt No Degree   0.103 \n 9 contplt &lt;NA&gt;        0     \n10 pbldmn  Degree      0.0591\n\n\nWe can now turn this into a plot although we should first filter our “NA” values.\n\n\nShow the code\ndescriptives_univ |&gt; \n1  filter(!is.na(univ)) |&gt;\n  mutate(measure = recode(measure,\n                          'badge' = 'Worn Badge', \n                          'bctprd' = 'Boycott Products', \n                          'contplt' = 'Contact Politician', \n                          'pbldmn' = 'Public Demonstration', \n                          'sgnptit' = 'Signed Petition', \n                          'wrkorg' = 'Worked in Organization', \n                          'wrkprty' = 'Worked in Party'), \n2         univ = factor(univ, levels = c(\"No Degree\", \"Degree\"))) |&gt;\n  ggplot(aes(x = univ, y = beh_prop)) + \n  geom_col() + \n  geom_text(aes(label = round(beh_prop, 2)), vjust = -0.4) + \n3  facet_wrap(~ measure) +\n  scale_y_continuous(limits = c(0, 0.5)) + \n  labs(x = \"University Degree?\", \n       y = \"Proportion\")\n\n\n\n1\n\nMakes sure we keep all observations where univ is not missing (! = not, is.na = is NA).\n\n2\n\nI convert univ to a factor and specify the levels with “No Degree” being the first one. This makes sure that “No Degree” (the “lower” value on education) is plotted before “Degree”.\n\n3\n\nCreates separate facets/graphs for each value of our measure variable.\n\n\n\n\n\n\n\n\n\n\n\nIt does look like those with higher levels of education are more like to take these particular political acts. The difference is fairly small when we look at behaviors that are uncommon, but much larger with behaviors that are a bit more likely to be observed in general (e.g., boycotting). Of course, this doesn’t mean that education is causing these differences. Indeed, whether education causes political participation is something of a recent debate. We would thus want to move to a more fully specified model to more fully justify our claims.\nThe foregoing can also be useful for making the same type of plot but for different variables. For instance, our ess dataset contains several measures of trust: trust in the country’s parliament (trstprl), trust in the legal system (trstlgl), trust in the police (trstplc), and trust in politicians (trstplt) on a scale ranging from 0 (no trust at all) to 10 (complete trust). . We could, for instance, provide a bar-graph showing the number (or even proportion) of respondents giving each response from 0-10. Instead of creating a plot for each variable separately and then using patchwork to combine them (as shown in Chapter 15), we could do this all at once by using tidy_longer(). For instance:\n\n\nShow the code\ness |&gt; \n  pivot_longer(\n1    cols = trstprl:trstplt,\n    names_to = \"target\", \n    values_to = \"trust\") |&gt; \n2  group_by(target, trust) |&gt;\n3  summarize(n = n()) |&gt;\n4  mutate(target = recode(target,\n                         'trstlgl' = 'Legal System', \n                         'trstplc' = 'Police', \n                         'trstplt' = 'Politicians', \n                         'trstprl' = 'Parliament')) |&gt;  \n  ggplot(aes(x = trust, y = n)) + \n  geom_col() + \n  facet_wrap(~ target) + \n5  scale_x_continuous(breaks = seq(from = 0, to = 10, by = 1)) +\n  labs( y = 'Count', x = 'Trust')\n\n\n\n1\n\nThe four variables all appear in a sequence, so I can do this instead of the c(\"var name\", \"var name\") procedure from above.\n\n2\n\nI’m going to plot the number of people in each category on each measure. So I combine both variables (the one listing the variable name, target, and the one listing the respondent’s response, trust, in group_by().\n\n3\n\nI could technically use a simpler command here: tally(). This would do the same thing as this line of syntax.\n\n4\n\nUsing recode to give the different trust variables nicer names for the plot.\n\n5\n\nThis makes sure there are breaks/ticks on the x-axis for each value between 0 and 10.\n\n\n\n\n\n\n\n\n\n\n\nHere, we can see that trust seems to be slightly higher when we look at the legal institutions (legal system and police) than for the more explicitly political institutions. This is a fairly common finding in the trust literature. We can see this by looking at the means for each variable as well using the describe() function from the psych package for simplicity:5\n5 We could, of course, have done this in the long dataset as well, e.g.: ess_long |&gt; group_by(target) |&gt; summarize(mean = mean(trust, na.rm=T) , although we would have needed to include some additional lines of syntax to get the standard deviation, etc.. Pivoting is not always the best solution!\ness |&gt; \n  select(trstprl:trstplt) |&gt; \n  psych::describe()\n\n        vars    n mean   sd median trimmed  mad min max range  skew kurtosis\ntrstprl    1 1824 5.24 2.06      6    5.43 1.48   0  10    10 -0.77     0.28\ntrstlgl    2 1830 6.02 2.02      6    6.22 1.48   0  10    10 -0.85     0.56\ntrstplc    3 1835 6.40 1.83      7    6.64 1.48   0  10    10 -1.25     1.83\ntrstplt    4 1829 5.05 1.96      5    5.25 1.48   0   9     9 -0.81     0.16\n          se\ntrstprl 0.05\ntrstlgl 0.05\ntrstplc 0.04\ntrstplt 0.05\n\n\nPivoting from wide to long can be a useful strategy for your analyses. We have used it above to summarize multiple variables and then produce nice looking figures to communicate our results. Pivoting could also be useful in other contexts as well. It may be a pre-requisite for some types of analyses (e.g., the analysis of repeated measures data such as with panel data). It could also be useful as a prelude to more advanced analyses such as performing the same regression model on lots and lots of different subsets of our data (e.g., the same model for each country in a dataset) although that use is beyond the scope of this document (see here for an example discussion).",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reshaping Datasets</span>"
    ]
  },
  {
    "objectID": "analysis_pivot.html#from-long-to-wide-pivot_wider",
    "href": "analysis_pivot.html#from-long-to-wide-pivot_wider",
    "title": "16  Reshaping Datasets",
    "section": "16.3 From Long to Wide: pivot_wider()",
    "text": "16.3 From Long to Wide: pivot_wider()\nWe can move from long to wide as well. This is perhaps less useful overall. The main use case I can think of here focuses on situations where you want to compare across different values within one of the dyad columns.\nLet’s start with a simple example involving our ess_long data from above. Let’s take a quick look at the data again to remind ourselves what it looks like:\n\nhead(ess_long, n = 10L)\n\n# A tibble: 10 × 9\n     idno eisced  agea trstprl trstlgl trstplc trstplt measure outcome\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1 600002      4    53       7       8       6       6 contplt       2\n 2 600002      4    53       7       8       6       6 wrkprty       2\n 3 600002      4    53       7       8       6       6 wrkorg        2\n 4 600002      4    53       7       8       6       6 badge         2\n 5 600002      4    53       7       8       6       6 sgnptit       2\n 6 600002      4    53       7       8       6       6 pbldmn        2\n 7 600002      4    53       7       8       6       6 bctprd        2\n 8 600003      5    41       4       5       5       2 contplt       1\n 9 600003      5    41       4       5       5       2 wrkprty       2\n10 600003      5    41       4       5       5       2 wrkorg        1\n\n\nWhy might we want to make this wider? Well, maybe we want to answer look at the cross-tabulation between contplt and wrkprty to get a sense of their inter-relationship: do people who work for political parties also tend to contact politicians?. That isn’t really easy to do with the data variables stacked on top of each other like this. Instead, we can use pivot_wider() to create a wider version of this dataset wherein the two variables are given their own columns that we can then pass into some other command. Here is how we can do that:\n\ness_wide &lt;- pivot_wider(ess_long, \n  id_cols = idno:trstplt, \n  names_from = measure, \n  values_from = outcome\n)\n\n\nid_cols = idno:trstplt\n\nThis is a very important row. id_cols tells the command which columns in the data are “id” columns - that is, which columns “uniquely identify each observation”. The idno:trstplt entry tells the command to treat all columns between those bookends (idno through trstplt) as unique identifiers. If I only put one of those variables here (e.g., id_cols = idno by itself), then the other columns would get dropped from the resulting dataset! That may or may not be an issue in a given example.\n\nnames_from = measure\n\nThis tells the command which column in our longer dataset contains the values that we want to use as the column names in our wider dataset. The measure variable contains that data in our example.\n\nvalues_from = outcome\n\nLikewise, we need to tell the command which column contains the values that should be used in these newly created columns. That column is named outcome in this example.\n\n\nLet’s take a look so you can see what this looks like:\n\nhead(ess_wide, n = 10L)\n\n# A tibble: 10 × 14\n     idno eisced  agea trstprl trstlgl trstplc trstplt contplt wrkprty wrkorg\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 600002      4    53       7       8       6       6       2       2      2\n 2 600003      5    41       4       5       5       2       1       2      1\n 3 600005      7    42       7       8       8       7       2       2      1\n 4 600011      1    87       3       4       8       3       2       2      2\n 5 600013      1    78       3       9       8       6       2       2      2\n 6 600014      2    48       5       3       0       3       2       2      2\n 7 600015      2    61       4       4       3       3       2       2      2\n 8 600017      2    21       8       6       7       7       2       2      2\n 9 600021      4    67       6       7       7       6       2       1      1\n10 600024      1    57       0       7       5       0       1       2      2\n# ℹ 4 more variables: badge &lt;dbl&gt;, sgnptit &lt;dbl&gt;, pbldmn &lt;dbl&gt;, bctprd &lt;dbl&gt;\n\n\nWe one again have a dataset wherein each row focuses on a unique observation (completely different survey respondent) complete with columns for each of our participation measures. We could then perform a simple cross-tabulation if we wanted:\n\n1with(ess_wide, table(contplt, wrkprty))\n\n\n1\n\nI am using with() here to avoid having to use the $ notation twice (e.g., instead of writing table(ess_wide$contplt, ess_wide$wrkprty).\n\n\n\n\n       wrkprty\ncontplt    1    2\n      1   33  215\n      2   31 1563\n\n\nRecall that 1 = “did the action” and 2 = “did not”. Most people took neither action. More people reported contacting a politician but not working for a party (n = 215) than doing both actions (n = 33) or working for a party but not contacting a politician (n= 31).\nLet’s look at another (slightly more complex example) using the vdem data. Here are the variables in our dataset:\n\nsjPlot::view_df(vdem)\n\n\n\nData frame: vdem\n\n\nID\nName\nLabel\nValues\nValue Labels\n\n\n1\ncountry_name\n\n\n&lt;output omitted&gt;\n\n\n2\nyear\n\nrange: 1789-2023\n\n\n3\nv2x_polybackground-color:#eeeeeehy\n\nrange: 0.0-0.9\n\n\n4\nv2x_egaldem\n\nrange: 0.0-0.9\n\n\n5\nv2x_corr\n\nrange: 0.0-1.0\n\n\n\n\n\n\nOur data focuses on a country’s level of electoral democracy (v2x_polyarchy), egalitarian democracy (v2x_egaldem) and level of corruption (v2x_corr). We have data over time for each measure - from 1789 to 2023!6\n6 Although, in practice, we have missing data for many years prior to the 20th century.One thing we might wonder is this: how stable are democracy scores? One way we could get at this is to consider the correlation between years, e.g., how strongly correlated are democracy scores in 1980 and 2000 or in 2000 and 2020? The more strongly correlated the scores, the more stable is democracy. Again, though, our data is “stacked” such that the democracy score data (either via v2x_polyarchy or v2x_egaldem) is all in one column. We thus need to widen our data such that there is a column for our democracy measure (or measures) for each separate year that we can then feed into a correlation.\nHere is how I will do this:\n\nvdem_wider &lt;- vdem |&gt; \n  filter(year %in% c(1980, 2000, 2020)) |&gt; \n  select(country_name, year, v2x_polyarchy, v2x_egaldem) |&gt; \n  pivot_wider(\n    id_cols = \"country_name\", \n    names_from = \"year\",\n    values_from = c(\"v2x_polyarchy\", \"v2x_egaldem\"))\n\n\nfilter(year %in% c(1980, 2000, 2020)\n\nI am filtering my original data such that only data from these three years is kept. If I skipped this step then my wide dataset would end up with a column for each year from 1793 to 2023! That is overkill in the present instance. This initial step may not be needed in other examples.\n\nselect(…)\n\nI am also getting rid of the corruption measure (while keeping the variables included in the select() command). I could have widen the data to include separate columns for it as well, but that is not really needed in this example. Getting rid of unnecessary columns also makes the id_cols part simpler.\n\nid_cols = \"country_name\"\n\nThis specifies the unique identifier variable. Here, the variable that uniquely identifies observations across time is country_name, which lists the, uh, country name for each observation.\n\nnames_from = \"year\"\n\nThis indicates that I want the values from the year column to be included in the names of the columns that this command creates.\n\nvalues_from = c(\"v2x_polyarchy\", \"v2x_egaldem\"))\n\nThis tells the command which columns in our long dataset contain the values we want to port over into the columns we’re creating.\n\n\nLet’s take a look:\n\nhead(vdem_wider, n = 10L)\n\n# A tibble: 10 × 7\n   country_name  v2x_polyarchy_1980 v2x_polyarchy_2000 v2x_polyarchy_2020\n   &lt;chr&gt;                      &lt;dbl&gt;              &lt;dbl&gt;              &lt;dbl&gt;\n 1 Mexico                     0.3                0.671              0.652\n 2 Suriname                   0.218              0.783              0.754\n 3 Sweden                     0.91               0.914              0.901\n 4 Switzerland                0.87               0.888              0.901\n 5 Ghana                      0.543              0.667              0.722\n 6 South Africa               0.16               0.745              0.714\n 7 Japan                      0.841              0.84               0.835\n 8 Burma/Myanmar              0.137              0.095              0.422\n 9 Russia                     0.107              0.405              0.258\n10 Albania                    0.174              0.407              0.536\n# ℹ 3 more variables: v2x_egaldem_1980 &lt;dbl&gt;, v2x_egaldem_2000 &lt;dbl&gt;,\n#   v2x_egaldem_2020 &lt;dbl&gt;\n\n\nThe first column is our ID column. We can now see that each country features once! We then get columns with our democracy data. The command has “glued” together the names from values_from with the values from year (e.g., v2x_polyarchy_1980 tells us the polyarchy scores for each country in the year 1980, while v2x_polyarchy_2000 gives us the year 2000 data and so on). The naming of variables in a pivot_wider() command can get a little bit complicated in more complicated examples and there are additional options useful on this front; see the reference page for the command.\nLet’s look at some correlations. I will use the correlation() command from the aptly named correlation package to do this because it makes it easy to calculate multiple correlations at once.\n\nlibrary(correlation)\n\nvdem_wider |&gt; \n1  select(contains(\"v2x_polyarchy\")) |&gt;\n  correlation()\n\n\n1\n\nThis is one of a variety of helpful shortcuts for selecting variables based on the contents of variable names, including ends_with() and starts_with() See this7se, pretty stable is not perfectly stable.\n\n7 See here for where I found the syntax needed for the 45 degree line.\n\n\n# Correlation Matrix (pearson-method)\n\nParameter1         |         Parameter2 |    r |       95% CI |     t |  df |         p\n---------------------------------------------------------------------------------------\nv2x_polyarchy_1980 | v2x_polyarchy_2000 | 0.70 | [0.61, 0.77] | 12.23 | 153 | &lt; .001***\nv2x_polyarchy_1980 | v2x_polyarchy_2020 | 0.68 | [0.58, 0.75] | 11.34 | 153 | &lt; .001***\nv2x_polyarchy_2000 | v2x_polyarchy_2020 | 0.83 | [0.78, 0.87] | 19.64 | 175 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 155-177\n\n\nWe could perhaps plot this data to get a better sense of matters. I will do so for the 1980 and 2020 data. I will also add a 45 degree diagonal line; points above this line will indicate countries with higher levels of democracy in 2020 than 1980, while points below the line indicate a decrease in electoral democracy over time.\n\nggplot(vdem_wider, aes(x = v2x_polyarchy_1980, y = v2x_polyarchy_2020)) + \n  geom_point(alpha = 0.2) + \n  geom_smooth(method = \"lm\") + \n1  geom_line(data = data.frame(x = c(-Inf, Inf), y = c(-Inf, Inf)), aes(x, y)) +\n2  scale_y_continuous(limits = c(0,1)) +\n  scale_x_continuous(limits = c(0,1)) + \n  labs(x = \"Electoral Democracy (1980)\", \n       y = \"Electoral Democracy (2020)\")\n\n\n1\n\nAdds the 45 degree line.\n\n2\n\nThe democracy variables have a theoretical range of 0 to 1. This extends the y- axis to cover that full range.\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe electoral democracy scores are quite stable per the correlation coefficient. Where there are deviations it tends to show up in countries with a higher democracy score in 2020 than 1980. This makes sense as the 1980s/1990s is commonly referred to as the ‘third wave of democratization’ We can also see that countries with comparatively high levels of democracy in 1980 generally have quite high levels in 2020 as well.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reshaping Datasets</span>"
    ]
  },
  {
    "objectID": "analysis_robust.html",
    "href": "analysis_robust.html",
    "title": "17  Heteroskedasticity and Robust Standard Errors",
    "section": "",
    "text": "17.1 Our Model\nWe need a model. I’ll use the same model as in the Statistics II example (but see my note below). Here, we regress a measure of political violence (pve) on a measure of a country’s level of democracy and a squared version of that democracy score to capture potential non-linearities in the relationship between democracy and violence. Here are the descriptive statistics for our DV and IV and then the model.\n#Descriptive Statistics\ndemdata |&gt; \n  select(pve, v2x_polyarchy) |&gt; \n  summary()\n\n      pve          v2x_polyarchy   \n Min.   :-2.7081   Min.   :0.0150  \n 1st Qu.:-0.7238   1st Qu.:0.2860  \n Median :-0.1978   Median :0.5290  \n Mean   :-0.2016   Mean   :0.5137  \n 3rd Qu.: 0.5937   3rd Qu.:0.7545  \n Max.   : 1.4920   Max.   :0.9080  \n NA's   :15                        \n\n#Run the model\nviolence &lt;- lm(pve ~ v2x_polyarchy + I(v2x_polyarchy^2), \n               data=demdata)\nNote: I include the squared term in a different manner here than in the example in the Statistics II R materials. In those materials we first created a new variable in our data via mutate() (e.g., mutate(v2x_polyarchy_sq = v2x_polyarchy^2). The use of I() will end up doing the same thing for me but without my having to manually create the squared term. In addition, the use of I() will make it a bit easier to get a nice plot of predicted values below using the marignaleffects package (this is a bit annoying to do with the manually created squared term).\nRegardless, let’s look at our results:\n#Summarize it\ntidy(violence, conf.int = TRUE)\n\n# A tibble: 3 × 7\n  term               estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)          -0.589     0.243     -2.42 0.0165      -1.07    -0.109\n2 v2x_polyarchy        -1.73      1.10      -1.58 0.117       -3.90     0.438\n3 I(v2x_polyarchy^2)    3.83      1.05       3.64 0.000361     1.75     5.90\nPer above, pve ranges from -2.71 to +1.49. Higher values on this measure indicate less violence in a country. Higher values on our v2x_polyarchy measure indicate greater levels of electoral democracy. The coefficients here imply…\nIn other words, there is a curvillinear relationship between the two variables.\nIt might be helpful to visualize this relationship to understand the coefficients. I will use a command from the marginaleffects package called plot_predictions(): this will create a plot of predicted values for us without us having to specify specific values of the predictor variable. This is particularly useful in instances where there is a squared term since increases in v2x_polyarchy necessarily imply increases in the squared term (e.g., it’s not really possible to look at an increase in v2x_polyarchy while holding constant its square).1\nplot_predictions(violence, condition = \"v2x_polyarchy\")\nWe can see a small decrease in pve as we move from v2x_polyarchy = 0 to v2x_polyarchy = 0.2 or so but then an increase in pve as the polyarchy variable continues to increase in value. We could use the slopes() function from marginal effects to directly estimate the slope for the line at different values of v2x_polyarchy:\nslopes(violence, newdata = datagrid(v2x_polyarchy = c(0, 0.10, 0.25, 0.5, 0.75)))\n\n\n          Term v2x_polyarchy Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 %\n v2x_polyarchy          0.00   -1.731      1.098 -1.58    0.115  3.1 -3.88\n v2x_polyarchy          0.10   -0.965      0.893 -1.08    0.280  1.8 -2.72\n v2x_polyarchy          0.25    0.184      0.594  0.31    0.757  0.4 -0.98\n v2x_polyarchy          0.50    2.098      0.226  9.28   &lt;0.001 65.7  1.66\n v2x_polyarchy          0.75    4.013      0.549  7.31   &lt;0.001 41.7  2.94\n 97.5 %\n  0.421\n  0.786\n  1.347\n  2.541\n  5.089\n\nColumns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, v2x_polyarchy, predicted_lo, predicted_hi, predicted, pve \nType:  response\nThe slope of a linear regression line is normally the same regardless of what value X takes on. However, the inclusion of the squared term relaxes this assumption. The slope is negative at at very low levels of polyarchy and then starts to peter out before becoming increasingly positive.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Heteroskedasticity and Robust Standard Errors</span>"
    ]
  },
  {
    "objectID": "analysis_robust.html#our-model",
    "href": "analysis_robust.html#our-model",
    "title": "17  Heteroskedasticity and Robust Standard Errors",
    "section": "",
    "text": "Intercept: We should expect a pve score less than 0 (-0.589) when v2x_polyarchy (and its squared term) = 0\nCoefficient for v2x_polyarchy: pve will decrease in value (i.e., violence will increase given how this variable is scaled) as v2x_polyarchy increases, but…\nCoefficient for squared term: pve values will then start to increase (i.e., violence will decrease) as we move further along the polyarchy scale.\n\n\n\n1 We could emulate this figure using a command in marginaleffects called avg_predictions() . For instance: avg_predictions(violence, variables = \"v2x_polyarchy\") would produce predicted variables across the range of v2x_polyarchy (at its minimum, 1st quarter, median, 3rd quarter, and maximum) along with confidence intervals which we could then plot. The bands for the confidence interval, however, would look a bit more jagged than what is produced by plot_predictions() .",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Heteroskedasticity and Robust Standard Errors</span>"
    ]
  },
  {
    "objectID": "analysis_robust.html#checking-the-assumption-and-finding-a-problem",
    "href": "analysis_robust.html#checking-the-assumption-and-finding-a-problem",
    "title": "17  Heteroskedasticity and Robust Standard Errors",
    "section": "17.2 Checking the Assumption and Finding a Problem",
    "text": "17.2 Checking the Assumption and Finding a Problem\nOkay, let’s check our homoskedasticity assumption. Recall that we can do this by looking at a plot of the model’s residuals against its predictions. Ideally we will see a random cloud of dots without any obvious pattern.2 Heteroskedasticity, on the other hand, would commonly show up as some type of funnel shape as in the top left figure here:\n2 If our DV was discrete (e.g., took on values 1 through 7), then we’d get a row of parallel dots.\nWe can use the resid_panel() command from the ggResidpanel library to do this, as so:\n\nresid_panel(violence, plots = c(\"resid\"))\n\n\n\n\n\n\n\n\nThis looks problematic. There is a quite wide array of dots around -0.25 or so on the x-axis that then narrows when get towards 1 on the x-axis. This is a pretty noticeable pattern, so I’d feel comfortable saying that the assumption is being violated.\nWe can use some other tools to help us check this assumption as well, although these were not taught in Statistics II. There are formal statistical tests for assessing heteroskedasticity (not taught in Statistics II). In particular, we can perform what is known as a Breusch-Pagan test. The null hypothesis for this test is homoskedasticity, while the alternative hypothesis is heteroskedasticity. We can perform this test using the bptest() command from the lmtest library.\n\nlibrary(lmtest)\nbptest(violence)\n\n\n    studentized Breusch-Pagan test\n\ndata:  violence\nBP = 14.584, df = 2, p-value = 0.0006811\n\n\nThe p-value here is well below 0.05, which is our standard metric for assessing statistical significance. We can thus reject the null hypothesis of homoskedasticity.\nOne important note and caution about this test: it is sensitive to sample size much like any statistical significance test. In practice, this means that it becomes easier and easier to reject this null hypothesis with with more and more data in our model. However, our residuals are unlikely to be perfectly homoskedastic in actual data. We might thus find ourselves in situations where we can reject the null hypothesis of homoskedasticity but where there really isn’t a serious problem that we need to address. This is why we recommend focusing on the plot of residual values to see if there is a really noticeable trend there. The BP test can then be used to check one’s instincts.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Heteroskedasticity and Robust Standard Errors</span>"
    ]
  },
  {
    "objectID": "analysis_robust.html#what-can-we-do",
    "href": "analysis_robust.html#what-can-we-do",
    "title": "17  Heteroskedasticity and Robust Standard Errors",
    "section": "17.3 What Can We Do?",
    "text": "17.3 What Can We Do?\nWe have found heteroskedasticity. That means we should be a bit careful with our claims of statistical significance. What can we do to address this issue?\nHeteroskedaticity implies that our model is making better predictions for some observations than others. Ideally, then, we should try and improve our model to address this imbalance. That could involve adding predictors or perhaps transforming our variables in some fashion.\nIt is not always clear how exactly we should improve our model, however. One thing we could do in such a situation is to use what are known as “heteroskedastic-robust” or “heteroskedastic-consistent” standard errors. These are standard errors that are calculated in a different way and which are more ‘robust’ to the violation of this assumption. Their use does not “fix” our problem - if heteroskedasticity is present due to a variable that is missing from our model, then using these SEs does not magically make our model better (or avoid the bias that the exclusion of the confounder introduces into our coefficient estimates). However, we could use this tool to probe whether our conclusions are robust to a more conservative test.\nThe easiest way to include heteroskedastic-robust standard errors in your results is via the modelsummary() command that you will use when reporting your regression results. See this section on the marginaleffects website. We can toggle the use of these SEs via the vcov option. There are various types of robust SEs, but we’ll use the “HC3” option as this is a pretty standard one. (Note: you might have to also install the sandwich library as modelsummary uses it to calculate the standard errors.)\nHere is an example of how to do that:\n\nmodelsummary(violence, \n             gof_map = c(\"nobs\", \"r.squared\", \"adj.r.squared\"), \n             stars = T,\n1             vcov = \"HC3\",\n             coef_rename = c(\n               '(Intercept)' = 'Intercept', \n               'v2x_polyarchy' = 'Polyarchy Score', \n               'v2x_polyarchy_sq' = 'Polyarchy Squared'), \n             notes = list(\"OLS Coefficients with Heteroskedastic-Robust SEs in Parentheses\"))\n\n\n1\n\nChanges the way the standard errors are calculated.\n\n\n\n\n \n\n  \n    \n    \n    tinytable_415cnnq1j7qmhgcvyd61\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\nOLS Coefficients with Heteroskedastic-Robust SEs in Parentheses\n        \n                \n                  Intercept         \n                  -0.589+ \n                \n                \n                                    \n                  (0.303) \n                \n                \n                  Polyarchy Score   \n                  -1.731  \n                \n                \n                                    \n                  (1.249) \n                \n                \n                  I(v2x_polyarchy^2)\n                  3.829***\n                \n                \n                                    \n                  (1.107) \n                \n                \n                  Num.Obs.          \n                  164     \n                \n                \n                  R2                \n                  0.401   \n                \n                \n                  R2 Adj.           \n                  0.394   \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nThe main difference from other examples you’ve seen is vcov = \"HC3\". That is what will update the standard errors.\nLet’s take a step back and compare results. Here is syntax that produces a regression table wherein Model 1 uses the standard SEs reported in lm() (the “classical” SEs) and Model 2 reports the robust version:\n\nmodelsummary(violence, \n             gof_map = c(\"nobs\", \"r.squared\", \"adj.r.squared\"),\n             stars = T, \n             vcov = c(\"classical\", \"HC3\"), \n             coef_rename = c(\n               '(Intercept)' = 'Intercept', \n               'v2x_polyarchy' = 'Polyarchy Score', \n               'v2x_polyarchy_sq' = 'Polyarchy Squared'), \n             notes = list(\"OLS Coefficients with Classic (Model 1) or Heteroskedastic-Robust (Model 2) SEs in Parentheses\"))\n\n \n\n  \n    \n    \n    tinytable_acb7j2ralw33mtttppx6\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\nOLS Coefficients with Classic (Model 1) or Heteroskedastic-Robust (Model 2) SEs in Parentheses\n        \n                \n                  Intercept         \n                  -0.589* \n                  -0.589+ \n                \n                \n                                    \n                  (0.243) \n                  (0.303) \n                \n                \n                  Polyarchy Score   \n                  -1.731  \n                  -1.731  \n                \n                \n                                    \n                  (1.098) \n                  (1.249) \n                \n                \n                  I(v2x_polyarchy^2)\n                  3.829***\n                  3.829***\n                \n                \n                                    \n                  (1.051) \n                  (1.107) \n                \n                \n                  Num.Obs.          \n                  164     \n                  164     \n                \n                \n                  R2                \n                  0.401   \n                  0.401   \n                \n                \n                  R2 Adj.           \n                  0.394   \n                  0.394   \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nWe can note a few things here:\n\nThe coefficients have not changed. Altering the way the SE is calculated will not effect the coefficient estimate.\nThe HC3 SEs are slightly larger. This is typically what happens since heteroskedasticity introduces a downward bias (too small) on the standard errors.\nOur overall conclusions are unchanged - while the SEs increase in value, the increase is pretty small and does not affect any of the statistical significance tests we care about.\n\nWe can also incorporate these types of SEs into our plots of predicted values, etc., by using the vcov option in the requisite commands. For instance:\n\n#Plot of predicted values\nplot_predictions(violence, vcov = \"HC3\", condition = \"v2x_polyarchy\")\n\n\n\n\n\n\n\n#Slopes at various levels of democracy\nslopes(violence, newdata = datagrid(v2x_polyarchy = c(0, 0.10, 0.25, 0.5, 0.75)), vcov = \"HC3\")\n\n\n          Term v2x_polyarchy Estimate Std. Error      z Pr(&gt;|z|)    S 2.5 %\n v2x_polyarchy          0.00   -1.731      1.249 -1.386    0.166  2.6 -4.18\n v2x_polyarchy          0.10   -0.965      1.031 -0.936    0.349  1.5 -2.99\n v2x_polyarchy          0.25    0.184      0.708  0.259    0.795  0.3 -1.20\n v2x_polyarchy          0.50    2.098      0.239  8.778   &lt;0.001 59.1  1.63\n v2x_polyarchy          0.75    4.013      0.475  8.449   &lt;0.001 54.9  3.08\n 97.5 %\n  0.717\n  1.055\n  1.572\n  2.567\n  4.943\n\nColumns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, v2x_polyarchy, predicted_lo, predicted_hi, predicted, pve \nType:  response \n\n\nAgain, the only thing that will really change are the standard errors and, by extension, the confidence intervals.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Heteroskedasticity and Robust Standard Errors</span>"
    ]
  },
  {
    "objectID": "analysis_clustering.html",
    "href": "analysis_clustering.html",
    "title": "18  Geographically Clustered/Nested Data",
    "section": "",
    "text": "18.1 An Example of the Problem\nSuppose that I was writing my thesis on the topic of regime satisfaction: when do people vary in how satisfied they are with the way that the political regime works in their country? There are lots of datasets with measures related to regime satisfaction, but perhaps I think that the World Values Survey is the most appropriate for my paper because it alone has a measure of my main independent variable.\nThe data I’ll work with is a subset of their Wave 7 data wherein individual respondents were sampled within 66 countries or territories across the world. Respondents were asked the following question: “On a scale from1 to 10 where ‘1’ is ‘not satisfied at all’ and ‘10’ is ‘completely satisfied’, how satisfied are you with how the political system is functioning in your country these days?\nShow the code\n# Load our data\nwvs &lt;- import(\"./data/wvs_small.rda\")\n\n#Some cleaning \nwvs &lt;- wvs |&gt; \n  mutate(\n1    country_name = sjlabelled::as_label(B_COUNTRY))\n  \n#A plot\nwvs |&gt; \n  filter(!is.na(Q252)) |&gt; \n  group_by(Q252) |&gt; \n  tally() |&gt; \n  ggplot(aes(x = Q252, y = n)) + \n  geom_col() + \n  labs(x = \"Regime Satisfaction\\n1 = Not Satisfied at All, 10 = Completely Satisfied\", \n       y = \"Count\") +\n  scale_x_continuous(breaks = c(1:10)) + \n  theme_minimal()\n\n\n\n1\n\nThe B_COUNTRY variable indicates which country the respondent is in. It has numeric values with associated labels (e.g, 20 = a person is from Andorra). This nifty command from the sjlabelled package enables me to create a new character variable with those country labels as the data. This is much nicer for subsequent plotting, etc.\n\n\n\n\n\n\n\n\n\n\nFigure 18.1: Distribution of Individual-Level Responses\nFigure 18.1 summarizes the individual level responses to this question pooling across countries. Each bar provides the number of respondents in the survey who gave each response. We can see that there is variation here: there are people who are “completely satisfied” with the way democracy works in their country, people in the middle, and people who are “not satisfied at all”.2 We might think of modeling this variation by considering attributes of the respondents that vary between them. For instance, we could consider whether people with higher levels of education tend to give higher responses on this question than those with less education. Or, we could ask whether there are differences between people who identify with a political party and those that do not, or by ideological extremity, or by political news consumption, and so on, and so on. The unit of analysis would be the individual and the differences explained by things that vary between individuals.\nThe respondents on this survey are not atoms floating freely in space. Rather, the people conducting the survey used some type of sampling procedure to select some people to interview rather than others within particular countries with these countries being a sample of some sort from the broader population of countries in the world.3 We might intuitively think that countries also vary on this measure, i.e., the average level of democratic satisfaction may be higher in some countries than others. Let’s take a look:\nShow the code\n#Country Averaegs\ncountry_avgs &lt;- wvs |&gt; \n  group_by(country_name) |&gt; \n  summarize(dem_satis = mean(Q252, na.rm = T))\n\n#Plot\nggplot(country_avgs, aes(x = dem_satis)) + \n  geom_histogram(fill = 'white', color = 'black') + \n  labs(y = \"Mean Democratic Satisfaction within Country\", \n       x = \"Regime Satisfaction\") + \n  scale_x_continuous(limits = c(1, 10), \n                     breaks = seq(from = 1, to = 10, by = 1)) + \n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 18.2: Distribution of Country Averages\nFigure 18.2 was constructed by first calculating the average score on the regime satisfaction measure in each country and then plotting the distribution via a histogram. We can see that there is also variation at this level of analysis as well: there is one country with a very low average level of satisfaction (around 2.5), others with rather high (around 8 or so), and countries in the middle.4 We can probably come up with some plausible suspects for things that would predict (and maybe even cause) this variation. Perhaps regime satisfaction is higher when countries are experiencing periods of economic growth, or among countries with lower levels of corruption, or in countries with more accountable governments, etc. A key point here: the source of this variation is something operating at the country-level, i.e., the predictors are things that vary between countries.\nWe can get a final look at this data via Figure 18.3, which plots the number of respondents within each country given each response broken down by country.\nShow the code\nwvs |&gt; \n  filter(!is.na(Q252)) |&gt; \n  group_by(country_name, Q252) |&gt; \n  tally() |&gt; \n  ggplot(aes(x = Q252, y = n)) + \n  geom_col() + \n  facet_wrap(~ country_name) + \n  labs(x = \"Regime Satisfaction\\n1 = Not Satisfied at All, 10 = Completely Satisfied\", \n       y = \"Count\") +\n  scale_x_continuous(breaks = c(1,3,5,7,9)) + \n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 18.3: Distribution of Individual Responses by Country\nHere we can perhaps better see that variation is occurring at two levels. Individuals within a country vary in regime satisfaction - some are the high end, some at the low, and others in the middle. Meanwhile, the distributions are not the same across countries - Brazil has many people at 1, China has many people at the high end, and so on.\nThe average level of regime satisfaction varies across countries. Per above, we can think of country-level attributes that might explain this variation (inequality, economic growth, corruption, etc.). Note that these are things that are a constant for people within a given country - everybody in Andorra is living under the same economic conditions, etc.5 This is where the problem emerges. If we pool all of the individual-level data together into a single model (i.e., include all respondents from every country together in one single model), then the errors for people from a given country will likely be correlated with one another because the responses of these individuals will be influenced by a common causal influence that is not accounted for our in statistical model. We will have a violation of our independence of errors assumption, our errors will be biased downwards, and we may end up reject a “null” hypothesis that we shouldn’t.6\nHow, then, can we deal with this type of data?",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Geographically Clustered/Nested Data</span>"
    ]
  },
  {
    "objectID": "analysis_clustering.html#an-example-of-the-problem",
    "href": "analysis_clustering.html#an-example-of-the-problem",
    "title": "18  Geographically Clustered/Nested Data",
    "section": "",
    "text": "2 Mean = 5.32 , SD = 2.76 .3 Some type of probability-based sample is used when selecting individual respondents in each country (see here). At the same time, I do think that the WVS uses a probability-based sampling procedure for selecting the countries involved.\n\n\n4 The country at the lowest end is Brazil with an average of 2.59. The countries at the top are Tajikistan (8.09), Vietnam (7.61) ,and China (7.55).\n\n\n\n5 Well, in some “objective” sense. People may, and very often do, vary in their subjective experience of these variables. People differ, for instance, on whether they think the economy is “good” or “bad” even though the economy is technically the country’s economy is technically the same for everyone. This could reflect differences in information about economic performance as well as motivations to perceive the world in attitude-consistent ways (e.g., government supporters could selectively downplay negative information and thereby come to a rosier view of the economy than non-supporters). For relevant reviews on perceptions of regime performance, and potential “biases” in them, see Anderson (2007) and Huber and Malhotra (2013).6 “A common cause” does not necessarily mean that everyone is reacting in the same way to the country-level stimulus. Rich and poor people could conceivably react very differently to the same level of inequality, for instance. Solt (2008) argues as such in this article on political engagement. People might also vary in how aware they are of a particular stimulus (e.g., the unemployment rate). This is something that could be investigated empirically via the final solution discussed below.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Geographically Clustered/Nested Data</span>"
    ]
  },
  {
    "objectID": "analysis_clustering.html#potential-solutions",
    "href": "analysis_clustering.html#potential-solutions",
    "title": "18  Geographically Clustered/Nested Data",
    "section": "18.2 Potential Solutions",
    "text": "18.2 Potential Solutions\nThe following sub-sections describe different ways of approaching geographically clustered/nested data. Perhaps the key difference between them is this: at what level are your independent variables being measured? In other words: what is the appropriate level of analysis for your hypothesis? The answer to this question will help inform which path is best suited for your project.\n\n18.2.1 Focus on a Single “Cluster”\nThe problem above emerges when we pool observations from clusters together with one another. If your main IV is at the individual level, then a simple solution would be to simply focus on respondents from a single country by filtering out observations from other countries.\nFor instance, let’s say we hypothesize that regime satisfaction will be positively associated with a person’s financial situation. The WVS asks respondents “how satisfied are you with the financial situation of your household” on a 1 (completely dissatisfied) to 10 (completely satisfied) response scale (variable Q50). Perhaps we are interested in the case of New Zealand in particular. We would first filter out observations from other countries and regress our DV on the IV (along with plausible confounding variables; see Chapter 11). For instance:\n\n# Filter\nnz &lt;- wvs |&gt; \n  filter(country_name == \"New Zealand\")\n\n#run our model\nmodel_nz &lt;- lm(Q252 ~ Q50, data = nz)\n\n#summary of coefficients\ntidy(model_nz)\n\n#predicted values via avg_predictions()\n1avg_predictions(model_nz, variables = \"Q50\")\n\n\n1\n\nYou learned about predictions() in Statistics II. avg_predictions() does something very similar - it calculates the average predicted value of Y given values of X. The main difference between the two commands concerns how control variables are handled: predictions() holds them at their mean/mode, while avg_predictions() makes predictions for each observations based on that observation’s unique values on the controls and then averages everything up after doing so. If there is only one predictor variable than the two commands converge on the same predicted values with avg_predictions() being a bit simpler to use with continuous variables (it will automatically calculate predictions at the min, 1st quartile, median, 3rd quartile, and maximum of the IV).\n\n\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    4.61     0.234      19.7  2.40e-73\n2 Q50            0.127    0.0317      4.01 6.53e- 5\n\n Q50 Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n   1     4.74     0.2035 23.3   &lt;0.001 395.9  4.34   5.14\n   6     5.37     0.0766 70.2   &lt;0.001   Inf  5.22   5.52\n   8     5.63     0.0756 74.5   &lt;0.001   Inf  5.48   5.78\n   9     5.76     0.0931 61.9   &lt;0.001   Inf  5.57   5.94\n  10     5.88     0.1167 50.4   &lt;0.001   Inf  5.65   6.11\n\nColumns: Q50, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nWe can see that there is a positive and statistically significant relationship here: regime satisfaction increases by 0.13 scale points, on average, with each one unit change on our financial situation variable. The overall degree of change when moving from minimum to maximum is fairly large: around 1.14 scale points. The SD for the DV in the New Zealand sub-sample is 2.21. We can see that there is a positive and statistically significant relationship here: regime satisfaction increases by 0.13 scale points, on average, with each one unit change on our financial situation variable. The overall degree of change when moving from minimum to maximum is fairly large: around 1.14 scale points. The SD for the DV in the New Zealand sub-sample is 2.21, so a min to max move is equal to a change of about half of a standard deviation. This seems non-trivial in scale, although note that we’d expect even those with very negative views of their financial situation, as well as those with very positive to be around the mid-point on regime satisfaction (predicted value of 4.74 when satisfaction = 1, 5.88 when it = 10). A politician who could get everybody feeling very positive about their financial situation would be better off than one who couldn’t, but they we’re probably not talking about “put the politician on money” levels of satisfaction.7\n7 Maybe this is unfair: the measure is about regime satisfaction, not leaderThis approach is very acceptable for your thesis. Its key advantage is analytic simplicity - just filter the data and fit the model you want to fit. Here, you’re just using the WVS, ESS, or whatever clustered data source as a convenient source of data for the country you care about. You would want to motivate the use of that particular source (rather than some other country-specific survey) by indicating why its measurements are particularly suitable for your project.\nOf course, there are potential ‘pull’ considerations, i.e., reasons to think about alternatives.\nFirst, this would make sense if the predictor variable you care about is something that varies at the individual level. If the IV you care about is something that varies between clusters/countries (e.g., inequality), then this does not work (unless, perhaps, you can find some type of subjective indicator - e.g., do people’s perception of inequality matter?).\nSecond, you do have to pick, and motivate the use of, a particular country to focus on and discuss how that decision influences what we learn about your question. In the former case, this could very well be part of the set up of your paper. For instance, if you begin your paper with an anecdote involving former German PM Angela Merkel, then it would seem obvious and non-controversial to use data focused on Germany in particular. You would still want to reflect in the discussion on whether the patterns you saw in the data might “travel” to other contexts (other times within Germany, other places besides) - if so, why do you think that? ; if not, in what ways would things be different and why? But, those are discussions you would likely have to have anyways. At the same time, there is some value in having a much more varied set of countries in the analyses as this would speak to such a concern to a certain degree at least in terms of geography.\n\n\n18.2.2 Fixed Effects + Clustered Standard Errors\nHere, your main IV varies at the individual level and you want to use all the data before you. The simplest solution is to use “country fixed effects” and “clustered standard errors”.\n“Fixed effects” is just a fancy sounding term for: “Include a factor variable for cluster in your model”. The cluster in this example is country (respondents in countries). So, step one would be to convert the variable in the dataset with data on what cluster each observation resides within into a factor variable and include it in the model. I already have this variable factorized (see earlier syntax), so here are the model results:\n\n#run our model\nmodel_all_indiv &lt;- lm(Q252 ~ Q50 + country_name, data = wvs)\n\n#summary of coefficients\ntidy(model_all_indiv)\n\n# A tibble: 48 × 5\n   term                   estimate std.error statistic   p.value\n   &lt;chr&gt;                     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)              4.59     0.0821     55.9   0        \n 2 Q50                      0.182    0.00410    44.3   0        \n 3 country_nameArgentina   -0.265    0.110      -2.41  1.60e-  2\n 4 country_nameAustralia   -0.0539   0.0970     -0.556 5.79e-  1\n 5 country_nameBangladesh   0.907    0.105       8.64  5.76e- 18\n 6 country_nameBolivia     -0.217    0.0949     -2.29  2.23e-  2\n 7 country_nameBrazil      -3.11     0.0977    -31.8   4.15e-220\n 8 country_nameMyanmar      0.852    0.105       8.12  4.76e- 16\n 9 country_nameChile       -0.220    0.111      -1.98  4.77e-  2\n10 country_nameChina        1.79     0.0895     19.9   3.09e- 88\n# ℹ 38 more rows\n\n\nOur main variable of interest is Q50 (financial satisfaction). Here, we can see a positive and statistically significant coefficient for this variable - regime satisfaction tends to increase alongside one’s financial situation. We also get coefficients for the countries in our dataset (e.g., country_nameArgentina, etc.). These compare the average value of the DV in the named country against the common reference group (here, Andorra). Regime satisfaction is lower in Argentina than Andora by -0.265 on average, while being higher in China than Andorra by 1.79 points on average, and so on.\nThe inclusion of these “fixed effects” essentially control for cluster-level sources of variation…whatever they might be. This accounts for the between-country differences above. However, it does not remove the bias from our standard errors. To fully accomplish this goal, we can use “clustered standard errors” - these are simply standard errors that are calculated in such a way they take into account the internal correlation within countries.\nWe can obtain the clustered standard errors when we use modelsummary() to create our regression tables. Here is how we can do it.\n\nmodelsummary(model_all_indiv, \n             stars = T,\n             coef_map = c(\n               \"(Intercept)\" = \"Intercept\", \n               \"Q50\" = \"Personal Economic Situation\"), \n1             vcov = ~country_name,\n2             gof_map = c(\"nobs\", \"r.squared\", \"adj.r.squared\", \"vcov.type\"),\n             notes = list(\"Linear regression coefficients with standard errors in parentheses. Standard errors are clustered by country.\"))\n\n\n1\n\nThis tells modelsummary() to use the clustered standard errors. Specifically, errors should be clustered by the country_name variable.\n\n2\n\nThe added bit here is \"vcov.type\". This tells the command to include a row at the bottom of the table indicating that SEs are clustered by this variable. This could be omitted provided you indicate in the notes that you are clustering SEs in some way.\n\n\n\n\n \n\n  \n    \n    \n    tinytable_7001gzbx0brhnl2ulw36\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\nLinear regression coefficients with standard errors in parentheses. Standard errors are clustered by country.\n        \n                \n                  Intercept                  \n                  4.586***        \n                \n                \n                                             \n                  (0.094)         \n                \n                \n                  Personal Economic Situation\n                  0.182***        \n                \n                \n                                             \n                  (0.014)         \n                \n                \n                  Num.Obs.                   \n                  66688           \n                \n                \n                  R2                         \n                  0.213           \n                \n                \n                  R2 Adj.                    \n                  0.212           \n                \n                \n                  Std.Errors                 \n                  by: country_name\n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n\nvcov = ~country_name\n\nThis is the one line of syntax you need. You would replace “country_name” with the name of the (factor) variable in your dataset that concerns the relevant cluster.\n\ngof_map = c(\"nobs\", \"r.squared\", \"adj.r.squared\", \"vcov.type\")\n\nOne note here. There is a new entry: “vcov.type”. The final row of the table above is “Std. Errors” and then “by: country_name”. This option controls whether that information is presented or not. If I omit “vcov.type” from this part of the command, then that line is omitted. That is fine provided that one then indicates in the notes that standard errors are being clustered.\n\ncoef_map = c(…)\n\ncoef_map() is another way of renaming variables in modelsummary(). I am using it here because anything that doesn’t match the contexts of this command is excluded from the regression table. In this example, I wanted to exclude all of the country-level fixed effects estimates to avoid creating a super long table. I would recommend doing the same in your paper for the regression table presented in the main body of the text and then provide the full table (with all estimates) in an appendix. Or, alternatively, use a coefficient plot in the main text and then give the full table in an appendix.\n\n\nOne thing to note: the use of these “clustered standard errors” does not change the coefficient estimates - they only change the standard errors. I show this in the following snippet using modelsummary(). Here, I specify that the output should be a “modelsummary_list” - this tells the command to show the underlying data that is to be included in the table rather than the table itself. I then filter the results so that you only see the coefficient for our financial situation variable and its standard error. The coefficients remain the same, while the standard error becomes much larger (0.014 vs. 0.004). This does not matter for statistical significance in the present case - we have lots of data to help us gain precision, but we could see differences in other contexts.\n\n#Without clustering standard errors: \nmodelsummary(model_all_indiv, output ='modelsummary_list')$tidy |&gt; \n  select(term, estimate, std.error) |&gt; \n  slice(2)\n\n  term  estimate   std.error\n1  Q50 0.1815909 0.004100451\n\n#With clustering of standard errors\nmodelsummary(model_all_indiv, output ='modelsummary_list', \n             vcov = ~country_name)$tidy |&gt; \n  select(term, estimate, std.error) |&gt; \n  slice(2)\n\n  term  estimate  std.error\n1  Q50 0.1815909 0.01425808\n\n\nWe can also incorporate the clustering into our estimates of predicted values and slopes when using predictions() and slopes(). We only need to specify the vcov = option again.\n\npredictions(model_all_indiv, newdata = datagrid(Q50 = c(1, 5, 6, 8, 10)), vcov = ~country_name)\n\n\n Q50 Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 % country_name\n   1     5.10    0.07826  65.2   &lt;0.001 Inf  4.95   5.26    Indonesia\n   5     5.83    0.02122 274.8   &lt;0.001 Inf  5.79   5.87    Indonesia\n   6     6.01    0.00696 863.3   &lt;0.001 Inf  6.00   6.03    Indonesia\n   8     6.38    0.02155 295.8   &lt;0.001 Inf  6.33   6.42    Indonesia\n  10     6.74    0.05007 134.6   &lt;0.001 Inf  6.64   6.84    Indonesia\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, country_name, Q50, Q252 \nType:  response \n\n\nYou would use these procedures when your main IV is at the lower level of variation (e.g., the individual level of analysis) and want to use all of the data. However, what if your cared about variation at the aggregate level? This approach would not let you know about that since you’re subsuming all sources of variation into those country dummy variables. Including country-level variables would introduce multicollinearity Instead, you’d need other tools. The next two sections are more relevant in those situations.\n\n\n18.2.3 Aggregation\nThe preceding examples focus on a situation where the main IV is at the lower level of aggregation - i.e., at the individual level when we have data wherein individuals are nested within countries. If your main IV varies at the higher level of aggregation, e.g., the country level, then the simplest approach is likely to aggregate the data up to that level of analyses and proceed as normal. In other words, find the country-mean for the DV and then use that as the DV in subsequent regression model.\nAs an example, let’s suppose our argument is that regime satisfaction is predicted by economic conditions. In particular, we might expect that satisfaction will be higher in places with lower levels of unemployment (conversely, it should be lower in places with higher unemployment). The WVS thankfully includes a variable about the unemployment rate in each country (what a coincidence for my example), named unemploytotal. We can see a snippet of the data here - note how the data in the unemploytotal column is the same for all Argentine respondents.\n\nwvs |&gt; \n  select(country_name, Q252, unemploytotal) |&gt; \n  na.omit() |&gt; \n  head()\n\n     country_name Q252 unemploytotal\n1005    Argentina    3          9.79\n1006    Argentina    4          9.79\n1007    Argentina    5          9.79\n1008    Argentina    5          9.79\n1009    Argentina    7          9.79\n1010    Argentina   10          9.79\n\n\nTo prepare our data we’ll use group_by() to calculate the mean value of both variables by country. The unemployment variable is a constant within each country (i.e., all observations in Argentina have the same value [9.79], all observations in China have a value of 4.32, etc.). Taking the mean of this variable is the simplest way of porting over those values to our new dataset.\n\n#Aggregate the data \naggregated &lt;- wvs |&gt; \n  group_by(country_name) |&gt;\n  summarize(reg_satis = mean(Q252, na.rm = T), \n            unemploy = mean(unemploytotal, na.rm = T))\n\n#Take a look\naggregated\n\n# A tibble: 48 × 3\n   country_name reg_satis unemploy\n   &lt;fct&gt;            &lt;dbl&gt;    &lt;dbl&gt;\n 1 Andorra           5.78   NaN   \n 2 Argentina         5.41     9.79\n 3 Australia         5.74     5.27\n 4 Bangladesh        6.76     4.19\n 5 Bolivia           5.55     3.5 \n 6 Brazil            2.59    12.1 \n 7 Myanmar           6.57     1.58\n 8 Chile             5.48     7.09\n 9 China             7.55     4.32\n10 Taiwan ROC        5.11   NaN   \n# ℹ 38 more rows\n\n\nWe could then proceed to examine this data via a scatterplot (since both variables are continuous) to see if our intuitions seem correct:\n\n\nShow the code\n#Bivariate plot\nggplot(aggregated, aes(x = unemploy, y = reg_satis)) + \n  geom_point() + \n  geom_smooth(method = 'lm') + \n  labs(y = \"Mean Regime Satisfaction\", \n       x = \"Unemployment (% of total labor force)\") + \n  theme_minimal() + \n  scale_y_continuous(limits = c(1, 10), \n                     breaks = seq(from = 1, to = 10, by = 1))\n\n\n\n\n\n\n\n\nFigure 18.4: Unemployment & Regime Satisfaction\n\n\n\n\n\nIndeed, we see a negatively sloped line: regime satisfaction tends to decrease as unemployment increases.\nWe could then proceed to a regression model.\n\n#Model\nmodel_aggregated &lt;- lm(reg_satis ~ unemploy, data = aggregated)\n\n#Coefficients\ntidy(model_aggregated)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   5.80      0.314      18.5  4.40e-22\n2 unemploy     -0.0897    0.0414     -2.16 3.60e- 2\n\n\nWe see a negative (and statistically significant) coefficient for unemployment. A 1% increase in the unemployment rate (since this variable is on the % scale) is associated with a drop of country average regime satisfaction of approximately -0.09 scale points. Moving from minimum to maximum is expected to result in a drop of approximately 1.5 scale points (5.73 to 4.25)…although an unemployment rate of 17% or so is pretty anomalous in our data so we get wide confidence intervals. This would represent a pretty large difference given that the DV has a standard deviation of 1.19.\n\navg_predictions(model_aggregated, variables = \"unemploy\")\n\n\n unemploy Estimate Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n     0.75     5.73      0.288 19.88   &lt;0.001 289.8  5.16   6.29\n     3.42     5.49      0.211 26.06   &lt;0.001 494.8  5.08   5.90\n     4.59     5.39      0.188 28.72   &lt;0.001 600.2  5.02   5.75\n     8.19     5.06      0.190 26.60   &lt;0.001 515.5  4.69   5.44\n    17.24     4.25      0.485  8.76   &lt;0.001  58.9  3.30   5.20\n\nColumns: unemploy, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nThe model above only includes one predictor variable. We could (and should) include plausible confounding variable in the model as well. Note though that these are thing that should vary at the country-level. I sometimes see students who go down this path say they’ll include gender, and age, and individual demographics, but that may not make sense. One could aggregate up those variables and include them as well (or, alternatively, use some type of census data to obtain even better estimates of country-level values), but are they actually good confounds? Maybe, maybe not. Countries do not differ all that much in terms of sex/gender distributions (although there is some variation there). They may differ more in overall educational attainment or age patterns, but whether that makes sense to adjust for or not is something you’ll need to consider.\n\n\n18.2.4 Mixed/Multi-Level Models\nThere is a final method for dealing with this type of data: “multilevel” or “mixed” models. The examples above saw us modeling either individual-level variation or country-level variation. A multilevel model enables us to do both things simultaneously in a single model. It can thus subsume the other approaches above. However, I do not recommend that you take this approach given that multilevel models are more complex and you have not been previously trained in their use. The only context in which they would be a better alternative for you is if you are investigating “cross-level interactions”: that is, a situation where you are interesting in examining the interaction between an individual-level variable and a cluster-level variable.\nThe remainder of this section will walk through how to set one of these models up and some basic interpretation. However, this is only the tip of a deeper iceberg.\n\n18.2.4.1 Basic Set Up with a “Null” Model\nWe’ll need some extra R libraries for the following example:\n\nlibrary(lme4)\nlibrary(lmerTest)\nlibrary(performance)\nlibrary(parameters)\n\nlme4 is the go-to package for fitting multilevel models. However, it does have one drawback: modelsummary() cannot produce statistical significance stars using a model created with this package. I thus also load lmerTest which adds that functionality. The performance package is one you’ve run into before: it helps produce various model performance statistics. Finally, the parameters package is kind of like broom in that it provides some tools for displaying the results of a regression model. I load it because tidy() cannot work with the output of objects created by lmerTest. (Confusing? You bet!)\nOur first step is to fit a “null” model wherein the DV is only regressed on a constant.\n\nmixed_null &lt;- lmer(Q252 ~ 1 + (1 | country_name), data = wvs)\n\nThe syntax here looks kind of similar to the lm() command albeit with lmer rather than lm(). However, there is one very noticeable difference:\n\n(1 | country_name)\n\nThis tells the command to perform a “random intercept” mixed model with observations nested within country. The intercept, recall, is the value we expect Y to take on when our IVs = 0. In a “random intercept” model, the intercept is allowed to take on different values across clusters. In essence, respondents in each country will have a different intercept value. The model will then average those intercepts together (in essence) as a grand summary. This is the simplest multi-level model we can run. It is also possible to run “random coefficient” versions wherein the coefficient for an IV is also allowed to vary across clusters before being averaged, but that is beyond the scope of this document. If you needed to run one of these models, you’d keep this as is but replace country_name with the name of the variable pertaining to relevant cluster in your analysis ((1 | cluster_name)).\n\n\nLet’s take a look at the output:\n\nsummary(mixed_null)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: Q252 ~ 1 + (1 | country_name)\n   Data: wvs\n\nREML criterion at convergence: 312216.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.84879 -0.70666 -0.01245  0.66745  2.97927 \n\nRandom effects:\n Groups       Name        Variance Std.Dev.\n country_name (Intercept) 1.408    1.186   \n Residual                 6.182    2.486   \nNumber of obs: 66949, groups:  country_name, 47\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(&gt;|t|)    \n(Intercept)   5.2410     0.1733 46.0121   30.23   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe output begins in a somewhat familiar way as lm() with some information about the formula being used in the model and some residuals information, which we can largely ignore. It is then separated into two areas “Random Effects” and “Fixed Effects”. What do these mean? I like the description offered in this resource, which focuses on a scenario in which we are trying to model student test scores from students sampled from within various different schools:\n\nFor our purposes of executing and interpreting MLMs, a fixed effect is an average effect across all clusters and a random effect is a variance that describes how much an effect differs across clusters. Generally, fixed effects in MLMs capture the mean of an effect and the random effect captures the variance of an effect. For example, we might have a fixed effect for the intercept that describes average math achievement across all schools. Then we have a random effect that describes how intercepts for math achievement vary across schools. Together, the fixed and random effect describe math achievement scores across schools.\n\nIn our example, the Intercept value in the Fixed Effects area is an estimate of the average regime satisfaction across all countries. If we take the average in each country and then average together those country averages, this is more or less what we’ll get. We previously found the average within each country; here is the average of those averages, which is right in line with the Intercept above.\n\nmean(aggregated$reg_satis, na.rm = T)\n\n[1] 5.240866\n\n\nWhat then do the values in the Random Effects tell us? We see two rows here: one for “country_name” and one for “Residual”. The value in the “country_name” value tell us about the degree of variation in the country-averages for regime satisfaction. Here, for instance, is the standard deviation of the country-averages that we calculated earlier, which is again in line with the value in the Random effects area:\n\nsd(aggregated$reg_satis, na.rm = T)\n\n[1] 1.188207\n\n\nThe value for Residual, meanwhile, tells us about the variance/standard deviation of individuals around their country mean regime satisfaction. A multi-level model essentially “partitions” the variance in a DV into two “levels”: variance at level 1 owing to the lowest level of the model (here: individuals) and variance at level 2 owing to the higher level of aggregation (here: countries).8 We can then model this variation by including predictors measured at either level as we’ll see in the next sub-section.\n8 Well, in a two-level model. These types of models can be made more complex.We can get a sense of how much of the variance in our DV varies at each level via the icc() command from the performance package:\n\nicc(mixed_null)\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.185\n  Unadjusted ICC: 0.185\n\n\nThe value here is 0.185. In essence, this tells us the proportion of the total variance in the DV that occurs at the cluster level. We can think of this as telling us about the degree of similarity between respondents within the same country. The value of 0.185 indicaetst that around 18.5% of the total variance in the regime satisfaction DV is attributable to differences between countries (i.e., ~18.5% of the variance occurs at the country-level). That means that 0.815 or 81.5% occurs at the individual level. There is more variance between individuals than between countries. As a side note, if the ICC was very very low, then that might be an indicator that we shouldn’t bother with this multi-level model business - a simpler fixed effects model focused on individual level predictors would likely be a better idea since there wouldn’t be much cluster level variance to explain in the first place.\n\n\n18.2.4.2 Adding Predictor Variables\nWe can add predictor variables much as we do with an lm() or glm() model. Here, I’ll perform three models: (1) one that only includes the individual level predictor (financial situation); (2) one that only includes the country level predictor (unemployment); and then one that includes both at once (for a later discussion).\n\n# Only individual\nmixed_indiv &lt;- lmer(Q252 ~ Q50 + (1 | country_name), data = wvs)\n\n#Only aggregate\nmixed_agg &lt;- lmer(Q252 ~ unemploytotal + (1 | country_name), data = wvs)\n\n#Both\nmixed_both &lt;- lmer(Q252 ~ Q50 + unemploytotal + (1 | country_name), data = wvs)\n\nOur interpretations of the coefficients from these models is basically the same as with an lm() model. The intercept tells us the expected average value when the predictor variable(s) = 0, while coefficients tell us about the slope of a line (continuous variable) or about a difference in means between categories (binary/categorical variables).\nLet’s take a look at the individual-level model first. I’ll focus on the output from parameters() as this will get rid of the scientific notation that summary() would use when showing the coefficients:\n\n# Via Summary\nsummary(mixed_indiv)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: Q252 ~ Q50 + (1 | country_name)\n   Data: wvs\n\nREML criterion at convergence: 309059.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1048 -0.7375  0.0226  0.6872  3.4004 \n\nRandom effects:\n Groups       Name        Variance Std.Dev.\n country_name (Intercept) 1.273    1.128   \n Residual                 6.004    2.450   \nNumber of obs: 66688, groups:  country_name, 47\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept) 4.120e+00  1.668e-01 4.820e+01   24.70   &lt;2e-16 ***\nQ50         1.818e-01  4.100e-03 6.668e+04   44.34   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n    (Intr)\nQ50 -0.152\n\n#Via parameters::parameters()\nparameters(mixed_indiv)\n\n# Fixed Effects\n\nParameter   | Coefficient |       SE |       95% CI | t(66684) |      p\n-----------------------------------------------------------------------\n(Intercept) |        4.12 |     0.17 | [3.79, 4.45] |    24.70 | &lt; .001\nQ50         |        0.18 | 4.10e-03 | [0.17, 0.19] |    44.34 | &lt; .001\n\n# Random Effects\n\nParameter                    | Coefficient\n------------------------------------------\nSD (Intercept: country_name) |        1.13\nSD (Residual)                |        2.45\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\nThe intercept is 4.12. This tells us the following: the average regime satisfaction cross all countries when the financial situation variable = 0. The intercept is generally not very interesting and especially so in this case given that our IV cannot even taken on a value of 0 (since it ranges from 1 to 10). The coefficient for Q50 is more relevant. It is 0.18. We would interpret this as telling us that each one unit increase in subjective financial situation is associated with an average increase in regime satisfaction of around 0.18 scale points. People who are more financially satisfied are also more satisfied with their regime. This relationship is statistically significant as well (p &lt; 0.001).\nWe can get a sense of how important this change is via the predictions() or avg_predictions() commands from the margnialeffects package.\n\npredictions(mixed_indiv, newdata = datagrid(Q50 = c(1, 3, 5, 7, 9, 10)))\n\nWarning: For this model type, `marginaleffects` only takes into account the\n  uncertainty in fixed-effect parameters. You can use the `re.form=NA`\n  argument to acknowledge this explicitly and silence this warning.\n\n\n\n Q50 Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 % country_name\n   1     5.10      0.166 30.7   &lt;0.001 684.8  4.78   5.43    Indonesia\n   3     5.47      0.165 33.0   &lt;0.001 793.1  5.14   5.79    Indonesia\n   5     5.83      0.165 35.3   &lt;0.001 906.3  5.51   6.15    Indonesia\n   7     6.19      0.165 37.6   &lt;0.001   Inf  5.87   6.52    Indonesia\n   9     6.56      0.165 39.7   &lt;0.001   Inf  6.23   6.88    Indonesia\n  10     6.74      0.166 40.7   &lt;0.001   Inf  6.41   7.06    Indonesia\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, country_name, Q50, Q252 \nType:  response \n\n\nWhat about the stuff in the Random effects area? The entry for “country_name” still tells us about the variation in the country means. This value is unchanged from above because we have not added any country-level predictor variables. The entry for Residual still tells us about individual-level variation around country-means. This value is slightly smaller than earlier because we have added a predictor variable that explains some of the total variation at the individual level.\nLet’s turn to the country-level model:\n\nsummary(mixed_agg)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: Q252 ~ unemploytotal + (1 | country_name)\n   Data: wvs\n\nREML criterion at convergence: 302394.5\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.83604 -0.70387 -0.01185  0.66439  2.96728 \n\nRandom effects:\n Groups       Name        Variance Std.Dev.\n country_name (Intercept) 1.351    1.162   \n Residual                 6.234    2.497   \nNumber of obs: 64727, groups:  country_name, 45\n\nFixed effects:\n              Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)    5.79713    0.31358 43.00928  18.487   &lt;2e-16 ***\nunemploytotal -0.08967    0.04143 43.02262  -2.164    0.036 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nunemployttl -0.833\n\n\nThe Intercept here tells us the expected mean level of our dependent variable when the IV = 0, i.e., if unemployment = 0%, then we’d expect to observe the country-level average of regime satisfaction to be about 5.8. Meanwhile, a 1 unit (1% in this instance) increase in the unemployment rate is associated with a decrease of approximately -0.09 scale points on the DV.\n\npredictions(mixed_agg, newdata = datagrid(unemploytotal = c(1:10)))\n\nWarning: For this model type, `marginaleffects` only takes into account the\n  uncertainty in fixed-effect parameters. You can use the `re.form=NA`\n  argument to acknowledge this explicitly and silence this warning.\n\n\n\n unemploytotal Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n             1     6.43      0.280 23.0   &lt;0.001 385.8  5.89   6.98\n             2     6.35      0.249 25.5   &lt;0.001 474.0  5.86   6.83\n             3     6.26      0.221 28.3   &lt;0.001 582.8  5.82   6.69\n             4     6.17      0.198 31.1   &lt;0.001 704.2  5.78   6.55\n             5     6.08      0.182 33.4   &lt;0.001 811.3  5.72   6.43\n             6     5.99      0.174 34.4   &lt;0.001 859.0  5.65   6.33\n             7     5.90      0.176 33.5   &lt;0.001 815.5  5.55   6.24\n             8     5.81      0.187 31.0   &lt;0.001 699.0  5.44   6.17\n             9     5.72      0.206 27.7   &lt;0.001 558.6  5.31   6.12\n            10     5.63      0.231 24.3   &lt;0.001 431.3  5.17   6.08\n country_name\n    Indonesia\n    Indonesia\n    Indonesia\n    Indonesia\n    Indonesia\n    Indonesia\n    Indonesia\n    Indonesia\n    Indonesia\n    Indonesia\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, country_name, unemploytotal, Q252 \nType:  response \n\n\nThe coefficient for the country-level variable is smaller than the individual-level variable. We should perhaps be cautious about saying it is “less important” however. The degree of variation at the individual level is much larger than the amount of variation at the country-level as we saw from the Random effects estimates from the null model:\n\n#Uses the parameters() command and some filter to focus\n#our attention on the random effects\n\nparameters(mixed_null) |&gt; \n  filter(Parameter != \"(Intercept)\")\n\n# Random Effects\n\nParameter                    | Coefficient | 95% CI\n---------------------------------------------------\nSD (Intercept: country_name) |        1.19 |       \nSD (Residual)                |        2.49 |       \n\n\n\n\n\nA smaller looking change might be more “meaningful” if there is less variance to explain the first place. One way of seeing this is to consider the standardized coefficients from the two models:\n\n#Individual \nstandardise_parameters(mixed_indiv)\n\n# Standardization method: refit\n\nParameter   | Std. Coef. |        95% CI\n----------------------------------------\n(Intercept) |      -0.03 | [-0.14, 0.09]\nQ50         |       0.16 | [ 0.15, 0.17]\n\n#Country\nstandardise_parameters(mixed_agg)\n\n# Standardization method: refit\n\nParameter     | Std. Coef. |         95% CI\n-------------------------------------------\n(Intercept)   |      -0.02 | [-0.15,  0.10]\nunemploytotal |      -0.13 | [-0.25, -0.01]\n\n\nPerhaps the important thing here to interpret the coefficients in relation to their level of analysis (e.g., individual level predictors explaining differences between individuals, country-level predictors explaining differences between countries).\n\n\n18.2.4.3 Comparison with Earlier\nLet’s compare our results to those from the earlier models (individual level fixed effect with clustered standard errors and the aggregated model).\n\n\nShow the code\n#See the chapter on 'regression table formatting suggesion' \n#for an explanation of the flextable code\n\n#List\nmodel_comps &lt;- list(\n  \"Indiv w/FE\" = model_all_indiv, \n  \"Aggregated\" = model_aggregated, \n  \"Multi-level\" = mixed_indiv, \n  \"Multi-Level\" = mixed_agg, \n  \"Multi-Level\" = mixed_both)\n\n#Table with some flextable formatting\nmodel_comps_table &lt;- modelsummary(model_comps, \n             estimate = \"{estimate}{stars}\\n{std.error}\", \n             statistic = NULL, \n             gof_map = c(\"nobs\", \"r.squared\", \"adj.r.squared\", \n                         \"r2.marginal\", \"r2.conditional\"),\n             vcov = c(~ country_name, \"classical\", \"classical\", \n                      \"classical\", \"classical\"), \n             coef_map = c(\n               \"(Intercept)\" = \"Intercept\", \n               \"Q50\" = \"Personal Financial Situation\", \n               \"unemploy\" = \"Country Unemployment Rate\", \n               \"unemploytotal\" = \"Country Unemployment Rate\"), \n             notes = list(\"Notes: OLS or multi-level model coefficients with SEs in parentheses. Country fixed effect estimates omitted from FE model. FE model clusters SEs by country.\", \n                          \"* p &lt; 0.05; ** p &lt; 0.01; *** p &lt; 0.001\"),\n             output = 'flextable')\n             \n             \nmodel_comps_table |&gt; \n  hline(i = nrow_part(model_comps_table) - 3) |&gt;\n  align(i = 1:nrow_part(model_comps_table), j = 2:ncol_keys(model_comps_table), align = 'center') |&gt;\n  align(align = 'center', part = 'header') |&gt; \n  autofit()\n\n\n\n\nTable 18.1: Comparison of Different Methods of Analysing Clustered Data\n\n\n\n Indiv w/FEAggregatedMulti-levelMulti-LevelMulti-Level Intercept4.586***0.0945.797***0.3144.120***0.1675.797***0.3144.581***0.304Personal Financial Situation0.182***0.0140.182***0.0040.182***0.004Country Unemployment Rate-0.090*0.041-0.090*0.041-0.074+0.040Num.Obs.6668845666886472764467R20.2130.098R2 Adj.0.2120.077R2 Marg.0.0260.0170.043R2 Cond.0.1970.1920.207Notes: OLS or multi-level model coefficients with SEs in parentheses. Country fixed effect estimates omitted from FE model. FE model clusters SEs by country.* p &lt; 0.05; ** p &lt; 0.01; *** p &lt; 0.001\n\n\n\n\n\nA few things may stand out here. First, the coefficient for “Personal Financial Situation” is the same in all models in which it appears. This is because fitting a random intercept multi-level model is basically equivalent to fitting a simpler linear regression model with country dummy variables. The random intercept multi-level model allows the intercept to be “random’ (i.e., to take on a different value for each cluster) while still estimating a common (or”fixed”) estimate for the independent variables.9 We can see this by using the coef() command, which will return the coefficients for the Intercept and for the independent variable(s):\n9 That is, unless we also specify that the coefficient for an IV should be allowed to vary between clusters before being summarized into a single weighted estimate.\n# Different intercept, same slope\ncoef(mixed_indiv)\n\n$country_name\n              (Intercept)       Q50\nAndorra          4.582194 0.1817805\nArgentina        4.318242 0.1817805\nAustralia        4.529366 0.1817805\nBangladesh       5.485730 0.1817805\nBolivia          4.366886 0.1817805\nBrazil           1.486339 0.1817805\nMyanmar          5.431446 0.1817805\nChile            4.362747 0.1817805\nChina            6.366356 0.1817805\nTaiwan ROC       3.948085 0.1817805\nColombia         2.608765 0.1817805\nCyprus           4.020333 0.1817805\nEcuador          4.079620 0.1817805\nEthiopia         3.762728 0.1817805\nGermany          5.054844 0.1817805\nGreece           3.249699 0.1817805\nGuatemala        2.660598 0.1817805\nHong Kong SAR    3.945368 0.1817805\nIndonesia        4.920654 0.1817805\nIran             4.597352 0.1817805\nIraq             2.791373 0.1817805\nJapan            4.500479 0.1817805\nKazakhstan       5.195977 0.1817805\nJordan           4.433007 0.1817805\nSouth Korea      5.673280 0.1817805\nKyrgyzstan       3.717767 0.1817805\nLebanon          2.931462 0.1817805\nMacau SAR        4.411652 0.1817805\nMalaysia         4.247233 0.1817805\nMexico           3.162958 0.1817805\nNew Zealand      4.226980 0.1817805\nNicaragua        3.626962 0.1817805\nNigeria          3.407309 0.1817805\nPakistan         5.101552 0.1817805\nPeru             2.631667 0.1817805\nPhilippines      5.199632 0.1817805\nPuerto Rico      2.202826 0.1817805\nRomania          2.923390 0.1817805\nRussia           4.411431 0.1817805\nSerbia           3.567437 0.1817805\nVietnam          6.298000 0.1817805\nZimbabwe         2.930848 0.1817805\nTajikistan       6.789928 0.1817805\nThailand         4.367818 0.1817805\nTunisia          2.853026 0.1817805\nTurkey           4.965436 0.1817805\nUnited States    3.284655 0.1817805\n\nattr(,\"class\")\n[1] \"coef.mer\"\n\n\nThe main advantage of the multi-level model is also enabling us to model variation at the country level including the estimation of interactions between levels of analysis (if that is what we want to do).\nSecond, the coefficient for Country Unemployment Rate is also the same in the Aggregated Model and Multi-Level model where it is the only predictor. Again, this is because the two models are doing the same thing: using variation in country-level unemployment to predict country-level average regime satisfaction. The coefficient for this variable is not the same in the final model, for reasons I’ll discuss in a subsequent sub-section.\nThird, we get different types of R2 for the multi-level models: R2 Marg. (marginal R2) and R2 Cond. (conditional R2). Calculating an R2 for a multi-level model is not straightforward (much as it wasn’t for logistic models) but these are attempts to do so anyways. The “marginal” version only takes into account the influence of the “fixed effects” portion of the model, while the “conditional” version takes into account both fixed and random effects. Their interpretation is not very straightforward, but higher is typically “better” (caveats about model specification aside).\n\n\n18.2.4.4 To Center or Not to Center?\nOne thing that changed in Table 18.1 was the coefficient for Country Unemployment rate when we switched from a single-variable model (whether OLS or multi-level) to one that includes both that variable and the one with an individual-level predictor (Personal Financial Situation). What is going on there?\nConsider these two variables: Personal Financial Situation and Country Unemployment Rate. Both variables pertain to economic conditions in a country. Perhaps more importantly, it seems very plausible that changes in a country’s unemployment rate could impact a person’s (subjective) financial situation. We could perhaps think of the individual measure as being somewhat ‘downstream’ or ‘post-treatment’ of the country level measure!\nOne way to see this is to look at the relationship between country unemployment rate and country average scores for personal financial situation. Figure 18.5 does just this with a negative relationship emerging: people are, on average, less satisfied with their personal financial situation in places with more unemployment.\n\n\nShow the code\n#Get the aggregated data\necon_data &lt;- wvs |&gt; \n  group_by(country_name) |&gt; \n  summarize(personal = mean(Q50, na.rm = T), \n            country = mean(unemploytotal, na.rm =T))\n\n#Correlation between them using the correlation package\necon_corr &lt;- correlation::correlation(econ_data)\n\n#Plot\nggplot(econ_data, aes(x = country, y = personal)) + \n  geom_point() + \n  geom_smooth(method = 'lm') + \n  labs(x = \"Country Unemployment Rate\",\n       y = \"Country Average Personal Financial Situation\") + \n  theme_minimal() + \n  scale_y_continuous(limits = c(1,10), \n                     breaks = c(1:10)) + \n  geom_text(x = 3, \n            y = 8, \n            label = paste(\"Correlation =\", \n                              round(econ_corr[1,3],2), \n                              sep = \" \"))\n\n\n\n\n\n\n\n\nFigure 18.5: Correlation between Macro and Micro Economic Indicators\n\n\n\n\n\nThis brings us to the subject of this sub-section: to “center” or not to “center”. Centering simply means subtracting a constant value from each observation.10 One way we could center our data is by subtracting a variable’s mean score from each observation - this is called “grand mean centering”. A different type of centering is perhaps more common and useful in the context of a multi-level: subtracting the cluster mean from each observation within that cluster. This can be useful in a multi-level model. Variation on the centered individual-level variable can only represent within-cluster variation on that variable.\n10 You may remember this as part of standardizing a variable wherein we subtract the mean from each observation and then divide by the variable’s standard deviation. In so doing we create a variable with a mean of 0 and a standard deviation of 1.Here is how we can do this in the present instance by using group_by():\n\n#Center the variable within country\nwvs &lt;- wvs |&gt; \n  group_by(country_name) |&gt; \n  mutate(personal_country_mean = mean(Q50, na.rm = T)) |&gt; \n  ungroup() |&gt; \n  mutate(personal_center = Q50 - personal_country_mean)\n\nLet’s compare the original variable to the centered one:\n\nwvs |&gt; \n  select(Q50, personal_center) |&gt; \n  psych::describe()\n\n                vars     n mean   sd median trimmed  mad   min   max range\nQ50                1 69238 6.17 2.45   6.00    6.28 2.97  1.00 10.00  9.00\npersonal_center    2 69238 0.00 2.32   0.03    0.09 2.30 -6.48  6.36 12.83\n                 skew kurtosis   se\nQ50             -0.32    -0.54 0.01\npersonal_center -0.30    -0.29 0.01\n\n\nCentering the variable has not meaningfully affected its variance (the standard deviations are basically the same). It has basically just shifted people over so that they vary around a mean of 0.\n\nggplot(wvs, aes(x = personal_center, y =Q50)) + \n  geom_point() + \n  facet_wrap(~ country_name) + \n  labs(x = \"Centered Financial Satisfaction\", \n       y = \"Original Scale\") + \n  theme_minimal()\n\nWarning: Removed 340 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nLet’s see how this affects our results:\n\n\nShow the code\n#Fit model with centered variable\nmixed_both_center &lt;- lmer(Q252 ~ personal_center + unemploytotal + \n                            (1 | country_name), data = wvs)\n\n#Table\n#List\nmodel_comps1 &lt;- list(\n  \"Indiv w/FE\" = model_all_indiv, \n  \"Aggregated\" = model_aggregated, \n  \"Multi-level\" = mixed_indiv, \n  \"Multi-Level\" = mixed_agg, \n  \"Multi-Level\" = mixed_both, \n  \"Multi-Level\" = mixed_both_center)\n\n#Table with some flextable formatting\nmodel_comps_table1 &lt;- modelsummary(model_comps1, \n             estimate = \"{estimate}{stars}\\n{std.error}\", \n             statistic = NULL, \n             gof_map = c(\"nobs\", \"r.squared\", \"adj.r.squared\", \n                         \"r2.marginal\", \"r2.conditional\"),\n             vcov = c(~ country_name, \"classical\", \"classical\", \n                      \"classical\", \"classical\", \"classical\"), \n             coef_map = c(\n               \"(Intercept)\" = \"Intercept\", \n               \"Q50\" = \"Personal Financial Situation\", \n               \"personal_center\" = \"Personal Financial Situation (Centered)\",\n               \"unemploy\" = \"Country Unemployment Rate\", \n               \"unemploytotal\" = \"Country Unemployment Rate\"), \n             notes = list(\"Notes: OLS or multi-level model coefficients with SEs in parentheses. Country fixed effect estimates omitted from FE model. Clustered standard errors not taken into account in the first model.\", \n                          \"* p &lt; 0.05; ** p &lt; 0.01; *** p &lt; 0.001\"),\n             output = 'flextable')\n             \n             \nmodel_comps_table1 |&gt; \n  hline(i = nrow_part(model_comps_table1) - 3) |&gt;\n  align(i = 1:nrow_part(model_comps_table1), j = 2:ncol_keys(model_comps_table1), align = 'center') |&gt;\n  align(align = 'center', part = 'header') |&gt; \n  autofit()\n\n\n\n\nTable 18.2: Comparison with Centered IV\n\n\n\n Indiv w/FEAggregatedMulti-levelMulti-LevelMulti-Level Multi-Level  Intercept4.586***0.0945.797***0.3144.120***0.1675.797***0.3144.581***0.3045.797***0.313Personal Financial Situation0.182***0.0140.182***0.0040.182***0.004Personal Financial Situation (Centered)0.182***0.004Country Unemployment Rate-0.090*0.041-0.090*0.041-0.074+0.040-0.090*0.041Num.Obs.666884566688647276446764467R20.2130.098R2 Adj.0.2120.077R2 Marg.0.0260.0170.0430.040R2 Cond.0.1970.1920.2070.215Notes: OLS or multi-level model coefficients with SEs in parentheses. Country fixed effect estimates omitted from FE model. Clustered standard errors not taken into account in the first model.* p &lt; 0.05; ** p &lt; 0.01; *** p &lt; 0.001\n\n\n\n\n\nWe now get our original estimates back!\nShould you center a lower-level variable like this if you run a multi-level variable? This would make most sense to me if the lower-level variable is continuous in nature and there is a plausible case that the lower level variable is being affected by the higher order one.\n\n\n18.2.4.5 Interactions\nI began this broader section by recommend that you not use a multi-level model unless you want to examine an interaction between a variable measured at one level of analysis and a variable measured at another. In this example, for instance, you might want to examine whether the relationship between personal financial situation and the DV varies based on country context (e.g., maybe it’s bigger when unemployment is high vs. low?). Alternatively, we might want to know whether the effects of unemployment vary based on individual level financial conditions. If that is the type of analysis that your hypothesis is setting up, then a multi-level model is most appropriate.11\n11 We could try fitting an lm() here with our two variables, their interaction term, and country fixed effects. However, this introduces some severe multicollinearity that can only be avoided by dropping a country-observation. In this example, for instance, R decided to drop observations from the United States in order to estimate the model.12 There is one element of the summary() output that I did not discuss: the “Correlation of Fixed Effects” stuff. I generally wouldn’t worry too much about that. This website goes into what it means in more depth.We include an interaction term in the model in the same way that we do so with lm() or glm() models, via a *. I will use the centered version of the personal finance variable in this example. Here are the results using both summary() and parameters() since the latter will show won’t use scientific notation in this example (well, except for the very small interaction term!).12\n\n#Mixed Model with interaction\nmixed_interaction &lt;- lmer(Q252 ~ personal_center*unemploytotal + \n                            (1 | country_name), data = wvs)\n\n#Results\nsummary(mixed_interaction)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: Q252 ~ personal_center * unemploytotal + (1 | country_name)\n   Data: wvs\n\nREML criterion at convergence: 299319.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1010 -0.7391  0.0229  0.6926  3.4080 \n\nRandom effects:\n Groups       Name        Variance Std.Dev.\n country_name (Intercept) 1.350    1.162   \n Residual                 6.054    2.461   \nNumber of obs: 64467, groups:  country_name, 45\n\nFixed effects:\n                                Estimate Std. Error         df t value Pr(&gt;|t|)\n(Intercept)                    5.797e+00  3.135e-01  4.301e+01  18.491   &lt;2e-16\npersonal_center                1.711e-01  7.699e-03  6.442e+04  22.229   &lt;2e-16\nunemploytotal                 -8.974e-02  4.142e-02  4.302e+01  -2.167   0.0358\npersonal_center:unemploytotal  1.724e-03  1.047e-03  6.442e+04   1.646   0.0998\n                                 \n(Intercept)                   ***\npersonal_center               ***\nunemploytotal                 *  \npersonal_center:unemploytotal .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) prsnl_ unmply\npersnl_cntr  0.000              \nunemployttl -0.833  0.000       \nprsnl_cntr:  0.000 -0.840  0.000\n\nparameters(mixed_interaction)\n\n# Fixed Effects\n\nParameter                       | Coefficient |       SE |         95% CI | t(64461) |      p\n---------------------------------------------------------------------------------------------\n(Intercept)                     |        5.80 |     0.31 | [ 5.18,  6.41] |    18.49 | &lt; .001\npersonal center                 |        0.17 | 7.70e-03 | [ 0.16,  0.19] |    22.23 | &lt; .001\nunemploytotal                   |       -0.09 |     0.04 | [-0.17, -0.01] |    -2.17 | 0.030 \npersonal center × unemploytotal |    1.72e-03 | 1.05e-03 | [ 0.00,  0.00] |     1.65 | 0.100 \n\n# Random Effects\n\nParameter                    | Coefficient\n------------------------------------------\nSD (Intercept: country_name) |        1.16\nSD (Residual)                |        2.46\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\nHere is a reminder about how to read coefficients in an interaction:\n\npersonal_center: This tells us the estimated relationship between the personal financial situation variable and the DV when unemployment = 0. If we could observe countries where unemployment = 0, then we’d expect regime satisfaction to increase by 0.17 scale points, on average, for each one unit increase in financial situation. We do not observe any such countries, so we should be a bit careful about simply relying on this coefficient to interpret the model.\nunemploytotal: We centered the financial situation variable such that 0 = country mean. A one unit increase in unemployment when personal financial situation is at the mean level within a country is around -0.09 (so, increasing unemployment = decreasing average regime satisfaction).\nInteraction term: The interaction term is 0.0017 or 0.002 when rounding. We could use this to say two different things:\n\nThe effect for personal situation becomes more positive by 0.002 scale points with every one unit increase in unemployment. In other words, it seems like personal financial situation is more important in countries with more unemployment…but the interaction term is statistically insignificant (p = 0.1) at conventional levels so we cannot rule out the possibility that the slope for financial situation doesn’t change when unemployment changes.\nThe effect of country unemployment becomes less negative by 0.002 scale points with each one unit increase in personal financial situation. I say less negative because the coefficient for unemploytotal is negative but the interaction term is positive. So, unemployment seems to matter less for those with more financial security…but again, not statistically significant.\n\n\nAs always with interactions we should turn to predicted values or slope estimates to actually make sense of these results (with graphs of these values being an especially good idea):\n\n\nShow the code\n# The syntax below uses some R syntax that is a little bit more advanced\n# than taught in Statistics I and II. Basically, I'm storing the values \n# I want to make predictions form in a data object and then \n# directly accessing those values in other syntax calls, instead of writing them \n# down and then manually entering them. \n\n##Finding 1 SD &lt; mean, mean, 1 SD &gt; mean for the two variables\n#Uses psych::describe to create a dataframe with the mean and sd\nmean_data &lt;- wvs |&gt; \n  select(personal_center, unemploytotal) |&gt; \n  psych::describe()\n\n#I use some base R notation here (the [] stuff) to pass the \n#values from the mean_data df into a vector. I could do this manually\n#as well, e.g., unemploy_sdbelow &lt;- 6.22 - 4.07, etc., but want to get on \n#with it! See the R book on interactions for doing this \n#manually\n\npersonal_values &lt;- c(mean_data[1,3] - mean_data[1,4], # mean - sd\n                     mean_data[1,3], #mean, \n                     mean_data[1,3] + mean_data[1,4]) #mean + sd\n\nunemploy_values &lt;- c(mean_data[2,3] - mean_data[2,4], #mean + sd\n                     mean_data[2,3], #mean, \n                     mean_data[2,3] + mean_data[2,4]) #mean + sd\n\n## Predicted Values\n# Personal by Country\n#Also uses some base R stuff to simplify getting min to max values for\n#unemployment. \nplot1 &lt;- predictions(mixed_interaction, \n            newdata = datagrid(personal_center = c(-6:6), \n                               unemploytotal = unemploy_values)) |&gt; \n  mutate(unemploytotal = factor(unemploytotal, \n                                labels = c(\"1 SD &lt; Mean\", \n                                           \"Mean\", \n                                           \"1 SD &gt; Mean\"))) |&gt;\n  ggplot(aes(x = personal_center, y = estimate, linetype = unemploytotal)) + \n  geom_line() + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.2) + \n  labs(x = \"(Centered) Personal Financial Situation\", \n       y = \"Predicted Value\", \n       linetype = \"Country Unemployment\") + \n  theme_bw() + \n  theme(legend.position = \"bottom\") + \n  guides(linetype = guide_legend(nrow = 2))\n\n# Country by Personal\nplot2 &lt;- predictions(mixed_interaction, \n            newdata = datagrid(personal_center = personal_values, \n                               unemploytotal = mean_data[2,8]:mean_data[2,9])) |&gt; \n  mutate(personal_center = factor(personal_center, \n                                labels = c(\"1 SD &lt; Mean\", \n                                           \"Mean\", \n                                           \"1 SD &gt; Mean\"))) |&gt;\n  ggplot(aes(x = unemploytotal, y = estimate, linetype = personal_center)) + \n  geom_line() + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.2) + \n  labs(x = \"Country Unemployment Rate\", \n       y = \"Predicted Value\", \n       linetype = \"Personal Financial\") + \n  theme_bw() + \n  theme(legend.position = \"bottom\")  + \n  guides(linetype = guide_legend(nrow = 2))\n\n\n##Slopes\nplot3 &lt;- avg_slopes(mixed_interaction, \n                    variables = \"personal_center\", \n                    by = \"unemploytotal\") |&gt; \n  ggplot(aes(x = unemploytotal, y = estimate)) + \n  geom_line() + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.2) + \n  geom_hline(yintercept = 0, linetype = 'dashed', color = 'red') + \n  labs(x = \"Country Unemployment\",\n       y = \"AME for Personal Financial Situation\") + \n  theme_bw()\n \nplot4 &lt;- avg_slopes(mixed_interaction, \n           variables = \"unemploytotal\", \n           by = \"personal_center\") |&gt; \n  ggplot(aes(x = personal_center, y = estimate)) + \n  geom_line() + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.2) +\n  geom_hline(yintercept = 0, linetype = 'dashed', color = 'red') + \n  labs(x = \"(Centered) Personal Financial Situation\",\n       y = \"AME for Country Unemployment\") + \n  theme_bw()\n \n#Combining\nplot3 + plot4 + plot1 + plot2\n\n\n\n\n\n\n\n\nFigure 18.6: Predicted Values and AMEs from the Interaction Model\n\n\n\n\n\nThe top part of Figure 18.6 show the average marginal effect of personal financial situation by unemployment (left) or unemployment by personal financial situation (right). We can see that the effect of a person’s personal financial situation is expected to increase in size as we move from countries with less to countries with more unemployment…but the change is quite wee. Likewise, we the effect of country unemployment is more negative among those with very poor financial situations (e.g., around -4 or -5) than among those with better ones (e.g., around 4 or 5)…but the change in effect is very wee and there is a lot of uncertainty here. This is brought home further by the predicted values plots on the bottom where we get nearly parallel lines. It would be hard to see these results and say that macro and micro economic conditions interact!\n\n\n18.2.4.6 Assumptions\nWe can use some of our same old toosl to check assumptions for a linear mixed model (e.g., resid_panel() and car::vif()), although car::avPlots() will not work. You can find a walk through of how to check assumptions for these types of models via this helpful guide.\n\n\n\n18.2.5 What about Logit Models?\nThe discussions above focus on situations where our DV is (assumed to be) continuous (interval/ratio) in nature. What about if our DV is binary? The same basic considerations apply:\n\nIs the relationship you’re interested in testing something that varies at the lower level of aggregation (e.g., between individuals)? Then focus on a single cluster or use fixed effects.\nIs the relationship of interest primarily at the higher level of aggregation (e.g., between countries)? Then recode the DV to be 0/1, find the mean by cluster, and regress that on your predictor variables. The DV here would vary continuously between 0 and 1, so a linear regression model would likely be sufficient.\nDo you need or want a more complicated solution? Then you could fit a mixed effects/multi-level logistic model instead (see here).",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Geographically Clustered/Nested Data</span>"
    ]
  },
  {
    "objectID": "analysis_mlogit.html",
    "href": "analysis_mlogit.html",
    "title": "19  Categorical DVs",
    "section": "",
    "text": "19.1 Example Question\nLet’s start with an initial question: what predicts variation in a person’s partisan identification? I will use data from the 2020 American National Election Studies to consider this question. Respondents on this survey are asked two questions on this front with the nature of the second question depending on the answer to the first:\nThe final question here is important because many Americans who say they are Independent do indeed ‘lean’ toward one of the parties and, more or less, act like partisans when it comes time to vote or evaluate partisan politics. They are essentially “stealth” partisans and so we don’t want to just dump them into the “(pure) Independent” bucket.1\nMost analyses of American politics that use these measures will use a 7-pt scale constructed from them arraying people from “Strong Democrat” to “Strong Republican”. Here is that the distribution of that variable looked like on the (raw, unweighted) 2020 ANES.\nShow the code\nanes |&gt; \n1  filter(V201231x &gt; 0) |&gt;\n  group_by(V201231x) |&gt; \n2  tally() |&gt;\n  ggplot(aes(x = V201231x, y = n)) + \n  geom_col() + \n  scale_x_continuous(breaks = c(1:7), \n3                     labels = c(\"Str.\\nDemocrat\", \"Not Str.\\nDemocrat\",\n                                \"Lean\\nDemocrat\", \"Independent\", \n                                \"Lean\\nRepublican\", \"Not Str.\\nRepublican\", \n                                \"Str.\\nRepublican\")) + \n  labs(x = \"Party Identification\", \n       y = \"# Respondents in Category\")\n\n\n\n1\n\nRemoves 31 respondents who refused to answer and 4 who ultimately said don’t know. This is out of 8280 total respondents.\n\n2\n\nCalculates the number in each group of V201231x. A simpler version of summarize(varname = n()).\n\n3\n\nThe \\n stuff here tells ggplot to add a line break. I do this to avoid the labels overlapping with one another.\nWe might then model this variable using an OLS linear regression model. In so doing, we are assuming that this ordinal variable is interval/ratio in nature. This is a common assumption in this literature. Here, I’ll predict it using a person’s attitudes regarding abortion (1 = abortion should never be permitted … 4 = women should always be able to obtain an abortion), government spending (higher = government should spend more on services), education, whether someone in the household is a union member, sex, race-ethnicity, and age.\nShow the code\n# Some data cleaning\nanes &lt;- anes |&gt; \n  mutate(\n1    abortion = ifelse(V201336 %in% c(-9,-8,5), NA, V201336),\n    spending = ifelse(V201246 %in% c(-9, 99), NA, V201246), \n    education = ifelse(V201511x &lt; 0, NA, V201511x), \n    union = ifelse(V201544 &lt; 0, NA, V201544), \n    union = factor(union, \n                   levels = c(2, 1), \n                   labels = c(\"Not a Union HH\", \n                              \"Union Household\")), \n    sex = ifelse(V201600 == -9, NA, V201600), \n    sex = factor(sex, \n                 levels = c(1, 2), \n                 labels = c(\"Male\", \"Female\")), \n    race_eth = ifelse(V201549x &lt; 0, NA, V201549x), \n    race_eth = factor(race_eth, \n                      levels = c(1,2,3,4,5,6), \n                      labels = c(\"White\", \"Black\", \"Hispanic\", \n                                 \"Asian\", \"Native American\", \n                                 \"Multiple Races\")), \n    age = ifelse(V201507x == -9, NA, V201507x), \n    partyid = ifelse(V201231x %in% c(-9,-8), NA, V201231x))\n\n# Initial model\nmodel_ols &lt;- lm(partyid ~ abortion + spending + education + union + sex + race_eth + age, data = anes)\n\nsummary(model_ols)\n\n\n\n1\n\nI need to convert respondents with missing value codes (e.g., -9, -8) or “Other” responses to NA. If I do not need to make any other change to the variable, then an ifelse() command is usually easiest. Here: if an observation has these values (-9, -8, 5) on the V201336 variable, convert them to NA; if some other value, give that observation whatever value it had on the V201336 variable.\n\n\n\n\n\nCall:\nlm(formula = partyid ~ abortion + spending + education + union + \n    sex + race_eth + age, data = anes)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8823 -1.2452 -0.0336  1.1839  6.0058 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              9.527003   0.125924  75.657  &lt; 2e-16 ***\nabortion                -0.711652   0.021640 -32.886  &lt; 2e-16 ***\nspending                -0.512268   0.013366 -38.325  &lt; 2e-16 ***\neducation               -0.182575   0.019975  -9.140  &lt; 2e-16 ***\nunionUnion Household    -0.218417   0.061601  -3.546 0.000394 ***\nsexFemale               -0.149999   0.043321  -3.462 0.000539 ***\nrace_ethBlack           -1.176989   0.079908 -14.729  &lt; 2e-16 ***\nrace_ethHispanic        -0.532062   0.077450  -6.870 7.04e-12 ***\nrace_ethAsian           -0.445115   0.119841  -3.714 0.000206 ***\nrace_ethNative American -0.141164   0.161144  -0.876 0.381056    \nrace_ethMultiple Races  -0.255277   0.118702  -2.151 0.031548 *  \nage                     -0.005102   0.001294  -3.943 8.12e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.706 on 6374 degrees of freedom\n  (1894 observations deleted due to missingness)\nMultiple R-squared:  0.4444,    Adjusted R-squared:  0.4435 \nF-statistic: 463.5 on 11 and 6374 DF,  p-value: &lt; 2.2e-16\nWe can see that policy attitudes significantly and substantially predict this dependent variable. For instance, moving from 1 on the abortion scale (abortion should never be permitted) to 4 (abortion should always be possible if a woman wants one) is associated with a change of around 2 scale points on the 7-pt scale (more/less equivalent of moving from leaning toward the Republican party to leaning to the Democratic party).\npredictions(model_ols, newdata = datagrid(abortion = c(1:4)))\n\n\n abortion Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 % spending\n        1     5.42     0.0558  97.1   &lt;0.001 Inf  5.31   5.53     4.59\n        2     4.71     0.0406 116.1   &lt;0.001 Inf  4.63   4.79     4.59\n        3     4.00     0.0333 120.0   &lt;0.001 Inf  3.93   4.06     4.59\n        4     3.29     0.0389  84.5   &lt;0.001 Inf  3.21   3.36     4.59\n education          union    sex race_eth  age\n      3.45 Not a Union HH Female    White 51.7\n      3.45 Not a Union HH Female    White 51.7\n      3.45 Not a Union HH Female    White 51.7\n      3.45 Not a Union HH Female    White 51.7\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, spending, education, union, sex, race_eth, age, abortion, partyid \nType:  response\nThe foregoing analyses use this variable on its original 7-pt scale. In so doing, we are examining both classification (is a person Democrat or Republican?) but also the strength of that identification (e.g., not strong, strong, lean). Perhaps we have a different RQ in mind or prefer a different way of analyzing this data that focuses simply on categorization: what predicts whether a person chooses “Republican” vs. “Democrat”, vs. “Independent” (regardless of whether they say “strong” or “not strong”)\nShow the code\n#Creating the 3-item categorical variable\nanes &lt;- anes |&gt; \n  mutate(\n    pid3 = case_when(\n      partyid %in% c(1:3) ~ \"Democrat\", \n      partyid == 4 ~ \"Independent\", \n      partyid %in% c(5:7) ~ \"Republican\"))\n\n#  Plot\nanes |&gt; \n  group_by(pid3) |&gt; \n  tally() |&gt; \n  filter(!is.na(pid3)) |&gt; \n  ggplot(aes(x = pid3, y = n)) + \n  geom_col() + \n  labs(y = \"# in Category\", \n       x = \"Party Identification\")\nA linear model doesn’t quite make sense here since our data is now categorical. The type of logistic regression you learned in Statistics II also won’t work because that is a type of logistic model (a “binomial” logistic model) that is used for predicting binary data. We could theoretically dichotomous this variable and run that type of model. For instance, we could create a binary variable where 1 = Republican and 0 = Democrat or 1 = Democrat and 0 = Republican or Independent. This might make sense in some contexts. Perhaps we are studying vote choices in a multi-party context and we ultimately care about predicting whether a person votes for a party in government vs. some other party. Creating a dummy variable and running a binomial logistic model in that situation (e.g., 1 = vote for government party, 0 = vote for other party) would make sense. However, this only makes sense in relation to a particular RQ and hypothesis. What if our RQ/hypothesis implied a model predicting whether a person votes for a government party, a left-wing opposition party, or a right-wing opposition party? Dichotomization of the DV wouldn’t help us there.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Categorical DVs</span>"
    ]
  },
  {
    "objectID": "analysis_mlogit.html#example-question",
    "href": "analysis_mlogit.html#example-question",
    "title": "19  Categorical DVs",
    "section": "",
    "text": "“Generally speaking, do you usually think of yourself as a Democrat, a Republican, an independent, or what?\n[If answering Democrat or Republican on first question] Would you call yourself a strong [Democrat / Republican] or a not very strong [Democrat / Republican]?\n[If answering something other than Democrat/Republican on the first question] Do you think of yourself as closer to the Republican Party or to the Democratic Party? (Response options: closer to Republican, closer to Democrat, Neither)\n\n\n1 If they walk like a partisan and talk like a partisan, then why don’t they say they’re a partisan on the first question? For some this may reflect ambivalence about the party due to either current events indicating poor performance by the party or, perhaps, good performance by the other side. Lavine, Johnston, and Steenbergen provide a great account of how partisan ambivalence influences decision making. For others, meanwhile, this may occur because many people don’t like “partisans” - they think they’re conflict oriented weirdos and most normies would rather cut off their foot then have to spend one more minute talking about nasty ole politics with them. This leads some partisans to say they’re Independent. Klar and Krupnikov provide the definitive account here.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Categorical DVs</span>"
    ]
  },
  {
    "objectID": "analysis_mlogit.html#multinomial-logistic-model",
    "href": "analysis_mlogit.html#multinomial-logistic-model",
    "title": "19  Categorical DVs",
    "section": "19.2 Multinomial Logistic Model",
    "text": "19.2 Multinomial Logistic Model\nYou can think of a multinomial logistic model as a type of extension of the binomial logistic model you learned about in Statistics II but one that is used for predicting membership in a categorical variable with more than two unordered categories. In a multinomial logistic model, we will specify a particular category of the DV as our ‘reference category’ and then use a logistic model to model the chances of being in each of the other categories of the DV relative to the chances of being in the reference category. This is kind of similar to what we do with categorical IVs (pick a reference category and compare every other category to it) only with the categorical variable serving as the DV rather than an IV.\n\n19.2.1 Running the Model\nMultinomial logistic models are not natively supported in R, so we will need a library with a multinomial-logistic model function in order to run such a model. There are actually a few such libraries out there, but I will use the nnet package. One advantage of this particular package relative to alternatives (such as the mlogit package) is that does not require us to reshape our data (see Chapter 16).\n\n# The relevant library\nlibrary(nnet)\n\n# Re-Running our model\nmodel_multinom &lt;- multinom(pid3 ~ abortion + spending + education + union + sex + race_eth + age, data = anes)\n\n# weights:  39 (24 variable)\ninitial  value 7015.738075 \niter  10 value 5033.809807\niter  20 value 4344.935414\niter  30 value 4109.995309\nfinal  value 4106.488373 \nconverged\n\n\nThe code chunk above loads the nnet library and then runs our model. The model specification process is the same as with OLS models, etc., but now we use the multinom() function. An important note about the DV: I created the pid3 variable above as a character variable with three categories (“Democrat”, “Independent”, and “Republican”). The multinom command will work with this variable but it will automatically use the first alphabetical category as the reference group in the analysis. If I wanted to change this (e.g., use “Republican” or “Independent” as the reference category), I could convert this pid3 variable to a factor variable and specify the first level manually.\nLet’s take a look at our results:\n\nsummary(model_multinom)\n\nCall:\nmultinom(formula = pid3 ~ abortion + spending + education + union + \n    sex + race_eth + age, data = anes)\n\nCoefficients:\n            (Intercept)   abortion   spending  education unionUnion Household\nIndependent     4.36886 -0.4528678 -0.4260675 -0.4422536           -0.2785250\nRepublican      8.19529 -1.0375700 -0.8203869 -0.2332257           -0.2007405\n             sexFemale race_ethBlack race_ethHispanic race_ethAsian\nIndependent -0.1566055    -0.6350033       0.00886481     0.3666845\nRepublican  -0.2239796    -2.1148844      -0.86714927    -0.7734978\n            race_ethNative American race_ethMultiple Races         age\nIndependent               1.2625264              0.3326440 -0.01376943\nRepublican               -0.1276978             -0.4767468 -0.00436562\n\nStd. Errors:\n            (Intercept)  abortion   spending  education unionUnion Household\nIndependent   0.3246467 0.0487264 0.03160227 0.04362111            0.1387706\nRepublican    0.2666399 0.0382086 0.02638759 0.03441781            0.1056968\n             sexFemale race_ethBlack race_ethHispanic race_ethAsian\nIndependent 0.09379058     0.1712307        0.1474319     0.2177100\nRepublican  0.07347862     0.1900124        0.1360432     0.2106834\n            race_ethNative American race_ethMultiple Races         age\nIndependent               0.2655892              0.2224507 0.002822830\nRepublican                0.2867023              0.2109192 0.002215591\n\nResidual Deviance: 8212.977 \nAIC: 8260.977 \n\n\nOur results look a little bit different, but are conveying the same basic information. We get one area for “Coefficients:”: this tells us the coefficient for each variable. We get another area for “Std. Errors” which gives the standard errors for each variable. There are two rows in each area meanwhile. This is because we are basically fitting two models: one model predicting whether a person is an Independent rather than a Democrat (the “Independent” row in each section) and another model predicting whether a person is a Republican rather than a Democrat (the “Republican” row).\nThese results are a bit hard to look at. Instead, we can use the parameters() function from the, uh, parameters package instead:2\n2 We could also use the tidy() command from the broom package, but parameters() separates the two different models more clearly.\nparameters(model_multinom)\n\n# Response level: independent\n\nParameter                  | Log-Odds |       SE |         95% CI |      z |      p\n-----------------------------------------------------------------------------------\n(Intercept)                |     4.37 |     0.32 | [ 3.73,  5.01] |  13.46 | &lt; .001\nabortion                   |    -0.45 |     0.05 | [-0.55, -0.36] |  -9.29 | &lt; .001\nspending                   |    -0.43 |     0.03 | [-0.49, -0.36] | -13.48 | &lt; .001\neducation                  |    -0.44 |     0.04 | [-0.53, -0.36] | -10.14 | &lt; .001\nunion [Union Household]    |    -0.28 |     0.14 | [-0.55, -0.01] |  -2.01 | 0.045 \nsex [Female]               |    -0.16 |     0.09 | [-0.34,  0.03] |  -1.67 | 0.095 \nrace eth [Black]           |    -0.64 |     0.17 | [-0.97, -0.30] |  -3.71 | &lt; .001\nrace eth [Hispanic]        | 8.86e-03 |     0.15 | [-0.28,  0.30] |   0.06 | 0.952 \nrace eth [Asian]           |     0.37 |     0.22 | [-0.06,  0.79] |   1.68 | 0.092 \nrace eth [Native American] |     1.26 |     0.27 | [ 0.74,  1.78] |   4.75 | &lt; .001\nrace eth [Multiple Races]  |     0.33 |     0.22 | [-0.10,  0.77] |   1.50 | 0.135 \nage                        |    -0.01 | 2.82e-03 | [-0.02, -0.01] |  -4.88 | &lt; .001\n\n# Response level: republican\n\nParameter                  |  Log-Odds |       SE |         95% CI |      z |      p\n------------------------------------------------------------------------------------\n(Intercept)                |      8.20 |     0.27 | [ 7.67,  8.72] |  30.74 | &lt; .001\nabortion                   |     -1.04 |     0.04 | [-1.11, -0.96] | -27.16 | &lt; .001\nspending                   |     -0.82 |     0.03 | [-0.87, -0.77] | -31.09 | &lt; .001\neducation                  |     -0.23 |     0.03 | [-0.30, -0.17] |  -6.78 | &lt; .001\nunion [Union Household]    |     -0.20 |     0.11 | [-0.41,  0.01] |  -1.90 | 0.058 \nsex [Female]               |     -0.22 |     0.07 | [-0.37, -0.08] |  -3.05 | 0.002 \nrace eth [Black]           |     -2.11 |     0.19 | [-2.49, -1.74] | -11.13 | &lt; .001\nrace eth [Hispanic]        |     -0.87 |     0.14 | [-1.13, -0.60] |  -6.37 | &lt; .001\nrace eth [Asian]           |     -0.77 |     0.21 | [-1.19, -0.36] |  -3.67 | &lt; .001\nrace eth [Native American] |     -0.13 |     0.29 | [-0.69,  0.43] |  -0.45 | 0.656 \nrace eth [Multiple Races]  |     -0.48 |     0.21 | [-0.89, -0.06] |  -2.26 | 0.024 \nage                        | -4.37e-03 | 2.22e-03 | [-0.01,  0.00] |  -1.97 | 0.049 \n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald normal distribution approximation.\n\n\n\nThe model has a log- or logit-link. Consider using `exponentiate =\n  TRUE` to interpret coefficients as ratios.\n\n\nAh, that’s much better.\nLet’s start with the first area: “Response level: independent”. The coefficients here tells about the log of the odds of being in the Independent category rather than the Democrat category. Positive values would thus indicate that a higher values on the IV are associated with an increased chance of saying one is an Independent rather than a Democrat while negative values imply the opposite. We can see that increasing left-wing abortion and spending attitudes are associated with a reduced chance of being an Independent and, concomitantly, an increased chance of being a Democrat. These relationships are statistically significant (p &lt; 0.001). Native Americans, on the other hand, are more likely to say they are Independents rather than Democrats compared to White respondents(ref. category of the race/ethnicity IV) conditional on the other predictors in the model (b = 1.26, p &lt; 0.001).\nThe second area focuses on “Response level: Republican”. The coefficients here tells about the log of the odds of being in the Republican category rather than the Democrat category. We get some similar patterns here, e.g., more liberal abortion attitudes are associated with a lower chance of being Republican and a higher chance of being Democrat. On the other hand, we see no difference between Native Americans and White respondents in being Republican rather than Democrat.\n\n\n19.2.2 Predicted Values & AMEs\nA multinomial logistic model is a logistic model. The coefficients are thus on a log of the odds scale. As such, they are only really useful to you in telling you, and the reader, about the direction of a relationship. If you want to know about substance, then you should use the predictions() command to calculate predicted probabilities and/or the slopes() command to estimate average marginal effects (with both commands coming from the marginaleffects package).\nLet’s focus first on the abortion attitude measure, which has the following levels:3\n3 Ignore the Refused, Don’t Know, and Other responses which I converted to NA before fitting the model. I have treated this variable as continuous, but there is a reasonable case for treating it as categorical/ordinal and converting it to a factor variable.\nanes |&gt; select(V201336) |&gt; sjPlot::view_df()\n\n\n\nData frame: select(anes, V201336)\n\n\n\n\n\n\n\n\n\nID\nName\nLabel\nValues\nValue Labels\n\n\n1\nV201336\nPRE: STD Abortion: self-placement\n-9\n-8\n1\n2\n3\n4\n5\n-9. Refused\n-8. Don't know\n1. By law, abortion should never be permitted\n2. The law should permit abortion only in case of rape, incest, or when the woma\n3. The law should permit abortion other than for rape/incest/danger to woman but\n4. By law, a woman should always be able to obtain an abortion as a matter of pe\n5. Other {SPECIFY}\n\n\n\n\n\n\nLet’s take a look at the output of the predictions() command:\n\npred_multinom &lt;- predictions(model_multinom, \n            newdata = datagrid(abortion = c(1:4)))\n\npred_multinom\n\n\n       Group abortion Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n Democrat           1   0.0986    0.00875 11.26   &lt;0.001  95.3 0.0814 0.1157\n Democrat           2   0.2261    0.01217 18.57   &lt;0.001 253.4 0.2022 0.2499\n Democrat           3   0.4293    0.01285 33.40   &lt;0.001 810.1 0.4041 0.4545\n Democrat           4   0.6473    0.01294 50.04   &lt;0.001   Inf 0.6220 0.6727\n Independent        1   0.0640    0.00734  8.71   &lt;0.001  58.2 0.0496 0.0783\n Independent        2   0.0933    0.00754 12.37   &lt;0.001 114.4 0.0785 0.1080\n Independent        3   0.1126    0.00720 15.65   &lt;0.001 180.9 0.0985 0.1267\n Independent        4   0.1080    0.00764 14.13   &lt;0.001 148.2 0.0930 0.1229\n Republican         1   0.8375    0.01213 69.05   &lt;0.001   Inf 0.8137 0.8613\n Republican         2   0.6807    0.01405 48.46   &lt;0.001   Inf 0.6531 0.7082\n Republican         3   0.4580    0.01310 34.98   &lt;0.001 887.9 0.4324 0.4837\n Republican         4   0.2447    0.01134 21.58   &lt;0.001 340.8 0.2225 0.2669\n\nColumns: rowid, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, spending, education, union, sex, race_eth, age, abortion, pid3 \nType:  probs \n\n\nThe “Group” (or, really group4) variable indicates the category of the DV that the prediction is for. The column for abortion then indicates what value of the abortion variable the prediction is for. We then get the predicted value (estimate) and associate uncertainty estimates (std.error, conf.low, etc.). Holding the other covariates constant at their mean/mode, the predicted probability of a person who thinks that abortion should never be permitted saying that they are a Democrat is 0.0986 [95% CI: 0.0814, 0.1157]. A person with this abortion attitude is also unlikely to say they are an Independent (predicted probability: 0.06) and very likely to say they are a Republican (predicted probability: 0.84). We can also see that the probability of saying “Democrat” increases as we move across the abortion scale, largely stays flat for saying one is an Independent, and decreases for saying one is a Republican.\n4 You can see the real names for each category at the bottom of the output. This command produces gussied up output by default that capitalizes some names.We can, of course, plot these values. Indeed, that would be a great way to communicate the results of this plot. The main complication is in figuring out how to plot the three categories of predicted values (predicted prob of being Democrat, Republican, Independent). We could do that in two basic ways as shown here: (1) we can map the group category indicator to an aesthetic (e.g., linetype or color) or (2) separate them into different facets.\n\n#Map to an aesthetic\nggplot(pred_multinom, \n       aes(x = abortion, y = estimate, linetype = group)) + \n  geom_line() + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.2) + \n  labs(y = \"Predicted Probability\", \n       x = \"Abortion Attitude\", \n       linetype = \"DV Category\")\n\n\n\n\n\n\n\n#Separate facets\nggplot(pred_multinom, \n       aes(x = abortion, y = estimate)) + \n  geom_line() + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.2) + \n  facet_wrap(~ group) + \n  labs(y = \"Predicted Probability\", \n       x = \"Abortion Attitude\")\n\n\n\n\n\n\n\n\nI think the first example is the superior option here as it makes it very easy to compare predictions within abortion attitudes (e.g., probability Democrat if abortion = 1 vs. probability Republican if aboriton = 2). Here, we can see that abortion attitudes primarily differentiate Republicans from Democrats and don’t really seem to matter all that much for whether a person says they are an “Independent” or not. The second option might be more useful if there were more categories insofar as that made the single plot difficult to read/interpret.\nWe can also use the avg_slopes() command to get the average marginal effect of a variable as in this example:\n\n# the slopes\navg_slopes(model_multinom, variables = \"abortion\")\n\n\n       Group     Term  Estimate Std. Error         z Pr(&gt;|z|)     S    2.5 %\n Democrat    abortion  1.22e-01    0.00415  29.50738   &lt;0.001 633.3  0.11423\n Independent abortion -1.41e-05    0.00318  -0.00444    0.996   0.0 -0.00625\n Republican  abortion -1.22e-01    0.00361 -33.91277   &lt;0.001 835.0 -0.12942\n   97.5 %\n  0.13049\n  0.00622\n -0.11527\n\nColumns: term, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  probs \n\n# plotting them\navg_slopes(model_multinom, variables = \"abortion\") |&gt; \n  ggplot(aes(x = group, y = estimate)) + \n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) + \n  geom_text(aes(label = round(estimate, 2)), hjust = -0.5) + \n  geom_hline(yintercept = 0, linetype = 'dashed', color = 'red') + \n  labs(x = \"DV Category\", \n       y = \"AME for Abortion Attitudes\")\n\n\n\n\n\n\n\n\nThe probability of being a Democrat is expected to increase by 0.12 probability points (12 percentage points) with each one unit increase in the abortion attitudes measure. One thing to note here: probabilities in an “event space” need to sum up to 1. The “event space” is the different categories we’re calculating probabilities for (e.g., Democrat, Independent, and Republican). In this instance, the +0.12 gain in the probability of being a Democrat is offset by a -0.12 loss in the probability of being a Republican. Per above, abortion attitudes are basically unrelated to being an “Independent”. In other contexts, we might see a +0.12 for group 1, -0.04 for group 2, and -0.08 for group 3.\n\n\n19.2.3 Regression Table\nLet’s create a regression table:\n\nmodelsummary(model_multinom)\n\nWarning: There are duplicate term names in the table.\n  The `shape` argument of the `modelsummary` function can be used to print\n  related terms together. The `group_map` argument can be used to reorder,\n  subset, and rename group identifiers. See `?modelsummary` for details.\n  You can find the group identifier to use in the `shape` argument by\n  calling `get_estimates()` on one of your models. Candidates include:\n  response, s.value, group\n\n\n \n\n  \n    \n    \n    tinytable_hevrxzfopq3qx6rw28fm\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)            \n                  4.369  \n                \n                \n                                         \n                  8.195  \n                \n                \n                                         \n                  (0.267)\n                \n                \n                                         \n                  (0.325)\n                \n                \n                  abortion               \n                  -0.453 \n                \n                \n                                         \n                  -1.038 \n                \n                \n                                         \n                  (0.038)\n                \n                \n                                         \n                  (0.049)\n                \n                \n                  spending               \n                  -0.426 \n                \n                \n                                         \n                  -0.820 \n                \n                \n                                         \n                  (0.026)\n                \n                \n                                         \n                  (0.032)\n                \n                \n                  education              \n                  -0.233 \n                \n                \n                                         \n                  -0.442 \n                \n                \n                                         \n                  (0.034)\n                \n                \n                                         \n                  (0.044)\n                \n                \n                  unionUnion Household   \n                  -0.201 \n                \n                \n                                         \n                  -0.279 \n                \n                \n                                         \n                  (0.106)\n                \n                \n                                         \n                  (0.139)\n                \n                \n                  sexFemale              \n                  -0.157 \n                \n                \n                                         \n                  -0.224 \n                \n                \n                                         \n                  (0.073)\n                \n                \n                                         \n                  (0.094)\n                \n                \n                  race_ethBlack          \n                  -0.635 \n                \n                \n                                         \n                  -2.115 \n                \n                \n                                         \n                  (0.171)\n                \n                \n                                         \n                  (0.190)\n                \n                \n                  race_ethHispanic       \n                  -0.867 \n                \n                \n                                         \n                  0.009  \n                \n                \n                                         \n                  (0.136)\n                \n                \n                                         \n                  (0.147)\n                \n                \n                  race_ethAsian          \n                  -0.773 \n                \n                \n                                         \n                  0.367  \n                \n                \n                                         \n                  (0.211)\n                \n                \n                                         \n                  (0.218)\n                \n                \n                  race_ethNative American\n                  -0.128 \n                \n                \n                                         \n                  1.263  \n                \n                \n                                         \n                  (0.266)\n                \n                \n                                         \n                  (0.287)\n                \n                \n                  race_ethMultiple Races \n                  -0.477 \n                \n                \n                                         \n                  0.333  \n                \n                \n                                         \n                  (0.211)\n                \n                \n                                         \n                  (0.222)\n                \n                \n                  age                    \n                  -0.004 \n                \n                \n                                         \n                  -0.014 \n                \n                \n                                         \n                  (0.002)\n                \n                \n                                         \n                  (0.003)\n                \n                \n                  Num.Obs.               \n                  6386   \n                \n                \n                  R2                     \n                  0.488  \n                \n                \n                  R2 Adj.                \n                  0.488  \n                \n                \n                  AIC                    \n                  8261.0 \n                \n                \n                  BIC                    \n                  8423.3 \n                \n                \n                  RMSE                   \n                  0.34   \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nOh boy. First, we get a warning message about duplicate terms. This is because each IV has two coefficients and standard errors associated with them: one for the Independent vs. Democrat model and one for the Republican vs. Democrat model. Second, modelsummary() has just dumped those values into the same area, yielding an impossible to understand table.\nWe can get around these issues by using the shape functionality in modelsummary() (see here).\n\nmodelsummary(model_multinom, \n             stars = T, \n             gof_map = c(\"nobs\"), \n             shape = model + term ~ response)\n\n \n\n  \n    \n    \n    tinytable_yyy50tarfmw8t589k958\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                  \n                Independent\n                Republican\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (1)\n                  (Intercept)            \n                  4.369*** \n                  8.195*** \n                \n                \n                     \n                                         \n                  (0.325)  \n                  (0.267)  \n                \n                \n                     \n                  abortion               \n                  -0.453***\n                  -1.038***\n                \n                \n                     \n                                         \n                  (0.049)  \n                  (0.038)  \n                \n                \n                     \n                  spending               \n                  -0.426***\n                  -0.820***\n                \n                \n                     \n                                         \n                  (0.032)  \n                  (0.026)  \n                \n                \n                     \n                  education              \n                  -0.442***\n                  -0.233***\n                \n                \n                     \n                                         \n                  (0.044)  \n                  (0.034)  \n                \n                \n                     \n                  unionUnion Household   \n                  -0.279*  \n                  -0.201+  \n                \n                \n                     \n                                         \n                  (0.139)  \n                  (0.106)  \n                \n                \n                     \n                  sexFemale              \n                  -0.157+  \n                  -0.224** \n                \n                \n                     \n                                         \n                  (0.094)  \n                  (0.073)  \n                \n                \n                     \n                  race_ethBlack          \n                  -0.635***\n                  -2.115***\n                \n                \n                     \n                                         \n                  (0.171)  \n                  (0.190)  \n                \n                \n                     \n                  race_ethHispanic       \n                  0.009    \n                  -0.867***\n                \n                \n                     \n                                         \n                  (0.147)  \n                  (0.136)  \n                \n                \n                     \n                  race_ethAsian          \n                  0.367+   \n                  -0.773***\n                \n                \n                     \n                                         \n                  (0.218)  \n                  (0.211)  \n                \n                \n                     \n                  race_ethNative American\n                  1.263*** \n                  -0.128   \n                \n                \n                     \n                                         \n                  (0.266)  \n                  (0.287)  \n                \n                \n                     \n                  race_ethMultiple Races \n                  0.333    \n                  -0.477*  \n                \n                \n                     \n                                         \n                  (0.222)  \n                  (0.211)  \n                \n                \n                     \n                  age                    \n                  -0.014***\n                  -0.004*  \n                \n                \n                     \n                                         \n                  (0.003)  \n                  (0.002)  \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n\nshape = model + term ~ response\n\nThis can basically be kept the same for your examples. It is telling modelsummary() to plot the results of the different models in separate columns. response refers to the categories of the DV. We would naturally want to clean this table up a bit (e.g,. rename variables, etc.), but I leave that out here for simplicity sake.\n\n\nThere is one remaining annoyance: for some reason, modelsummary() is not printing the number of observations or goodness of fit statistics. It does this in the example given on the package’s webpage for the shape function as well. I have worked out some janky work around that mostly but doesn’t fully work, but, frankly, the simplest solution would be to save the table to Word document and then manually add additional values for the number of observations and the Nagelkerke R2:\n\n# Number of observations = nobs\n# glance() is from the broom package\nglance(model_multinom)\n\n# A tibble: 1 × 4\n    edf deviance   AIC  nobs\n  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1    24    8213. 8261.  6386\n\n# Nagelkerke R2\nperformance::r2_nagelkerke(model_multinom)\n\nNagelkerke's R2 \n      0.7684656 \n\n\n\n\n19.2.4 Assumptions\nFirst, we can investigate multicollinearity using the vif() package from the car library:\n\ncar::vif(model_multinom)\n\nWarning in vif.default(model_multinom): No intercept: vifs may not be sensible.\n\n\n               GVIF Df GVIF^(1/(2*Df))\nabortion  13.689869  1        3.699982\nspending  12.962702  1        3.600375\neducation 11.057295  1        3.325251\nunion      1.275787  1        1.129507\nsex        2.407957  1        1.551759\nrace_eth   3.521012  5        1.134140\nage       11.254592  1        3.354786\n\n\nIn which case we get a warning. The command won’t produce sensible output for this type of model. Instead, we can run a normal linear regression on the DV and calculate VIF from that. VIF, recall, is about the inter-correlation of the independent variables. In my example, my DV is a character variable, so I need to create a numeric version of the variable to use in this model:\n\nanes &lt;- anes |&gt; \n  mutate(\n    pid3_numeric  = case_when(\n      pid3 == \"Democrat\" ~ 1, \n      pid3 == \"Independent\" ~ 2, \n      pid3 == \"Republican\" ~ 3)\n  )\n\nvif_model &lt;- lm(pid3_numeric ~ abortion + spending + education + union + sex + race_eth + age, data = anes)\n\ncar::vif(vif_model)\n\n              GVIF Df GVIF^(1/(2*Df))\nabortion  1.193542  1        1.092493\nspending  1.254863  1        1.120206\neducation 1.053162  1        1.026237\nunion     1.007607  1        1.003796\nsex       1.025055  1        1.012450\nrace_eth  1.133282  5        1.012590\nage       1.064573  1        1.031781\n\n\nWe don’t have a problem with multicollinearity.\nThere are some other assumptions related to multinomial logistic models (see discussion here) but I wouldn’t expect you to investigate them (e.g., the IIA assumption) given that we’re already on somewhat advanced grounds.",
    "crumbs": [
      "Practical Issues in Data Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Categorical DVs</span>"
    ]
  }
]